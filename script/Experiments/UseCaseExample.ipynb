{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from script.NeuralNets.Networks import SequentialNN\n",
    "from script.settings import device, data_type\n",
    "import script.DHOV.MultiDHOV as multidhov\n",
    "from script.Verification.Verifier import SingleNeuronVerifier, MILPVerifier\n",
    "import gurobipy as grp\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from script.NeuralNets.ICNNFactory import ICNNFactory\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:09.733970523Z",
     "start_time": "2024-04-09T03:44:08.928150863Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx2pytorch import ConvertModel\n",
    "from auto_LiRPA import BoundedModule, BoundedTensor, PerturbationLpNorm\n",
    "\n",
    "from vnnlib.compat import read_vnnlib_simple\n",
    "from collections import OrderedDict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:10.044518402Z",
     "start_time": "2024-04-09T03:44:09.731683634Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import the Feedforward-NN\n",
    "It has to bee a SequentialNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'nn = SequentialNN([28 * 28 * 1, 256, 256, 256, 256, 10])\\nnn.load_state_dict(torch.load(\"../../mnist_fc 4x256.pth\", map_location=torch.device(\"cpu\")), strict=False)'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"nn = SequentialNN([28 * 28 * 1, 100, 30, 10])\n",
    "nn.load_state_dict(torch.load(\"../../mnist_fc.pth\", map_location=torch.device(device)), strict=False)\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "nn = SequentialNN([28 * 28 * 1, 256, 256, 256, 256, 256, 256, 10])\n",
    "nn.load_state_dict(torch.load(\"../../mnist_fc_6x256.pth\", map_location=torch.device('cpu')), strict=False)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"nn = SequentialNN([28 * 28 * 1, 256, 256, 256, 256, 10])\n",
    "nn.load_state_dict(torch.load(\"../../mnist_fc 4x256.pth\", map_location=torch.device(\"cpu\")), strict=False)\"\"\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:10.045737616Z",
     "start_time": "2024-04-09T03:44:10.024144348Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### or make onnx to Sequential"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ufuk/miniconda3/envs/autolirpa_icnn/lib/python3.10/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model = onnx.load(\"mnist-net_256x4.onnx\")\n",
    "pytorch_model = ConvertModel(onnx_model)\n",
    "\n",
    "nn = SequentialNN([28 * 28 * 1, 256, 256, 256, 256, 10])\n",
    "state_dict = pytorch_model.state_dict()\n",
    "my_state_dict = {}\n",
    "for i, key in enumerate(state_dict):\n",
    "    actual_index = i * 2\n",
    "    my_state_dict[f\"{actual_index}.bias\"] = state_dict[f\"_initializer_layers_{actual_index}_bias\"]\n",
    "    my_state_dict[f\"{actual_index}.weight\"] = state_dict[f\"_initializer_layers_{actual_index}_weight\"]\n",
    "    if actual_index == 8:\n",
    "        break\n",
    "    \n",
    "nn.load_state_dict(my_state_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:10.505735274Z",
     "start_time": "2024-04-09T03:44:10.493460642Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:11.103844833Z",
     "start_time": "2024-04-09T03:44:11.099316644Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "parameter_list = list(nn.parameters())\n",
    "output_size = 10\n",
    "number_layer = (len(parameter_list) - 2) // 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:11.406436180Z",
     "start_time": "2024-04-09T03:44:11.403882432Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the data to apply the verification process for"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + .05  # revert normalization for viewing\n",
    "    npimg = img.to(\"cpu\").numpy()\n",
    "    plt.imshow(npimg, cmap=\"gray\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:12.030476558Z",
     "start_time": "2024-04-09T03:44:12.024264247Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "transform = Compose([ToTensor()])\n",
    "training_data = MNIST(root=\"../../mnist\", train=True, download=True, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:12.474864191Z",
     "start_time": "2024-04-09T03:44:12.429793972Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Settings for the optimization "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label is 3\n",
      "NN classifies image correctly as 3\n"
     ]
    }
   ],
   "source": [
    "# image to do the verification for\n",
    "image_index = 10\n",
    "\n",
    "# epsilon radius\n",
    "eps = 0.03\n",
    "\n",
    "# whether a verification attempt should be done using just MILP encoding for comparison (can take long)\n",
    "use_milp = False\n",
    "\n",
    "image, label = training_data[image_index]\n",
    "torch_image = torch.unsqueeze(image, 0).to(dtype=data_type).to(device)\n",
    "imshow(torch_image[0][0])\n",
    "print(f\"The label is {label}\")\n",
    "\n",
    "if torch.argmax(nn(torch_image)).item() == label:\n",
    "    print(\"NN classifies image correctly as {}\".format(label))\n",
    "else:\n",
    "    print(\"NN classifies image wrong\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:13.381686653Z",
     "start_time": "2024-04-09T03:44:13.333715098Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "total_correct = 0\n",
    "total_wrong = 0\n",
    "do_test = False\n",
    "if do_test:\n",
    "    for image, label in training_data:\n",
    "        torch_image = torch.unsqueeze(image, 0).to(dtype=data_type).to(device)\n",
    "        if torch.argmax(nn(torch_image)).item() == label:\n",
    "            total_correct += 1\n",
    "        else:\n",
    "            total_wrong += 1\n",
    "            \n",
    "        print(total_correct, total_wrong)\n",
    "    print(f\"accuracy {total_correct/ len(training_data)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:14.270441191Z",
     "start_time": "2024-04-09T03:44:14.264208921Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Add the objective to the optimization problem\n",
    "in this case we want to find the maximum difference between any one neuron and the target neuron (label)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def add_output_constraints(model, nn_layer_out_bounds, label, output_vars, sovler_bound=1e-3):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param model: the optimization problem in gurobi encoding the NN\n",
    "    :param nn_layer_out_bounds: torch.Tensor, approximating the upper and lower bounding the output layer of the NN\n",
    "    :param label: index of the label or target neuron which is compared against\n",
    "    :param output_vars: the gurobi variables from the model of the NN describing the output neurons of the NN\n",
    "    :param sovler_bound: provides a bound for the gurobi solver. If this bound is achieved, the optimizer stops\n",
    "    \"\"\"\n",
    "    \n",
    "    out_lb = nn_layer_out_bounds[-1][0].detach().cpu().numpy()\n",
    "    out_ub = nn_layer_out_bounds[-1][1].detach().cpu().numpy()\n",
    "    \n",
    "    difference_lb = out_lb - out_ub[label]\n",
    "    difference_ub = out_ub - out_lb[label]\n",
    "    difference_lb = difference_lb.tolist()\n",
    "    difference_ub = difference_ub.tolist()\n",
    "    \n",
    "    difference_lb.pop(label)\n",
    "    difference_ub.pop(label)\n",
    "    \n",
    "    min_diff = min(difference_lb)\n",
    "    max_diff = max(difference_ub)\n",
    "    \n",
    "    difference = model.addVars(9, lb=difference_lb, ub=difference_ub, name=\"diff_var\")\n",
    "    model.addConstrs((difference[i] == output_vars.tolist()[i] - output_vars.tolist()[label] for i in range(0, label)), name=\"diff_const0\")\n",
    "    model.addConstrs((difference[i - 1] == output_vars.tolist()[i] - output_vars.tolist()[label] for i in range(label + 1, 10)), name=\"diff_const1\")\n",
    "\n",
    "    max_var = model.addVar(lb=min_diff, ub=max_diff, name=\"max_var\")\n",
    "    model.addConstr(max_var == grp.max_(difference))\n",
    "\n",
    "    if sovler_bound != None:\n",
    "        model.setParam(\"BestObjStop\", sovler_bound)\n",
    "\n",
    "    model.update()\n",
    "    model.setObjective(max_var, grp.GRB.MAXIMIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:14.891553767Z",
     "start_time": "2024-04-09T03:44:14.886929899Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get the output variables by name \n",
    "given the gurobi model, the output size of the NN and the index of output layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def get_output_vars_snv_milp(model, output_size, output_layer_index):\n",
    "    output_vars = []\n",
    "    for i in range(output_size):\n",
    "        output_vars.append(model.getVarByName(\"affine_var{}[{}]\".format(output_layer_index, i)))\n",
    "    output_vars = grp.MVar.fromlist(output_vars)\n",
    "    return output_vars"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:15.269510014Z",
     "start_time": "2024-04-09T03:44:15.264415898Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def get_output_vars_dhov(model, output_size, output_layer_index):\n",
    "    output_vars = []\n",
    "    for i in range(output_size):\n",
    "        output_vars.append(model.getVarByName(\"output_layer_[{}]_[{}]\".format(output_layer_index, i)))\n",
    "    output_vars = grp.MVar.fromlist(output_vars)\n",
    "    return output_vars"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:15.442703452Z",
     "start_time": "2024-04-09T03:44:15.433431825Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining function to call Gurobi optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def optimize_model(model, output_vars):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param model: the optimization problem in gurobi encoding the NN and the objective \n",
    "    :param output_vars: the gurobi variables from the model of the NN describing the output neurons of the NN\n",
    "    :return True if verification was successful, else false \n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    model.update()\n",
    "    model.optimize()\n",
    "    \n",
    "    if model.Status == grp.GRB.OPTIMAL or model.Status == grp.GRB.USER_OBJ_LIMIT:\n",
    "        \n",
    "        for i, var in enumerate(output_vars.tolist()):\n",
    "            print(\"var {}: {}\".format(i, var.getAttr(\"x\")))\n",
    "        max_var = model.getVarByName(\"max_var\").getAttr(\"x\")\n",
    "        \n",
    "        if max_var < 0:\n",
    "            print(\"property verified with max difference {}\".format(max_var))\n",
    "            return True\n",
    "        else:\n",
    "             print(\"property NOT verified with max difference {}\".format(max_var))\n",
    "             return False\n",
    "\n",
    "    elif model.Status == grp.GRB.INFEASIBLE:\n",
    "        print(\"model infeasible\")\n",
    "\n",
    "        model.computeIIS()\n",
    "        print(\"constraint\")\n",
    "        all_constr = model.getConstrs()\n",
    "\n",
    "        for const in all_constr:\n",
    "            if const.IISConstr:\n",
    "                print(\"{}\".format(const))\n",
    "\n",
    "        print(\"lower bound\")\n",
    "        all_var = model.getVars()\n",
    "        for var in all_var:\n",
    "            if var.IISLB:\n",
    "                print(\"{}, lb: {}, ub: {}\".format(var, var.getAttr(\"lb\"), var.getAttr(\"ub\")))\n",
    "\n",
    "        print(\"upper bound\")\n",
    "        all_var = model.getVars()\n",
    "        for var in all_var:\n",
    "            if var.IISUB:\n",
    "                print(\"{}, lb: {}, ub: {}\".format(var, var.getAttr(\"lb\"), var.getAttr(\"ub\")))\n",
    "\n",
    "    print(\"time to optimize: {}\".format(time.time() - start))\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:15.777019055Z",
     "start_time": "2024-04-09T03:44:15.772822784Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate box-bounds to provide a rough approximation of NN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1032.553494946478, -872.9257565705025, -1141.959246872821, -1203.297658206732, -1105.71108391385, -986.7314910178, -954.3195523478022, -1036.1493852084127, -1236.6732217595502, -945.4201504345255]\n",
      "[822.8642229269997, 734.7687204634503, 739.7653141953177, 644.5772098039624, 858.5091913462222, 782.4145591124344, 917.3556478411418, 876.5302067990469, 868.9227335602428, 935.6565249544531]\n"
     ]
    }
   ],
   "source": [
    "input_flattened = torch.flatten(torch_image)\n",
    "simple_bounds_affine_out, simple_bounds_layer_out = nn.calculate_box_bounds([input_flattened.add(-eps), input_flattened.add(eps)])\n",
    "print(simple_bounds_affine_out[-1][0].tolist())\n",
    "print(simple_bounds_affine_out[-1][1].tolist())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:16.140217061Z",
     "start_time": "2024-04-09T03:44:16.133530663Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Do Single-Neuron-Verification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "use_snv = False\n",
    "if use_snv:\n",
    "    t = time.time()\n",
    "    \n",
    "    # define the Verifier\n",
    "    snv_verifier = SingleNeuronVerifier(nn, torch_image, eps, print_log=False, print_new_bounds=True, optimize_bounds=True)\n",
    "    \n",
    "    # generate the constraints\n",
    "    snv_verifier.generate_constraints_for_net()\n",
    "    snv_model = snv_verifier.model\n",
    "    snv_model.update()\n",
    "    \n",
    "    # add the objective to the gurobi model\n",
    "    snv_out_vars = get_output_vars_snv_milp(snv_model, output_size, number_layer * 2)\n",
    "    add_output_constraints(snv_model, simple_bounds_layer_out, label, snv_out_vars, sovler_bound=None)\n",
    "    print(\"constraint generation time: {}\".format(time.time() - t))\n",
    "    \n",
    "    print(\"start with optimization\")\n",
    "    optimize_model(snv_model, snv_out_vars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:16.522465450Z",
     "start_time": "2024-04-09T03:44:16.517482863Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " # Do MILP-Verification if wanted"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "if use_milp:\n",
    "    # define the Verifier\n",
    "    milp_verifier = MILPVerifier(nn, torch_image, eps, print_log=False)\n",
    "    \n",
    "    # generate the constraints\n",
    "    milp_verifier.generate_constraints_for_net()\n",
    "    milp_model = milp_verifier.model\n",
    "    milp_model.update()\n",
    "    \n",
    "    # add the objective to the gurobi model\n",
    "    milp_out_vars = get_output_vars_snv_milp(milp_model, output_size, number_layer * 2)\n",
    "    add_output_constraints(milp_model, simple_bounds_layer_out, label, milp_out_vars)\n",
    "    \n",
    "    print(\"start with optimization\")\n",
    "    optimize_model(milp_model, milp_out_vars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:17.138455103Z",
     "start_time": "2024-04-09T03:44:17.083154372Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# get bounds with crown"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_onnx_model(onnx_path, input_shape):\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    torch_model = ConvertModel(onnx_model)\n",
    "    \n",
    "    x_concrete = torch.zeros(input_shape)\n",
    "    model = BoundedModule(torch_model, x_concrete)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:17.669531785Z",
     "start_time": "2024-04-09T03:44:17.664740767Z"
    }
   },
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_vnnlib_spec(vnnlib_path, input_shape, n_out):\n",
    "    n_in = np.prod(input_shape)\n",
    "    res = read_vnnlib_simple(vnnlib_path, n_in, n_out)\n",
    "    bnds, spec = res[0]\n",
    "    \n",
    "    bnds = np.array(bnds)\n",
    "    lbs = bnds[:,0]\n",
    "    ubs = bnds[:,1]\n",
    "    \n",
    "    data_min = torch.tensor(lbs, dtype=torch.float32).reshape(input_shape)\n",
    "    data_max = torch.tensor(ubs, dtype=torch.float32).reshape(input_shape)\n",
    "    center = 0.5*(data_min + data_max)\n",
    "\n",
    "    ptb = PerturbationLpNorm(x_L=data_min, x_U=data_max)\n",
    "    x = BoundedTensor(center, ptb)\n",
    "    \n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:18.355715681Z",
     "start_time": "2024-04-09T03:44:18.351450661Z"
    }
   },
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "onnx_path = 'mnist-net_256x4.onnx'\n",
    "vnnlib_path = 'prop_0_0.03.vnnlib'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:18.597235390Z",
     "start_time": "2024-04-09T03:44:18.592447432Z"
    }
   },
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ufuk/miniconda3/envs/autolirpa_icnn/lib/python3.10/site-packages/onnx2pytorch/convert/model.py:163: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not self.experimental and inputs[0].shape[self.batch_dim] > 1:\n",
      "/home/ufuk/miniconda3/envs/autolirpa_icnn/lib/python3.10/site-packages/auto_LiRPA/parse_graph.py:154: FutureWarning: 'torch.onnx.symbolic_helper._set_opset_version' is deprecated in version 1.13 and will be removed in 2.0. Please remove its usage and avoid setting internal variables directly.\n",
      "  _set_opset_version(12)\n",
      "/home/ufuk/miniconda3/envs/autolirpa_icnn/lib/python3.10/site-packages/vnnlib/compat.py:283: UserWarning: literal negation does not strictly follow SMT-LIB\n",
      "  ast_node = parse_file(vnnlib_filename, strict=False)\n"
     ]
    }
   ],
   "source": [
    "model = load_onnx_model(onnx_path, [1,1,1,784])\n",
    "x = load_vnnlib_spec(vnnlib_path, [1,1,1,784], 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:19.385277680Z",
     "start_time": "2024-04-09T03:44:19.276751358Z"
    }
   },
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_layers(model):\n",
    "    return [l for l in model.nodes() if l.perturbed]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:20.189347348Z",
     "start_time": "2024-04-09T03:44:20.184386651Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_intermediate_bounds(model):\n",
    "    \"\"\"\n",
    "    Returns a dictionary containing the concrete lower and upper bounds of each layer.\n",
    "    \n",
    "    Implemented own method to filter out bounds for weight matrices.\n",
    "    \n",
    "    Only call this method after compute_bounds()!\n",
    "    \"\"\"\n",
    "    od = OrderedDict()\n",
    "    for l in get_layers(model):\n",
    "        if hasattr(l, 'lower'):\n",
    "            od[l.name] = (l.lower, l.upper)\n",
    "            \n",
    "    return od"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:20.471665917Z",
     "start_time": "2024-04-09T03:44:20.467152258Z"
    }
   },
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[-16.1574, -13.2932, -17.4020, -26.5492, -17.5962, -16.9601, -15.3861,\n          -17.6454, -20.9628, -15.7071]], grad_fn=<ViewBackward0>),\n tensor([[13.8042, 12.8868, 11.2402,  7.7739, 14.1580, 12.1219, 15.4775, 13.8468,\n          12.8768, 15.7155]], grad_fn=<ViewBackward0>))"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compute_bounds(x=(x,), method='ibp')\n",
    "model.compute_bounds(x=(x,), method='crown')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:20.822877674Z",
     "start_time": "2024-04-09T03:44:20.788857753Z"
    }
   },
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bounds_dict_crown = get_intermediate_bounds(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:21.116051544Z",
     "start_time": "2024-04-09T03:44:21.110197171Z"
    }
   },
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/0\n",
      "/input\n",
      "/input.3\n",
      "/input.7\n",
      "/input.11\n",
      "/30\n"
     ]
    }
   ],
   "source": [
    "bounds_dict_crown.keys()\n",
    "crown_bounds_affine_out = []\n",
    "for i, key in enumerate(bounds_dict_crown.keys()):\n",
    "    print(key)\n",
    "    if i == 0: # use this if ibp is used (or i % 2 == 1:)\n",
    "        continue\n",
    "    lb, ub = bounds_dict_crown[key]\n",
    "    crown_bounds_affine_out.append([lb.type(data_type).view(-1).to(device), ub.type(data_type).view(-1).to(device)])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:21.397517547Z",
     "start_time": "2024-04-09T03:44:21.392348421Z"
    }
   },
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crown_bounds_layer_out = []\n",
    "relu = torch.nn.ReLU()\n",
    "for i, (lb, ub) in enumerate(crown_bounds_affine_out):\n",
    "    if i == len(crown_bounds_affine_out) - 1:\n",
    "        crown_bounds_layer_out.append([lb, ub])\n",
    "    else:\n",
    "        lb_layer = relu(lb)\n",
    "        ub_layer = relu(ub)\n",
    "        crown_bounds_layer_out.append([lb_layer, ub_layer])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:22.131319517Z",
     "start_time": "2024-04-09T03:44:22.075183998Z"
    }
   },
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-16.1574, -13.2932, -17.4020, -26.5492, -17.5962, -16.9601, -15.3861,\n",
      "        -17.6454, -20.9628, -15.7071], dtype=torch.float64,\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(crown_bounds_affine_out[-1][0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:22.323491788Z",
     "start_time": "2024-04-09T03:44:22.318602381Z"
    }
   },
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-186.3934, -166.1222, -133.4717, -167.3497, -141.4671, -291.8646,\n",
      "        -145.1084, -152.4255, -105.8477, -119.2242, -158.8032, -335.8949,\n",
      "        -126.8841, -106.2285, -100.5970, -144.9053, -128.5827, -216.1721,\n",
      "        -104.5580, -176.2673, -241.1868, -135.9195, -116.9115, -149.1140,\n",
      "        -172.8038, -109.1280, -167.3485, -276.8488, -105.1384, -172.5290,\n",
      "        -128.1274, -281.1724, -120.7568, -179.6152, -190.0453, -197.2409,\n",
      "        -175.8437, -137.7844,  -88.1701, -168.7748, -128.0328, -165.3708,\n",
      "        -175.1400, -227.6097, -131.5107, -129.4821, -113.5110, -127.5611,\n",
      "        -223.4063, -121.9744, -151.3296, -150.1186, -121.7823, -129.9400,\n",
      "        -190.6910, -152.7667, -126.3819, -123.5460, -159.6351, -241.9066,\n",
      "        -114.8181, -288.5023, -137.2866, -112.5156, -162.7558, -157.9243,\n",
      "        -237.5894, -131.1929, -186.1840, -109.7676, -165.9870, -158.3168,\n",
      "        -300.4587, -124.3646, -362.5540, -122.0489,  -99.1765, -143.4968,\n",
      "        -151.3960, -114.2471, -112.7730, -112.5733, -115.4081, -172.6636,\n",
      "        -103.8998, -128.5794, -176.1133, -158.8269, -142.1364, -133.9637,\n",
      "        -133.7865, -194.0288, -137.0399, -150.1549, -199.1741, -169.8477,\n",
      "        -179.7442, -175.4719, -101.4554, -151.2302, -121.8008, -122.6872,\n",
      "        -173.3127, -257.5265, -162.9561, -250.8247, -104.3047, -238.6204,\n",
      "        -130.5028, -265.8863, -173.2680, -156.1561, -112.3295, -138.3398,\n",
      "        -168.4479, -119.3255, -143.3329, -100.6397, -279.3831, -164.8405,\n",
      "        -223.1750, -325.2364, -157.6851, -117.8452, -147.6275, -109.8071,\n",
      "        -252.7931, -198.2370, -136.8897, -177.9918, -132.0564, -117.0051,\n",
      "        -126.9812, -115.2443, -126.4513, -111.3819, -157.5078, -176.5843,\n",
      "        -131.3983, -134.6679, -103.0494, -135.0540, -203.2134, -134.6521,\n",
      "        -113.3443, -190.0200, -246.3852, -159.2128, -134.3103, -114.0819,\n",
      "        -155.0182, -116.1482, -248.5558, -127.7926, -134.6300, -227.1549,\n",
      "        -229.2332, -230.4422, -154.5544, -160.6041, -111.6315,  -87.9451,\n",
      "        -108.1292, -134.9607, -135.3442, -108.3047, -272.9389, -157.0391,\n",
      "        -156.4060, -148.9137, -117.1439, -150.3957, -117.2139, -113.3516,\n",
      "        -272.6296, -172.5640, -136.0557, -136.5133, -188.2690, -117.2945,\n",
      "        -173.6996, -108.6314, -108.6978, -150.9273, -117.8900, -141.8904,\n",
      "         -98.4711,  -98.0646, -186.1697, -146.9012, -201.0540, -183.6186,\n",
      "        -306.1072,  -95.1792, -124.3484, -169.9929, -104.2702, -118.2162,\n",
      "        -165.1493, -116.9200, -285.6527, -130.5676, -115.7462, -226.7602,\n",
      "        -116.0455, -279.3685, -108.1503, -141.8006, -180.7984, -172.1093,\n",
      "        -161.7695, -291.1242, -150.7416, -262.0826, -243.3222, -284.6442,\n",
      "         -95.4991, -153.8577, -114.9658, -160.6414, -195.6164, -164.5998,\n",
      "        -121.8726, -226.1636, -143.7778, -106.2142, -260.8815, -139.2729,\n",
      "        -126.0475, -115.9344, -107.1182, -179.5000, -175.5242, -139.7872,\n",
      "        -141.2714, -146.0680, -129.3048, -120.7368, -105.3489, -180.2963,\n",
      "        -179.1040, -123.8305, -135.8887,  -98.3802, -234.0404, -176.5208,\n",
      "        -114.3593, -130.6487, -124.5880, -232.6939, -205.0162, -193.3223,\n",
      "        -320.6713, -131.7776, -110.7680, -131.9771], dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(simple_bounds_affine_out[2][0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:22.540335925Z",
     "start_time": "2024-04-09T03:44:22.499756484Z"
    }
   },
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'snv_verifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m snv_affine_out, snv_layer_out \u001B[38;5;241m=\u001B[39m \u001B[43msnv_verifier\u001B[49m\u001B[38;5;241m.\u001B[39mbounds_affine_out, snv_verifier\u001B[38;5;241m.\u001B[39mbounds_layer_out\n",
      "\u001B[0;31mNameError\u001B[0m: name 'snv_verifier' is not defined"
     ]
    }
   ],
   "source": [
    "snv_affine_out, snv_layer_out = snv_verifier.bounds_affine_out, snv_verifier.bounds_layer_out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:22.967242210Z",
     "start_time": "2024-04-09T03:44:22.722759313Z"
    }
   },
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'snv_affine_out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[30], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43msnv_affine_out\u001B[49m[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m][\u001B[38;5;241m1\u001B[39m])\n",
      "\u001B[0;31mNameError\u001B[0m: name 'snv_affine_out' is not defined"
     ]
    }
   ],
   "source": [
    "print(snv_affine_out[-1][1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:23.559978071Z",
     "start_time": "2024-04-09T03:44:23.556390408Z"
    }
   },
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(False)\n",
      "===============\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "===============\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "===============\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "===============\n",
      "tensor(False)\n",
      "tensor(False)\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "for (lb_crown, ub_crown), (lb_simple, ub_simple) in zip(crown_bounds_affine_out, simple_bounds_affine_out):\n",
    "    print(torch.isclose(lb_crown, lb_simple).all())\n",
    "    print(torch.isclose(ub_crown, ub_simple).all())\n",
    "    print(\"===============\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T03:44:23.745032647Z",
     "start_time": "2024-04-09T03:44:23.732700074Z"
    }
   },
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Attempt verification using DHOV\n",
    "See MultiDHOV.py and ICNNFactory.py for detailed documentation about the parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2025-02-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ufuk/Documents/Programming/ICNN_verification/script/DHOV/Sampling/PerGroupLineSearchSampling.py:19: UserWarning: keep_ambient_space is True and sampling method is per_group_sampling. Keeping previous samples is not supported when using per group sampling\n",
      "  warnings.warn(\"keep_ambient_space is True and sampling method is per_group_sampling. \"\n",
      "/home/ufuk/Documents/Programming/ICNN_verification/script/DHOV/MultiDHOV.py:188: UserWarning: value for group number multiplier is given with grouping method consecutive. consecutive grouping does not use variable number of groups\n",
      "  warnings.warn(\"value for group number multiplier is given with grouping method consecutive. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "approximation of layer: 0\n",
      "    number of fixed neurons for current layer: 202\n",
      "        time for sampling: 0.9501633644104004\n",
      "    layer progress, group 1 of 3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ufuk/Documents/Programming/ICNN_verification/script/Optimizer/sdlbfgs.py:83: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)\n",
      "  p.data.add_(step_size, update[offset:offset + numel].view_as(p.data))\n",
      "/home/ufuk/Documents/Programming/ICNN_verification/script/NeuralNets/Networks.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b.data = b - torch.tensor(value, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        time for training: 10.628385305404663\n",
      "        actual verification time 0.6850028038024902\n",
      "        time for verification: 0.7169051170349121\n",
      "    layer progress, group 2 of 3 \n",
      "        time for training: 10.871278524398804\n",
      "        actual verification time 0.24419403076171875\n",
      "        time for verification: 0.27588391304016113\n",
      "    layer progress, group 3 of 3 \n",
      "        time for training: 11.258182287216187\n",
      "        actual verification time 0.1515347957611084\n",
      "        time for verification: 0.17595505714416504\n",
      "\n",
      "approximation of layer: 1\n",
      "    number of fixed neurons for current layer: 164\n",
      "        time for sampling: 33.567941665649414\n",
      "    layer progress, group 1 of 5 \n",
      "        time for training: 20.01220679283142\n",
      "        actual verification time 0.8050930500030518\n",
      "        time for verification: 0.8378758430480957\n",
      "    layer progress, group 2 of 5 \n",
      "        time for training: 15.730262517929077\n",
      "        actual verification time 0.631234884262085\n",
      "        time for verification: 0.6631689071655273\n",
      "    layer progress, group 3 of 5 \n",
      "        time for training: 13.208032608032227\n",
      "        actual verification time 0.8706417083740234\n",
      "        time for verification: 0.9032120704650879\n",
      "    layer progress, group 4 of 5 \n",
      "        time for training: 17.862109184265137\n",
      "        actual verification time 0.5220873355865479\n",
      "        time for verification: 0.5547363758087158\n",
      "    layer progress, group 5 of 5 \n",
      "        time for training: 19.86023736000061\n",
      "        actual verification time 0.44671154022216797\n",
      "        time for verification: 0.5358889102935791\n",
      "\n",
      "approximation of layer: 2\n",
      "    number of fixed neurons for current layer: 231\n",
      "        time for sampling: 18.63607406616211\n",
      "    layer progress, group 1 of 2 \n",
      "        time for training: 12.807648181915283\n",
      "        actual verification time 1.6834330558776855\n",
      "        time for verification: 1.7163076400756836\n",
      "    layer progress, group 2 of 2 \n",
      "        time for training: 31.61520767211914\n",
      "        actual verification time 0.29079318046569824\n",
      "        time for verification: 0.303530216217041\n",
      "\n",
      "approximation of layer: 3\n",
      "    number of fixed neurons for current layer: 1\n",
      "    encode layer 3 as MILP\n",
      "time for icnn_bound calculation (last layer):0.0011191368103027344\n",
      "done...\n",
      "overall time: 224.2832169532776\n"
     ]
    }
   ],
   "source": [
    "from script.DHOV.Sampling.PerGroupLineSearchSampling import PerGroupLineSearchSamplingStrategy\n",
    "from script.DHOV.Sampling.PerGroupSamplingStrategy import PerGroupSamplingStrategy\n",
    "center = torch.flatten(torch_image)\n",
    "sampling_strategy = PerGroupLineSearchSamplingStrategy(center, eps, nn, keep_ambient_space=True, sample_new=False, sample_count=10000)\n",
    "# number of neurons which are in one group\n",
    "group_size = 20\n",
    "\n",
    "# method of sampling data points\n",
    "#sampling_method = \"per_group_sampling\"\n",
    "\n",
    "# Size of each layer of the ICNN except for the input layer. (given by group size)\n",
    "net_size = [5, 1]\n",
    "\n",
    "# picking the typ of ICNN to use\n",
    "#icnn_factory = ICNNFactory(\"approx_max\", net_size, maximum_function=\"SMU\", function_parameter=0.3)\n",
    "icnn_factory = ICNNFactory(\"logical\", net_size, always_use_logical_layer=False)\n",
    "#icnn_factory = ICNNFactory(\"standard\", net_size, adapt_layer_for_init=True)\n",
    "\n",
    "bounds_affine_out_snr, bounds_layer_out_snr = crown_bounds_affine_out, crown_bounds_layer_out\n",
    "\n",
    "overall_time = time.time()\n",
    "dhov_verifier = multidhov.MultiDHOV()\n",
    "dhov_verifier.start_verification(nn, torch_image, icnn_factory, group_size, eps=eps, icnn_epochs=200,\n",
    "                                 init_affine_bounds=bounds_affine_out_snr, init_layer_bounds=bounds_layer_out_snr,\n",
    "                                 icnn_batch_size=10000, use_over_approximation=True, break_after=None,\n",
    "                                 tighten_bounds=False, \n",
    "                                 use_fixed_neurons_in_grouping=False, layers_as_snr=[], layers_as_milp=[3],\n",
    "                                 force_inclusion_steps=3, preemptive_stop=True,\n",
    "                                data_grad_descent_steps=0, opt_steps_gd=100,\n",
    "                                 train_outer=False, print_training_loss=False, print_new_bounds=False,\n",
    "                                 grouping_method=\"consecutive\", group_num_multiplier=5, store_samples=False, print_optimization_steps=False, print_last_loss=False,\n",
    "                                 should_plot=\"none\", optimizer=\"SdLBFGS\", init_network=True, adapt_lambda=\"included\", sampling_strategy=sampling_strategy)\n",
    "print(\"overall time: {}\".format(time.time() - overall_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-04-09T03:48:08.992194286Z",
     "start_time": "2024-04-09T03:44:24.663002270Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting an overview over what DHOV has done\n",
    "show the neuron groups created by DHOV for each layer by index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0\n",
      "    Group 0: [0, 1, 8, 10, 13, 15, 29, 30, 35, 42, 61, 62, 69, 75, 78, 80, 86, 92, 93, 97]\n",
      "    Group 1: [114, 120, 127, 134, 137, 146, 156, 166, 172, 175, 176, 177, 178, 179, 182, 187, 188, 190, 194, 195]\n",
      "    Group 2: [199, 200, 201, 204, 207, 213, 216, 219, 220, 229, 230, 235, 251, 255]\n",
      "Layer 1\n",
      "    Group 0: [2, 3, 5, 6, 7, 14, 17, 20, 21, 25, 33, 40, 42, 43, 44, 47, 49, 54, 63, 64]\n",
      "    Group 1: [65, 67, 69, 70, 71, 72, 74, 75, 76, 79, 80, 82, 83, 92, 93, 94, 95, 96, 100, 101]\n",
      "    Group 2: [103, 110, 118, 121, 123, 126, 128, 131, 132, 134, 135, 136, 140, 141, 142, 143, 154, 156, 162, 165]\n",
      "    Group 3: [166, 181, 186, 189, 190, 191, 193, 204, 205, 208, 212, 214, 215, 216, 217, 218, 219, 220, 223, 225]\n",
      "    Group 4: [233, 234, 237, 239, 240, 241, 242, 243, 246, 248, 252, 254]\n",
      "Layer 2\n",
      "    Group 0: [27, 31, 38, 43, 44, 61, 107, 117, 118, 131, 133, 146, 151, 157, 161, 172, 186, 187, 200, 203]\n",
      "    Group 1: [205, 211, 229, 243, 246]\n",
      "Layer 3\n"
     ]
    }
   ],
   "source": [
    "for i, groups_in_layer in enumerate(dhov_verifier.all_group_indices):\n",
    "    print(f\"Layer {i}\")\n",
    "    for k, group in enumerate(groups_in_layer):\n",
    "        print(f\"    Group {k}: {group}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-04-09T03:48:13.757569741Z",
     "start_time": "2024-04-09T03:48:13.752950560Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "show the output bounds for the NN. Hopefully these are better than the box-bounds and the bounds obtained by the Single-Neuron-Verification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-16.157411575317383, -13.293179512023926, -17.402015686035156, -26.54916000366211, -17.59623908996582, -16.960063934326172, -15.386096954345703, -17.645418167114258, -20.96283721923828, -15.707077026367188]\n",
      "[13.804232597351074, 12.88681411743164, 11.240226745605469, 7.773935794830322, 14.157960891723633, 12.121850967407227, 15.477540969848633, 13.84675121307373, 12.876795768737793, 15.715452194213867]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(dhov_verifier.bounds_affine_out[-1][0].tolist())\n",
    "print(dhov_verifier.bounds_affine_out[-1][1].tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-04-09T03:48:15.173541082Z",
     "start_time": "2024-04-09T03:48:15.165002148Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Starting the optimization process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# make changes on a copy of the gurobi model generated by DHOV\n",
    "dhov_model = dhov_verifier.nn_encoding_model.copy()\n",
    "dhov_model.update()\n",
    "dhov_out_vars = get_output_vars_dhov(dhov_model, output_size, number_layer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-04-09T03:48:16.168019453Z",
     "start_time": "2024-04-09T03:48:16.166714698Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "add_output_constraints(dhov_model, simple_bounds_layer_out, label, dhov_out_vars, sovler_bound=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-04-09T03:48:16.474989130Z",
     "start_time": "2024-04-09T03:48:16.469321264Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var 0: 0.125204392417111\n",
      "var 1: 0.019707263396536084\n",
      "var 2: 0.04259643624594632\n",
      "var 3: -0.04030091125113179\n",
      "var 4: 0.033767361411350685\n",
      "var 5: -0.12147283351876137\n",
      "var 6: 0.0718239145502837\n",
      "var 7: -0.011287063236289044\n",
      "var 8: -0.028540220342562997\n",
      "var 9: 1.0932997117190466\n",
      "property NOT verified with max difference 1.1336006229701785\n",
      "overall time: 15.77420687675476\n"
     ]
    }
   ],
   "source": [
    "overall_time = time.time()\n",
    "optimize_model(dhov_model, dhov_out_vars)\n",
    "print(\"overall time: {}\".format(time.time() - overall_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-04-09T03:48:32.610116348Z",
     "start_time": "2024-04-09T03:48:16.833896778Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " "
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
