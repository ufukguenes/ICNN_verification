{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from script.NeuralNets.Networks import SequentialNN\n",
    "from script.settings import device, data_type\n",
    "import script.DHOV.MultiDHOV as multidhov\n",
    "from script.Verification.Verifier import SingleNeuronVerifier, MILPVerifier\n",
    "import gurobipy as grp\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from script.NeuralNets.ICNNFactory import ICNNFactory\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:01.674202125Z",
     "start_time": "2024-05-02T23:40:00.803128535Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx2pytorch import ConvertModel\n",
    "from auto_LiRPA import BoundedModule, BoundedTensor, PerturbationLpNorm\n",
    "\n",
    "from vnnlib.compat import read_vnnlib_simple\n",
    "from collections import OrderedDict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:01.877872600Z",
     "start_time": "2024-05-02T23:40:01.715145210Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import the Feedforward-NN\n",
    "It has to bee a SequentialNN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "'nn = SequentialNN([28 * 28 * 1, 256, 256, 256, 256, 10])\\nnn.load_state_dict(torch.load(\"../../mnist_fc 4x256.pth\", map_location=torch.device(\"cpu\")), strict=False)'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"nn = SequentialNN([28 * 28 * 1, 100, 30, 10])\n",
    "nn.load_state_dict(torch.load(\"../../mnist_fc.pth\", map_location=torch.device(device)), strict=False)\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "nn = SequentialNN([28 * 28 * 1, 256, 256, 256, 256, 256, 256, 10])\n",
    "nn.load_state_dict(torch.load(\"../../mnist_fc_6x256.pth\", map_location=torch.device('cpu')), strict=False)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"nn = SequentialNN([28 * 28 * 1, 256, 256, 256, 256, 10])\n",
    "nn.load_state_dict(torch.load(\"../../mnist_fc 4x256.pth\", map_location=torch.device(\"cpu\")), strict=False)\"\"\"\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:01.884557361Z",
     "start_time": "2024-05-02T23:40:01.879211808Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### or make onnx to Sequential"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ufuk/miniconda3/envs/autolirpa_icnn/lib/python3.10/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model = onnx.load(\"mnist-net_256x4.onnx\")\n",
    "pytorch_model = ConvertModel(onnx_model)\n",
    "\n",
    "nn = SequentialNN([28 * 28 * 1, 256, 256, 256, 256, 10])\n",
    "state_dict = pytorch_model.state_dict()\n",
    "my_state_dict = {}\n",
    "for i, key in enumerate(state_dict):\n",
    "    actual_index = i * 2\n",
    "    my_state_dict[f\"{actual_index}.bias\"] = state_dict[f\"_initializer_layers_{actual_index}_bias\"]\n",
    "    my_state_dict[f\"{actual_index}.weight\"] = state_dict[f\"_initializer_layers_{actual_index}_weight\"]\n",
    "    if actual_index == 8:\n",
    "        break\n",
    "    \n",
    "nn.load_state_dict(my_state_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:01.892721520Z",
     "start_time": "2024-05-02T23:40:01.881691185Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:01.893321339Z",
     "start_time": "2024-05-02T23:40:01.891144202Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "parameter_list = list(nn.parameters())\n",
    "output_size = 10\n",
    "number_layer = (len(parameter_list) - 2) // 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:01.938301098Z",
     "start_time": "2024-05-02T23:40:01.893302889Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the data to apply the verification process for"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + .05  # revert normalization for viewing\n",
    "    npimg = img.to(\"cpu\").numpy()\n",
    "    plt.imshow(npimg, cmap=\"gray\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:01.938899008Z",
     "start_time": "2024-05-02T23:40:01.935171253Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "transform = Compose([ToTensor()])\n",
    "training_data = MNIST(root=\"../../mnist\", train=True, download=True, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:01.939419637Z",
     "start_time": "2024-05-02T23:40:01.935241153Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Settings for the optimization "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label is 3\n",
      "NN classifies image correctly as 3\n"
     ]
    }
   ],
   "source": [
    "# image to do the verification for\n",
    "image_index = 10\n",
    "\n",
    "# whether a verification attempt should be done using just MILP encoding for comparison (can take long)\n",
    "use_milp = False\n",
    "\n",
    "image, label = training_data[image_index]\n",
    "torch_image = torch.unsqueeze(image, 0).to(dtype=data_type).to(device)\n",
    "imshow(torch_image[0][0])\n",
    "print(f\"The label is {label}\")\n",
    "\n",
    "if torch.argmax(nn(torch_image)).item() == label:\n",
    "    print(\"NN classifies image correctly as {}\".format(label))\n",
    "else:\n",
    "    print(\"NN classifies image wrong\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:01.979234673Z",
     "start_time": "2024-05-02T23:40:01.935262123Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "total_correct = 0\n",
    "total_wrong = 0\n",
    "do_test = False\n",
    "if do_test:\n",
    "    for image, label in training_data:\n",
    "        torch_image = torch.unsqueeze(image, 0).to(dtype=data_type).to(device)\n",
    "        if torch.argmax(nn(torch_image)).item() == label:\n",
    "            total_correct += 1\n",
    "        else:\n",
    "            total_wrong += 1\n",
    "            \n",
    "        print(total_correct, total_wrong)\n",
    "    print(f\"accuracy {total_correct/ len(training_data)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.024714022Z",
     "start_time": "2024-05-02T23:40:01.980542161Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Add the objective to the optimization problem\n",
    "in this case we want to find the maximum difference between any one neuron and the target neuron (label)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def add_output_constraints(model, nn_layer_out_bounds, label, output_vars, sovler_bound=1e-3):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param model: the optimization problem in gurobi encoding the NN\n",
    "    :param nn_layer_out_bounds: torch.Tensor, approximating the upper and lower bounding the output layer of the NN\n",
    "    :param label: index of the label or target neuron which is compared against\n",
    "    :param output_vars: the gurobi variables from the model of the NN describing the output neurons of the NN\n",
    "    :param sovler_bound: provides a bound for the gurobi solver. If this bound is achieved, the optimizer stops\n",
    "    \"\"\"\n",
    "    \n",
    "    out_lb = nn_layer_out_bounds[-1][0].detach().cpu().numpy()\n",
    "    out_ub = nn_layer_out_bounds[-1][1].detach().cpu().numpy()\n",
    "    \n",
    "    difference_lb = out_lb - out_ub[label]\n",
    "    difference_ub = out_ub - out_lb[label]\n",
    "    difference_lb = difference_lb.tolist()\n",
    "    difference_ub = difference_ub.tolist()\n",
    "    \n",
    "    difference_lb.pop(label)\n",
    "    difference_ub.pop(label)\n",
    "    \n",
    "    min_diff = min(difference_lb)\n",
    "    max_diff = max(difference_ub)\n",
    "    \n",
    "    difference = model.addVars(9, lb=difference_lb, ub=difference_ub, name=\"diff_var\")\n",
    "    model.addConstrs((difference[i] == output_vars.tolist()[i] - output_vars.tolist()[label] for i in range(0, label)), name=\"diff_const0\")\n",
    "    model.addConstrs((difference[i - 1] == output_vars.tolist()[i] - output_vars.tolist()[label] for i in range(label + 1, 10)), name=\"diff_const1\")\n",
    "\n",
    "    max_var = model.addVar(lb=min_diff, ub=max_diff, name=\"max_var\")\n",
    "    model.addConstr(max_var == grp.max_(difference))\n",
    "\n",
    "    if sovler_bound != None:\n",
    "        model.setParam(\"BestObjStop\", sovler_bound)\n",
    "\n",
    "    model.update()\n",
    "    model.setObjective(max_var, grp.GRB.MAXIMIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.025333871Z",
     "start_time": "2024-05-02T23:40:02.023207484Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get the output variables by name \n",
    "given the gurobi model, the output size of the NN and the index of output layer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def get_output_vars_snv_milp(model, output_size, output_layer_index):\n",
    "    output_vars = []\n",
    "    for i in range(output_size):\n",
    "        output_vars.append(model.getVarByName(\"affine_var{}[{}]\".format(output_layer_index, i)))\n",
    "    output_vars = grp.MVar.fromlist(output_vars)\n",
    "    return output_vars"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.025842470Z",
     "start_time": "2024-05-02T23:40:02.023307864Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def get_output_vars_dhov(model, output_size, output_layer_index):\n",
    "    output_vars = []\n",
    "    for i in range(output_size):\n",
    "        output_vars.append(model.getVarByName(\"output_layer_[{}]_[{}]\".format(output_layer_index, i)))\n",
    "    output_vars = grp.MVar.fromlist(output_vars)\n",
    "    return output_vars"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.026332699Z",
     "start_time": "2024-05-02T23:40:02.023339074Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Defining function to call Gurobi optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def optimize_model(model, output_vars):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param model: the optimization problem in gurobi encoding the NN and the objective \n",
    "    :param output_vars: the gurobi variables from the model of the NN describing the output neurons of the NN\n",
    "    :return True if verification was successful, else false \n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    model.update()\n",
    "    model.optimize()\n",
    "    \n",
    "    if model.Status == grp.GRB.OPTIMAL or model.Status == grp.GRB.USER_OBJ_LIMIT:\n",
    "        \n",
    "        for i, var in enumerate(output_vars.tolist()):\n",
    "            print(\"var {}: {}\".format(i, var.getAttr(\"x\")))\n",
    "        max_var = model.getVarByName(\"max_var\").getAttr(\"x\")\n",
    "        \n",
    "        if max_var < 0:\n",
    "            print(\"property verified with max difference {}\".format(max_var))\n",
    "            return True\n",
    "        else:\n",
    "             print(\"property NOT verified with max difference {}\".format(max_var))\n",
    "             return False\n",
    "\n",
    "    elif model.Status == grp.GRB.INFEASIBLE:\n",
    "        print(\"model infeasible\")\n",
    "\n",
    "        model.computeIIS()\n",
    "        print(\"constraint\")\n",
    "        all_constr = model.getConstrs()\n",
    "\n",
    "        for const in all_constr:\n",
    "            if const.IISConstr:\n",
    "                print(\"{}\".format(const))\n",
    "\n",
    "        print(\"lower bound\")\n",
    "        all_var = model.getVars()\n",
    "        for var in all_var:\n",
    "            if var.IISLB:\n",
    "                print(\"{}, lb: {}, ub: {}\".format(var, var.getAttr(\"lb\"), var.getAttr(\"ub\")))\n",
    "\n",
    "        print(\"upper bound\")\n",
    "        all_var = model.getVars()\n",
    "        for var in all_var:\n",
    "            if var.IISUB:\n",
    "                print(\"{}, lb: {}, ub: {}\".format(var, var.getAttr(\"lb\"), var.getAttr(\"ub\")))\n",
    "\n",
    "    print(\"time to optimize: {}\".format(time.time() - start))\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.026802569Z",
     "start_time": "2024-05-02T23:40:02.023363144Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Obtain input bounds from vnn specification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ufuk/miniconda3/envs/autolirpa_icnn/lib/python3.10/site-packages/vnnlib/compat.py:283: UserWarning: literal negation does not strictly follow SMT-LIB\n",
      "  ast_node = parse_file(vnnlib_filename, strict=False)\n"
     ]
    }
   ],
   "source": [
    "def load_vnnlib_bounds(vnnlib_path, input_shape, n_out):\n",
    "    n_in = np.prod(input_shape)\n",
    "    res = read_vnnlib_simple(vnnlib_path, n_in, n_out)\n",
    "    bnds, spec = res[0]\n",
    "    \n",
    "    bnds = np.array(bnds)\n",
    "    lbs = bnds[:,0]\n",
    "    ubs = bnds[:,1]\n",
    "    \n",
    "    data_min = torch.tensor(lbs, dtype=data_type).reshape(input_shape).to(device)\n",
    "    data_max = torch.tensor(ubs, dtype=data_type).reshape(input_shape).to(device)\n",
    "\n",
    "    return [data_min, data_max]\n",
    "\n",
    "vnnlib_path = 'prop_0_0.03.vnnlib'\n",
    "input_bounds = load_vnnlib_bounds(vnnlib_path, [784,], 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.073025736Z",
     "start_time": "2024-05-02T23:40:02.023429873Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate box-bounds to provide a rough approximation of NN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1032.5533447265625, -872.9257202148438, -1141.9591064453125, -1203.2977294921875, -1105.7109375, -986.7314453125, -954.3194580078125, -1036.1492919921875, -1236.6732177734375, -945.4200439453125]\n",
      "[822.8641967773438, 734.7686157226562, 739.7653198242188, 644.5771484375, 858.5091552734375, 782.4144287109375, 917.3555908203125, 876.5300903320312, 868.9226684570312, 935.656494140625]\n"
     ]
    }
   ],
   "source": [
    "simple_bounds_affine_out, simple_bounds_layer_out = nn.calculate_box_bounds(input_bounds)\n",
    "print(simple_bounds_affine_out[-1][0].tolist())\n",
    "print(simple_bounds_affine_out[-1][1].tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.119296173Z",
     "start_time": "2024-05-02T23:40:02.073134296Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Do Single-Neuron-Verification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "use_snv = False\n",
    "if use_snv:\n",
    "    t = time.time()\n",
    "    \n",
    "    # define the Verifier\n",
    "    snv_verifier = SingleNeuronVerifier(nn, torch_image, input_bounds, print_log=False, print_new_bounds=True, optimize_bounds=True)\n",
    "    \n",
    "    # generate the constraints\n",
    "    snv_verifier.generate_constraints_for_net()\n",
    "    snv_model = snv_verifier.model\n",
    "    snv_model.update()\n",
    "    \n",
    "    # add the objective to the gurobi model\n",
    "    snv_out_vars = get_output_vars_snv_milp(snv_model, output_size, number_layer * 2)\n",
    "    add_output_constraints(snv_model, simple_bounds_layer_out, label, snv_out_vars, sovler_bound=None)\n",
    "    print(\"constraint generation time: {}\".format(time.time() - t))\n",
    "    \n",
    "    print(\"start with optimization\")\n",
    "    optimize_model(snv_model, snv_out_vars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.120371882Z",
     "start_time": "2024-05-02T23:40:02.119183214Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " # Do MILP-Verification if wanted"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "if use_milp:\n",
    "    # define the Verifier\n",
    "    milp_verifier = MILPVerifier(nn, torch_image, input_bounds, print_log=False)\n",
    "    \n",
    "    # generate the constraints\n",
    "    milp_verifier.generate_constraints_for_net()\n",
    "    milp_model = milp_verifier.model\n",
    "    milp_model.update()\n",
    "    \n",
    "    # add the objective to the gurobi model\n",
    "    milp_out_vars = get_output_vars_snv_milp(milp_model, output_size, number_layer * 2)\n",
    "    add_output_constraints(milp_model, simple_bounds_layer_out, label, milp_out_vars)\n",
    "    \n",
    "    print(\"start with optimization\")\n",
    "    optimize_model(milp_model, milp_out_vars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.120853761Z",
     "start_time": "2024-05-02T23:40:02.119247264Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# get bounds with crown"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_onnx_model(onnx_path, input_shape):\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    torch_model = ConvertModel(onnx_model)\n",
    "    \n",
    "    x_concrete = torch.zeros(input_shape)\n",
    "    model = BoundedModule(torch_model, x_concrete)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.121315931Z",
     "start_time": "2024-05-02T23:40:02.119270754Z"
    }
   },
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ufuk/miniconda3/envs/autolirpa_icnn/lib/python3.10/site-packages/onnx2pytorch/convert/model.py:163: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not self.experimental and inputs[0].shape[self.batch_dim] > 1:\n",
      "/home/ufuk/miniconda3/envs/autolirpa_icnn/lib/python3.10/site-packages/auto_LiRPA/parse_graph.py:154: FutureWarning: 'torch.onnx.symbolic_helper._set_opset_version' is deprecated in version 1.13 and will be removed in 2.0. Please remove its usage and avoid setting internal variables directly.\n",
      "  _set_opset_version(12)\n"
     ]
    }
   ],
   "source": [
    "def load_vnnlib_spec_for_auto_lirpa(vnnlib_path, input_shape, n_out):\n",
    "    n_in = np.prod(input_shape)\n",
    "    res = read_vnnlib_simple(vnnlib_path, n_in, n_out)\n",
    "    bnds, spec = res[0]\n",
    "    \n",
    "    bnds = np.array(bnds)\n",
    "    lbs = bnds[:,0]\n",
    "    ubs = bnds[:,1]\n",
    "    \n",
    "    data_min = torch.tensor(lbs, dtype=torch.float32).reshape(input_shape)\n",
    "    data_max = torch.tensor(ubs, dtype=torch.float32).reshape(input_shape)\n",
    "    center = 0.5*(data_min + data_max)\n",
    "\n",
    "    ptb = PerturbationLpNorm(x_L=data_min, x_U=data_max)\n",
    "    x = BoundedTensor(center, ptb)\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "onnx_path = 'mnist-net_256x4.onnx'\n",
    "vnnlib_path = 'prop_0_0.03.vnnlib'\n",
    "model = load_onnx_model(onnx_path, [1,1,1,784])\n",
    "x = load_vnnlib_spec_for_auto_lirpa(vnnlib_path, [1,1,1,784], 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.122258969Z",
     "start_time": "2024-05-02T23:40:02.119292194Z"
    }
   },
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_layers(model):\n",
    "    return [l for l in model.nodes() if l.perturbed]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.122717299Z",
     "start_time": "2024-05-02T23:40:02.119350753Z"
    }
   },
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_intermediate_bounds(model):\n",
    "    \"\"\"\n",
    "    Returns a dictionary containing the concrete lower and upper bounds of each layer.\n",
    "    \n",
    "    Implemented own method to filter out bounds for weight matrices.\n",
    "    \n",
    "    Only call this method after compute_bounds()!\n",
    "    \"\"\"\n",
    "    od = OrderedDict()\n",
    "    for l in get_layers(model):\n",
    "        if hasattr(l, 'lower'):\n",
    "            od[l.name] = (l.lower, l.upper)\n",
    "            \n",
    "    return od"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.123193648Z",
     "start_time": "2024-05-02T23:40:02.119370113Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[-16.1574, -13.2932, -17.4020, -26.5492, -17.5962, -16.9601, -15.3861,\n          -17.6454, -20.9628, -15.7071]], grad_fn=<ViewBackward0>),\n tensor([[13.8042, 12.8868, 11.2402,  7.7739, 14.1580, 12.1219, 15.4775, 13.8468,\n          12.8768, 15.7155]], grad_fn=<ViewBackward0>))"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compute_bounds(x=(x,), method='ibp')\n",
    "model.compute_bounds(x=(x,), method='crown')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.164363613Z",
     "start_time": "2024-05-02T23:40:02.119395883Z"
    }
   },
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bounds_dict_crown = get_intermediate_bounds(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.164955992Z",
     "start_time": "2024-05-02T23:40:02.155778104Z"
    }
   },
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/0\n",
      "/input\n",
      "/input.3\n",
      "/input.7\n",
      "/input.11\n",
      "/30\n"
     ]
    }
   ],
   "source": [
    "bounds_dict_crown.keys()\n",
    "crown_bounds_affine_out = []\n",
    "for i, key in enumerate(bounds_dict_crown.keys()):\n",
    "    print(key)\n",
    "    if i == 0: # use this if ibp is used (or i % 2 == 1:)\n",
    "        continue\n",
    "    lb, ub = bounds_dict_crown[key]\n",
    "    crown_bounds_affine_out.append([lb.type(data_type).view(-1).to(device), ub.type(data_type).view(-1).to(device)])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.203266610Z",
     "start_time": "2024-05-02T23:40:02.157667022Z"
    }
   },
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crown_bounds_layer_out = []\n",
    "relu = torch.nn.ReLU()\n",
    "for i, (lb, ub) in enumerate(crown_bounds_affine_out):\n",
    "    if i == len(crown_bounds_affine_out) - 1:\n",
    "        crown_bounds_layer_out.append([lb, ub])\n",
    "    else:\n",
    "        lb_layer = relu(lb)\n",
    "        ub_layer = relu(ub)\n",
    "        crown_bounds_layer_out.append([lb_layer, ub_layer])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.203848849Z",
     "start_time": "2024-05-02T23:40:02.203164550Z"
    }
   },
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-16.1574, -13.2932, -17.4020, -26.5492, -17.5962, -16.9601, -15.3861,\n",
      "        -17.6454, -20.9628, -15.7071], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(crown_bounds_affine_out[-1][0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.204798548Z",
     "start_time": "2024-05-02T23:40:02.203205060Z"
    }
   },
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-186.3933, -166.1221, -133.4717, -167.3497, -141.4671, -291.8646,\n",
      "        -145.1084, -152.4255, -105.8477, -119.2242, -158.8032, -335.8949,\n",
      "        -126.8841, -106.2285, -100.5970, -144.9053, -128.5827, -216.1721,\n",
      "        -104.5580, -176.2673, -241.1868, -135.9195, -116.9115, -149.1140,\n",
      "        -172.8038, -109.1281, -167.3485, -276.8488, -105.1384, -172.5289,\n",
      "        -128.1274, -281.1724, -120.7568, -179.6152, -190.0453, -197.2408,\n",
      "        -175.8437, -137.7844,  -88.1701, -168.7748, -128.0328, -165.3708,\n",
      "        -175.1399, -227.6097, -131.5107, -129.4820, -113.5110, -127.5611,\n",
      "        -223.4063, -121.9743, -151.3296, -150.1186, -121.7823, -129.9400,\n",
      "        -190.6909, -152.7667, -126.3819, -123.5460, -159.6351, -241.9066,\n",
      "        -114.8180, -288.5023, -137.2865, -112.5156, -162.7558, -157.9243,\n",
      "        -237.5894, -131.1928, -186.1840, -109.7676, -165.9870, -158.3168,\n",
      "        -300.4587, -124.3646, -362.5540, -122.0489,  -99.1765, -143.4968,\n",
      "        -151.3960, -114.2471, -112.7730, -112.5732, -115.4081, -172.6636,\n",
      "        -103.8998, -128.5794, -176.1133, -158.8269, -142.1364, -133.9637,\n",
      "        -133.7865, -194.0288, -137.0399, -150.1549, -199.1740, -169.8477,\n",
      "        -179.7442, -175.4718, -101.4554, -151.2301, -121.8008, -122.6872,\n",
      "        -173.3127, -257.5265, -162.9561, -250.8247, -104.3047, -238.6204,\n",
      "        -130.5028, -265.8862, -173.2680, -156.1561, -112.3295, -138.3398,\n",
      "        -168.4478, -119.3255, -143.3328, -100.6396, -279.3831, -164.8405,\n",
      "        -223.1750, -325.2364, -157.6850, -117.8452, -147.6275, -109.8071,\n",
      "        -252.7931, -198.2370, -136.8897, -177.9918, -132.0563, -117.0051,\n",
      "        -126.9812, -115.2443, -126.4513, -111.3819, -157.5078, -176.5843,\n",
      "        -131.3982, -134.6679, -103.0494, -135.0540, -203.2134, -134.6520,\n",
      "        -113.3442, -190.0199, -246.3852, -159.2128, -134.3102, -114.0819,\n",
      "        -155.0182, -116.1482, -248.5558, -127.7926, -134.6300, -227.1548,\n",
      "        -229.2332, -230.4422, -154.5544, -160.6041, -111.6315,  -87.9451,\n",
      "        -108.1292, -134.9607, -135.3442, -108.3046, -272.9388, -157.0391,\n",
      "        -156.4060, -148.9137, -117.1439, -150.3956, -117.2138, -113.3516,\n",
      "        -272.6296, -172.5640, -136.0556, -136.5133, -188.2690, -117.2945,\n",
      "        -173.6996, -108.6314, -108.6978, -150.9273, -117.8900, -141.8904,\n",
      "         -98.4711,  -98.0646, -186.1697, -146.9012, -201.0540, -183.6186,\n",
      "        -306.1071,  -95.1792, -124.3484, -169.9929, -104.2702, -118.2161,\n",
      "        -165.1493, -116.9200, -285.6527, -130.5675, -115.7462, -226.7602,\n",
      "        -116.0455, -279.3685, -108.1502, -141.8006, -180.7984, -172.1093,\n",
      "        -161.7695, -291.1242, -150.7416, -262.0826, -243.3222, -284.6442,\n",
      "         -95.4991, -153.8577, -114.9658, -160.6414, -195.6163, -164.5998,\n",
      "        -121.8726, -226.1636, -143.7778, -106.2142, -260.8814, -139.2729,\n",
      "        -126.0475, -115.9344, -107.1181, -179.5000, -175.5242, -139.7872,\n",
      "        -141.2713, -146.0680, -129.3048, -120.7368, -105.3489, -180.2963,\n",
      "        -179.1039, -123.8305, -135.8887,  -98.3802, -234.0404, -176.5208,\n",
      "        -114.3593, -130.6487, -124.5879, -232.6939, -205.0162, -193.3223,\n",
      "        -320.6713, -131.7776, -110.7680, -131.9771], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(simple_bounds_affine_out[2][0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.205288507Z",
     "start_time": "2024-05-02T23:40:02.203226320Z"
    }
   },
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'snv_verifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[28], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m snv_affine_out, snv_layer_out \u001B[38;5;241m=\u001B[39m \u001B[43msnv_verifier\u001B[49m\u001B[38;5;241m.\u001B[39mbounds_affine_out, snv_verifier\u001B[38;5;241m.\u001B[39mbounds_layer_out\n",
      "\u001B[0;31mNameError\u001B[0m: name 'snv_verifier' is not defined"
     ]
    }
   ],
   "source": [
    "snv_affine_out, snv_layer_out = snv_verifier.bounds_affine_out, snv_verifier.bounds_layer_out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.414251454Z",
     "start_time": "2024-05-02T23:40:02.203269100Z"
    }
   },
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "print(snv_affine_out[-1][1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.415422363Z",
     "start_time": "2024-05-02T23:40:02.414654504Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "for (lb_crown, ub_crown), (lb_simple, ub_simple) in zip(crown_bounds_affine_out, simple_bounds_affine_out):\n",
    "    print(torch.isclose(lb_crown, lb_simple).all())\n",
    "    print(torch.isclose(ub_crown, ub_simple).all())\n",
    "    print(\"===============\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:02.455172579Z",
     "start_time": "2024-05-02T23:40:02.455111169Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Attempt verification using DHOV\n",
    "See MultiDHOV.py and ICNNFactory.py for detailed documentation about the parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2025-02-14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ufuk/Documents/Programming/ICNN_verification/script/DHOV/MultiDHOV.py:152: UserWarning: value for group number multiplier is given with grouping method consecutive. consecutive grouping does not use variable number of groups\n",
      "  warnings.warn(\"value for group number multiplier is given with grouping method consecutive. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "approximation of layer: 0\n",
      "    number of fixed neurons for current layer: 202\n",
      "        time for sampling: 2.2529351711273193\n",
      "    layer progress, group 1 of 3 \n",
      "        time for training: 12.489895343780518\n",
      "        actual verification time 0.11947226524353027\n",
      "        time for verification: 0.20722627639770508\n",
      "    layer progress, group 2 of 3 \n",
      "        time for training: 0.4374535083770752\n",
      "        actual verification time 0.11164045333862305\n",
      "        time for verification: 0.20134210586547852\n",
      "    layer progress, group 3 of 3 \n",
      "        time for training: 0.42087745666503906\n",
      "        actual verification time 0.026474475860595703\n",
      "        time for verification: 0.08197426795959473\n",
      "\n",
      "approximation of layer: 1\n",
      "    time for icnn_bound calculation: 4.326870679855347\n",
      "    number of fixed neurons for current layer: 173\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 23\u001B[0m\n\u001B[1;32m     21\u001B[0m overall_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m     22\u001B[0m dhov_verifier \u001B[38;5;241m=\u001B[39m multidhov\u001B[38;5;241m.\u001B[39mMultiDHOV()\n\u001B[0;32m---> 23\u001B[0m \u001B[43mdhov_verifier\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart_verification\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch_image\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43micnn_factory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroup_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_bounds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msampling_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43micnn_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43minit_affine_bounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbounds_affine_out_snr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minit_layer_bounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbounds_layer_out_snr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43micnn_batch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_over_approximation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbreak_after\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mtighten_bounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     28\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43muse_fixed_neurons_in_grouping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayers_as_snr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayers_as_milp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_icnn_enlargement_as_lp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mforce_inclusion_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreemptive_stop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprint_training_loss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprint_new_bounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43mgrouping_method\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mconsecutive\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgroup_num_multiplier\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstore_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprint_optimization_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprint_last_loss\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m                                 \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43madam\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minit_network\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madapt_lambda\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mincluded\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverall time: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(time\u001B[38;5;241m.\u001B[39mtime() \u001B[38;5;241m-\u001B[39m overall_time))\n",
      "File \u001B[0;32m~/Documents/Programming/ICNN_verification/script/DHOV/MultiDHOV.py:238\u001B[0m, in \u001B[0;36mMultiDHOV.start_verification\u001B[0;34m(self, nn, input, icnn_factory, group_size, input_bounds, sampling_strategy, icnn_batch_size, icnn_epochs, hyper_lambda, init_affine_bounds, init_layer_bounds, break_after, tighten_bounds, use_fixed_neurons_in_grouping, layers_as_milp, layers_as_snr, use_over_approximation, skip_last_layer, preemptive_stop, store_samples, encode_icnn_enlargement_as_lp, encode_relu_enlargement_as_lp, force_inclusion_steps, grouping_method, group_num_multiplier, init_network, adapt_lambda, optimizer, print_training_loss, print_last_loss, print_optimization_steps, print_new_bounds)\u001B[0m\n\u001B[1;32m    235\u001B[0m total_neurons_fixed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(fix_lower) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlen\u001B[39m(fix_upper)\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    number of fixed neurons for current layer: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(total_neurons_fixed))\n\u001B[0;32m--> 238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtotal_neurons_fixed\u001B[49m \u001B[38;5;241m==\u001B[39m total_number_of_neurons_in_current_layer:\n\u001B[1;32m    239\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m current_layer_index \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m layers_as_milp \u001B[38;5;129;01mor\u001B[39;00m current_layer_index \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m layers_as_snr:\n\u001B[1;32m    240\u001B[0m         layers_as_milp\u001B[38;5;241m.\u001B[39mappend(current_layer_index)\n",
      "File \u001B[0;32m~/Documents/Programming/ICNN_verification/script/DHOV/MultiDHOV.py:238\u001B[0m, in \u001B[0;36mMultiDHOV.start_verification\u001B[0;34m(self, nn, input, icnn_factory, group_size, input_bounds, sampling_strategy, icnn_batch_size, icnn_epochs, hyper_lambda, init_affine_bounds, init_layer_bounds, break_after, tighten_bounds, use_fixed_neurons_in_grouping, layers_as_milp, layers_as_snr, use_over_approximation, skip_last_layer, preemptive_stop, store_samples, encode_icnn_enlargement_as_lp, encode_relu_enlargement_as_lp, force_inclusion_steps, grouping_method, group_num_multiplier, init_network, adapt_lambda, optimizer, print_training_loss, print_last_loss, print_optimization_steps, print_new_bounds)\u001B[0m\n\u001B[1;32m    235\u001B[0m total_neurons_fixed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(fix_lower) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlen\u001B[39m(fix_upper)\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m    number of fixed neurons for current layer: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(total_neurons_fixed))\n\u001B[0;32m--> 238\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtotal_neurons_fixed\u001B[49m \u001B[38;5;241m==\u001B[39m total_number_of_neurons_in_current_layer:\n\u001B[1;32m    239\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m current_layer_index \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m layers_as_milp \u001B[38;5;129;01mor\u001B[39;00m current_layer_index \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m layers_as_snr:\n\u001B[1;32m    240\u001B[0m         layers_as_milp\u001B[38;5;241m.\u001B[39mappend(current_layer_index)\n",
      "File \u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:755\u001B[0m, in \u001B[0;36mPyDBFrame.trace_dispatch\u001B[0;34m(self, frame, event, arg)\u001B[0m\n\u001B[1;32m    753\u001B[0m \u001B[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001B[39;00m\n\u001B[1;32m    754\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m info\u001B[38;5;241m.\u001B[39mpydev_state \u001B[38;5;241m==\u001B[39m STATE_SUSPEND:\n\u001B[0;32m--> 755\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    756\u001B[0m     \u001B[38;5;66;03m# No need to reset frame.f_trace to keep the same trace function.\u001B[39;00m\n\u001B[1;32m    757\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrace_dispatch\n",
      "File \u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python/helpers/pydev/_pydevd_bundle/pydevd_frame.py:412\u001B[0m, in \u001B[0;36mPyDBFrame.do_wait_suspend\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    411\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdo_wait_suspend\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 412\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_args\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python/helpers/pydev/pydevd.py:1184\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1181\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1183\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1184\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/share/JetBrains/Toolbox/apps/pycharm-professional/plugins/python/helpers/pydev/pydevd.py:1199\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1196\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1198\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1199\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1201\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1203\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from script.DHOV.Sampling.PerGroupLineSearchSampling import PerGroupLineSearchSamplingStrategy\n",
    "from script.DHOV.Sampling.PerGroupSamplingStrategy import PerGroupSamplingStrategy\n",
    "from script.DHOV.Sampling.PerGroupFeasibleSamplingStrategy import PerGroupFeasibleSamplingStrategy\n",
    "from script.DHOV.Sampling.ZonotopeSamplingStrategy import ZonotopeSamplingStrategy\n",
    "\n",
    "center = torch.flatten(torch_image)\n",
    "sampling_strategy = PerGroupLineSearchSamplingStrategy(center, input_bounds, nn, sample_count=1000)\n",
    "# number of neurons which are in one group\n",
    "group_size = 20\n",
    "\n",
    "# Size of each layer of the ICNN except for the input layer. (given by group size)\n",
    "net_size = [5, 1]\n",
    "\n",
    "# picking the typ of ICNN to use\n",
    "#icnn_factory = ICNNFactory(\"approx_max\", net_size, maximum_function=\"SMU\", function_parameter=0.3)\n",
    "icnn_factory = ICNNFactory(\"logical\", net_size, always_use_logical_layer=False)\n",
    "#icnn_factory = ICNNFactory(\"standard\", net_size, adapt_layer_for_init=True)\n",
    "\n",
    "bounds_affine_out_snr, bounds_layer_out_snr = crown_bounds_affine_out, crown_bounds_layer_out\n",
    "\n",
    "overall_time = time.time()\n",
    "dhov_verifier = multidhov.MultiDHOV()\n",
    "dhov_verifier.start_verification(nn, torch_image, icnn_factory, group_size, input_bounds, sampling_strategy,\n",
    "                                 icnn_epochs=100,\n",
    "                                 init_affine_bounds=bounds_affine_out_snr, init_layer_bounds=bounds_layer_out_snr,\n",
    "                                 icnn_batch_size=10000, use_over_approximation=True, break_after=None,\n",
    "                                 tighten_bounds=True, \n",
    "                                 use_fixed_neurons_in_grouping=False, layers_as_snr=[], layers_as_milp=[3], encode_icnn_enlargement_as_lp=False,\n",
    "                                 force_inclusion_steps=3, preemptive_stop=True, print_training_loss=False, print_new_bounds=False,\n",
    "                                 grouping_method=\"consecutive\", group_num_multiplier=5, store_samples=False, print_optimization_steps=False, print_last_loss=False,\n",
    "                                 optimizer=\"adam\", init_network=True, adapt_lambda=\"included\")\n",
    "print(\"overall time: {}\".format(time.time() - overall_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-05-02T23:40:32.231253323Z",
     "start_time": "2024-05-02T23:40:03.986121187Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting an overview over what DHOV has done\n",
    "show the neuron groups created by DHOV for each layer by index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0\n",
      "    Group 0: [0, 1, 8, 10, 13, 15, 29, 30, 35, 42, 61, 62, 69, 75, 78, 80, 86, 92, 93, 97]\n",
      "    Group 1: [114, 120, 127, 134, 137, 146, 156, 166, 172, 175, 176, 177, 178, 179, 182, 187, 188, 190, 194, 195]\n",
      "    Group 2: [199, 200, 201, 204, 207, 213, 216, 219, 220, 229, 230, 235, 251, 255]\n",
      "Layer 1\n",
      "    Group 0: [2, 3, 5, 6, 7, 14, 17, 20, 21, 25, 33, 42, 43, 44, 47, 49, 54, 63, 64, 65]\n",
      "    Group 1: [70, 71, 72, 74, 75, 76, 80, 82, 83, 93, 94, 95, 96, 100, 101, 103, 110, 118, 121, 123]\n",
      "    Group 2: [126, 128, 131, 132, 134, 135, 136, 140, 141, 142, 143, 154, 162, 165, 166, 181, 186, 189, 190, 191]\n",
      "    Group 3: [193, 204, 205, 208, 212, 214, 215, 217, 218, 219, 223, 225, 233, 234, 237, 239, 240, 241, 242, 246]\n",
      "    Group 4: [248, 252, 254]\n",
      "Layer 2\n",
      "    Group 0: [27, 31, 38, 43, 44, 107, 118, 157, 161, 172, 200, 229, 246]\n",
      "Layer 3\n"
     ]
    }
   ],
   "source": [
    "for i, groups_in_layer in enumerate(dhov_verifier.all_group_indices):\n",
    "    print(f\"Layer {i}\")\n",
    "    for k, group in enumerate(groups_in_layer):\n",
    "        print(f\"    Group {k}: {group}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-05-02T13:05:45.768831794Z",
     "start_time": "2024-05-02T13:05:45.761675130Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "show the output bounds for the NN. Hopefully these are better than the box-bounds and the bounds obtained by the Single-Neuron-Verification"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-16.157411575317383, -13.293179512023926, -17.402015686035156, -26.54916000366211, -17.59623908996582, -16.960063934326172, -15.386096954345703, -17.645418167114258, -20.96283721923828, -15.707077026367188]\n",
      "[13.804232597351074, 12.88681411743164, 11.240226745605469, 7.773935794830322, 14.157960891723633, 12.121850967407227, 15.477540969848633, 13.84675121307373, 12.876795768737793, 15.715452194213867]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(dhov_verifier.bounds_affine_out[-1][0].tolist())\n",
    "print(dhov_verifier.bounds_affine_out[-1][1].tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-05-02T13:05:46.531104929Z",
     "start_time": "2024-05-02T13:05:46.524305037Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Starting the optimization process"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# make changes on a copy of the gurobi model generated by DHOV\n",
    "dhov_model = dhov_verifier.nn_encoding_model.copy()\n",
    "dhov_model.update()\n",
    "dhov_out_vars = get_output_vars_dhov(dhov_model, output_size, number_layer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-05-02T13:05:47.134338254Z",
     "start_time": "2024-05-02T13:05:47.128441916Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "add_output_constraints(dhov_model, bounds_layer_out_snr, label, dhov_out_vars, sovler_bound=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-05-02T13:05:47.595849591Z",
     "start_time": "2024-05-02T13:05:47.592925237Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var 0: 0.08803591539041829\n",
      "var 1: 0.032871534096052285\n",
      "var 2: 0.030977546348321436\n",
      "var 3: -0.07476098861875573\n",
      "var 4: -0.01565874965556365\n",
      "var 5: -0.08667263719125633\n",
      "var 6: 0.0425343136263376\n",
      "var 7: -0.007148917450075662\n",
      "var 8: -0.02111829983009824\n",
      "var 9: 1.0495106219045167\n",
      "property NOT verified with max difference 1.1242716105232724\n",
      "overall time: 15.414306163787842\n"
     ]
    }
   ],
   "source": [
    "overall_time = time.time()\n",
    "optimize_model(dhov_model, dhov_out_vars)\n",
    "print(\"overall time: {}\".format(time.time() - overall_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2024-05-02T13:06:03.487291810Z",
     "start_time": "2024-05-02T13:05:47.995465512Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-05-02T11:54:27.655449620Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
