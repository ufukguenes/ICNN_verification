{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from script.NeuralNets.Networks import SequentialNN, ICNN, ICNNApproxMax, ICNNLogical\n",
    "from script.settings import device, data_type\n",
    "import script.DHOV.MultiDHOV as multidhov\n",
    "from script.Verification.Verifier import SingleNeuronVerifier, MILPVerifier, DHOVVerifier\n",
    "import gurobipy as grp\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from script.NeuralNets.ICNNFactory import ICNNFactory"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def add_max_constr(model, neuron_name):\n",
    "    neuron_var = model.getVarByName(neuron_name)\n",
    "    model.setObjective(neuron_var, grp.GRB.MAXIMIZE)\n",
    "\n",
    "def add_min_constr(model, neuron):\n",
    "    neuron_var = model.getVarByName(neuron)\n",
    "    model.setObjective(neuron_var, grp.GRB.MINIMIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def optimize_model(model, neuron_name):\n",
    "    model.update()\n",
    "    model.optimize()\n",
    "    if model.Status == grp.GRB.OPTIMAL:\n",
    "        print(\"opt value: {}\".format(model.getVarByName(neuron_name).getAttr(\"x\")))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def icnn_model(icnn, nn, input_x, eps, layer_index, from_neuron, to_neuron, print_log=False):\n",
    "    m = grp.Model()\n",
    "    if not print_log:\n",
    "        m.Params.LogToConsole = 0\n",
    "\n",
    "    input_flattened = torch.flatten(input_x)\n",
    "    bounds_affine_out, bounds_layer_out = nn.calculate_box_bounds(\n",
    "        [input_flattened.add(-eps), input_flattened.add(eps)])\n",
    "\n",
    "    parameter_list = list(nn.parameters())\n",
    "\n",
    "    input_size = len(parameter_list[2*layer_index])\n",
    "    lb = bounds_layer_out[layer_index][0].detach().cpu().numpy()\n",
    "    ub = bounds_layer_out[layer_index][1].detach().cpu().numpy()\n",
    "    in_var = m.addMVar(input_size, lb=lb, ub=ub, name=\"icnn_var\")\n",
    "\n",
    "    low = bounds_layer_out[layer_index][0][from_neuron: to_neuron]\n",
    "    up = bounds_layer_out[layer_index][1][from_neuron: to_neuron]\n",
    "    constraint_bounds_affine_out, constraint_bounds_layer_out = icnn.calculate_box_bounds([low, up])\n",
    "    icnn.add_max_output_constraints(m, in_var[from_neuron: to_neuron], constraint_bounds_affine_out, constraint_bounds_layer_out)\n",
    "\n",
    "    return m"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\"\"\"W1 = [1. 1.; 1. -1.]\n",
    "    b1 = [0., 0.]\n",
    "    W2 = [1. 1.; 1. -1.]\n",
    "    b2 = [-0.5, 0.]\n",
    "    W3 = [-1. 1.; 1. 1.]\n",
    "    b3 = [3., 0.] \"\"\"\n",
    "\n",
    "\"\"\"nn = SequentialNN([2, 2, 2, 2])\n",
    "\n",
    "with torch.no_grad():\n",
    "    parameter_list = list(nn.parameters())\n",
    "    parameter_list[0].data = torch.tensor([[1, 1], [1, -1]], dtype=data_type).to(device)\n",
    "    parameter_list[1].data = torch.tensor([0, 0], dtype=data_type).to(device)\n",
    "    parameter_list[2].data = torch.tensor([[1, 1], [1, -1]], dtype=data_type).to(device)\n",
    "    parameter_list[3].data = torch.tensor([-0.5, 0], dtype=data_type).to(device)\n",
    "    parameter_list[4].data = torch.tensor([[-1, 1], [1, 1]], dtype=data_type).to(device)\n",
    "    parameter_list[5].data = torch.tensor([3, 0], dtype=data_type).to(device)\n",
    "\n",
    "test_image = torch.tensor([[0, 0]], dtype=data_type).to(device)\"\"\"\n",
    "\n",
    "transform = Compose([ToTensor(),\n",
    "                         Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "                        )\n",
    "\n",
    "training_data = CIFAR10(root=\"../../cifar\", train=True, download=True, transform=transform)\n",
    "images, labels = training_data.__getitem__(0)\n",
    "test_image, test_label = torch.unsqueeze(images, 0).to(dtype=data_type).to(device), torch.unsqueeze(\n",
    "    torch.tensor(labels), 0).to(dtype=data_type).to(device)\n",
    "\n",
    "nn = SequentialNN([32 * 32 * 3, 1024, 512, 10])\n",
    "nn.load_state_dict(torch.load(\"../../cifar_fc.pth\", map_location=torch.device(device)), strict=False)\n",
    "parameter_list = list(nn.parameters())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n",
      "\n",
      "approximation of layer: 0\n",
      "    number of fixed neurons for current layer: 571\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2023-11-12\n",
      "    layer progress, group 1 of 76 \n",
      "        time for training: 0.008027076721191406\n",
      "        actual verification time 0.08054542541503906\n",
      "        time for verification: 0.13294005393981934\n",
      "    layer progress, group 2 of 76 \n",
      "        time for training: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ufuk\\Documents\\Programming\\ICNN_verification\\script\\Optimizer\\sdlbfgs.py:83: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1420.)\n",
      "  p.data.add_(step_size, update[offset:offset + numel].view_as(p.data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        actual verification time 0.09885644912719727\n",
      "        time for verification: 0.15926218032836914\n",
      "    layer progress, group 3 of 76 \n",
      "        time for training: 0.0020301342010498047\n",
      "        actual verification time 0.09072661399841309\n",
      "        time for verification: 0.16114139556884766\n",
      "    layer progress, group 4 of 76 \n",
      "        time for training: 0.010062694549560547\n",
      "        actual verification time 0.11076235771179199\n",
      "        time for verification: 0.17125749588012695\n",
      "    layer progress, group 5 of 76 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.13082528114318848\n",
      "        time for verification: 0.19115233421325684\n",
      "    layer progress, group 6 of 76 \n",
      "        time for training: 0.008031606674194336\n",
      "        actual verification time 0.07048678398132324\n",
      "        time for verification: 0.13282442092895508\n",
      "    layer progress, group 7 of 76 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.1116952896118164\n",
      "        time for verification: 0.17226600646972656\n",
      "    layer progress, group 8 of 76 \n",
      "        time for training: 0.007060050964355469\n",
      "        actual verification time 0.12883639335632324\n",
      "        time for verification: 0.1812589168548584\n",
      "    layer progress, group 9 of 76 \n",
      "        time for training: 0.0020253658294677734\n",
      "        actual verification time 0.07053923606872559\n",
      "        time for verification: 0.1208641529083252\n",
      "    layer progress, group 10 of 76 \n",
      "        time for training: 0.008040428161621094\n",
      "        actual verification time 0.07845664024353027\n",
      "        time for verification: 0.13080477714538574\n",
      "    layer progress, group 11 of 76 \n",
      "        time for training: 0.0020220279693603516\n",
      "        actual verification time 0.09050774574279785\n",
      "        time for verification: 0.14079904556274414\n",
      "    layer progress, group 12 of 76 \n",
      "        time for training: 0.008090734481811523\n",
      "        actual verification time 0.09856295585632324\n",
      "        time for verification: 0.16097545623779297\n",
      "    layer progress, group 13 of 76 \n",
      "        time for training: 0.0020232200622558594\n",
      "        actual verification time 0.07048583030700684\n",
      "        time for verification: 0.13089966773986816\n",
      "    layer progress, group 14 of 76 \n",
      "        time for training: 0.010096549987792969\n",
      "        actual verification time 0.08058857917785645\n",
      "        time for verification: 0.14105534553527832\n",
      "    layer progress, group 15 of 76 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.07877683639526367\n",
      "        time for verification: 0.13938117027282715\n",
      "    layer progress, group 16 of 76 \n",
      "        time for training: 0.002038240432739258\n",
      "        actual verification time 0.11097407341003418\n",
      "        time for verification: 0.19170737266540527\n",
      "    layer progress, group 17 of 76 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.10084080696105957\n",
      "        time for verification: 0.18160414695739746\n",
      "    layer progress, group 18 of 76 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.11078405380249023\n",
      "        time for verification: 0.17924714088439941\n",
      "    layer progress, group 19 of 76 \n",
      "        time for training: 0.0020384788513183594\n",
      "        actual verification time 0.10098505020141602\n",
      "        time for verification: 0.16147398948669434\n",
      "    layer progress, group 20 of 76 \n",
      "        time for training: 0.008077621459960938\n",
      "        actual verification time 0.1010138988494873\n",
      "        time for verification: 0.16340947151184082\n",
      "    layer progress, group 21 of 76 \n",
      "        time for training: 0.008039474487304688\n",
      "        actual verification time 0.07058167457580566\n",
      "        time for verification: 0.1310563087463379\n",
      "    layer progress, group 22 of 76 \n",
      "        time for training: 0.010096073150634766\n",
      "        actual verification time 0.09082984924316406\n",
      "        time for verification: 0.15128636360168457\n",
      "    layer progress, group 23 of 76 \n",
      "        time for training: 0.010051488876342773\n",
      "        actual verification time 0.06040644645690918\n",
      "        time for verification: 0.11076784133911133\n",
      "    layer progress, group 24 of 76 \n",
      "        time for training: 0.010069131851196289\n",
      "        actual verification time 0.0705420970916748\n",
      "        time for verification: 0.13103008270263672\n",
      "    layer progress, group 25 of 76 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.10068154335021973\n",
      "        time for verification: 0.16109490394592285\n",
      "    layer progress, group 26 of 76 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.09059667587280273\n",
      "        time for verification: 0.15907764434814453\n",
      "    layer progress, group 27 of 76 \n",
      "        time for training: 0.0020296573638916016\n",
      "        actual verification time 0.11090803146362305\n",
      "        time for verification: 0.18141865730285645\n",
      "    layer progress, group 28 of 76 \n",
      "        time for training: 0.00803375244140625\n",
      "        actual verification time 0.10077524185180664\n",
      "        time for verification: 0.16330718994140625\n",
      "    layer progress, group 29 of 76 \n",
      "        time for training: 0.010112524032592773\n",
      "        actual verification time 0.05858421325683594\n",
      "        time for verification: 0.10895800590515137\n",
      "    layer progress, group 30 of 76 \n",
      "        time for training: 0.01208639144897461\n",
      "        actual verification time 0.08060407638549805\n",
      "        time for verification: 0.14093565940856934\n",
      "    layer progress, group 31 of 76 \n",
      "        time for training: 0.008034467697143555\n",
      "        actual verification time 0.07775473594665527\n",
      "        time for verification: 0.14125823974609375\n",
      "    layer progress, group 32 of 76 \n",
      "        time for training: 0.0020308494567871094\n",
      "        actual verification time 0.09871983528137207\n",
      "        time for verification: 0.15930724143981934\n",
      "    layer progress, group 33 of 76 \n",
      "        time for training: 0.002032041549682617\n",
      "        actual verification time 0.07054638862609863\n",
      "        time for verification: 0.1411757469177246\n",
      "    layer progress, group 34 of 76 \n",
      "        time for training: 0.008071184158325195\n",
      "        actual verification time 0.17132282257080078\n",
      "        time for verification: 0.23378610610961914\n",
      "    layer progress, group 35 of 76 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.06244993209838867\n",
      "        time for verification: 0.14107489585876465\n",
      "    layer progress, group 36 of 76 \n",
      "        time for training: 0.01009678840637207\n",
      "        actual verification time 0.07049226760864258\n",
      "        time for verification: 0.12077927589416504\n",
      "    layer progress, group 37 of 76 \n",
      "        time for training: 0.010049581527709961\n",
      "        actual verification time 0.0684061050415039\n",
      "        time for verification: 0.13079571723937988\n",
      "    layer progress, group 38 of 76 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.07068490982055664\n",
      "        time for verification: 0.13111209869384766\n",
      "    layer progress, group 39 of 76 \n",
      "        time for training: 0.010102033615112305\n",
      "        actual verification time 0.09053516387939453\n",
      "        time for verification: 0.15097832679748535\n",
      "    layer progress, group 40 of 76 \n",
      "        time for training: 0.011144638061523438\n",
      "        actual verification time 0.09073042869567871\n",
      "        time for verification: 0.16015625\n",
      "    layer progress, group 41 of 76 \n",
      "        time for training: 0.010048866271972656\n",
      "        actual verification time 0.10875225067138672\n",
      "        time for verification: 0.16916441917419434\n",
      "    layer progress, group 42 of 76 \n",
      "        time for training: 0.0020208358764648438\n",
      "        actual verification time 0.0725409984588623\n",
      "        time for verification: 0.14092230796813965\n",
      "    layer progress, group 43 of 76 \n",
      "        time for training: 0.010045051574707031\n",
      "        actual verification time 0.06034660339355469\n",
      "        time for verification: 0.11065673828125\n",
      "    layer progress, group 44 of 76 \n",
      "        time for training: 0.010051727294921875\n",
      "        actual verification time 0.030210494995117188\n",
      "        time for verification: 0.08858060836791992\n",
      "    layer progress, group 45 of 76 \n",
      "        time for training: 0.002031087875366211\n",
      "        actual verification time 0.07857799530029297\n",
      "        time for verification: 0.14910364151000977\n",
      "    layer progress, group 46 of 76 \n",
      "        time for training: 0.0020220279693603516\n",
      "        actual verification time 0.0928349494934082\n",
      "        time for verification: 0.24190020561218262\n",
      "    layer progress, group 47 of 76 \n",
      "        time for training: 0.010137319564819336\n",
      "        actual verification time 0.13907527923583984\n",
      "        time for verification: 0.2118055820465088\n",
      "    layer progress, group 48 of 76 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.0806736946105957\n",
      "        time for verification: 0.15127873420715332\n",
      "    layer progress, group 49 of 76 \n",
      "        time for training: 0.010066747665405273\n",
      "        actual verification time 0.16103744506835938\n",
      "        time for verification: 0.22147059440612793\n",
      "    layer progress, group 50 of 76 \n",
      "        time for training: 0.00806283950805664\n",
      "        actual verification time 0.06856775283813477\n",
      "        time for verification: 0.14118695259094238\n",
      "    layer progress, group 51 of 76 \n",
      "        time for training: 0.002032756805419922\n",
      "        actual verification time 0.08056211471557617\n",
      "        time for verification: 0.16120624542236328\n",
      "    layer progress, group 52 of 76 \n",
      "        time for training: 0.008066892623901367\n",
      "        actual verification time 0.0625162124633789\n",
      "        time for verification: 0.12289094924926758\n",
      "    layer progress, group 53 of 76 \n",
      "        time for training: 0.010063648223876953\n",
      "        actual verification time 0.11094522476196289\n",
      "        time for verification: 0.18157219886779785\n",
      "    layer progress, group 54 of 76 \n",
      "        time for training: 0.00806283950805664\n",
      "        actual verification time 0.12775635719299316\n",
      "        time for verification: 0.1921989917755127\n",
      "    layer progress, group 55 of 76 \n",
      "        time for training: 0.002028226852416992\n",
      "        actual verification time 0.11083483695983887\n",
      "        time for verification: 0.17122435569763184\n",
      "    layer progress, group 56 of 76 \n",
      "        time for training: 0.008028984069824219\n",
      "        actual verification time 0.11104679107666016\n",
      "        time for verification: 0.17360568046569824\n",
      "    layer progress, group 57 of 76 \n",
      "        time for training: 0.010072469711303711\n",
      "        actual verification time 0.14932751655578613\n",
      "        time for verification: 0.23009586334228516\n",
      "    layer progress, group 58 of 76 \n",
      "        time for training: 0.002032756805419922\n",
      "        actual verification time 0.07264208793640137\n",
      "        time for verification: 0.1512758731842041\n",
      "    layer progress, group 59 of 76 \n",
      "        time for training: 0.010125875473022461\n",
      "        actual verification time 0.09322690963745117\n",
      "        time for verification: 0.18151307106018066\n",
      "    layer progress, group 60 of 76 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.12097716331481934\n",
      "        time for verification: 0.20171499252319336\n",
      "    layer progress, group 61 of 76 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.13164281845092773\n",
      "        time for verification: 0.21253156661987305\n",
      "    layer progress, group 62 of 76 \n",
      "        time for training: 0.008088111877441406\n",
      "        actual verification time 0.07277822494506836\n",
      "        time for verification: 0.16372299194335938\n",
      "    layer progress, group 63 of 76 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.08859848976135254\n",
      "        time for verification: 0.1610400676727295\n",
      "    layer progress, group 64 of 76 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.08053421974182129\n",
      "        time for verification: 0.14095711708068848\n",
      "    layer progress, group 65 of 76 \n",
      "        time for training: 0.008026599884033203\n",
      "        actual verification time 0.1515212059020996\n",
      "        time for verification: 0.21392393112182617\n",
      "    layer progress, group 66 of 76 \n",
      "        time for training: 0.008040666580200195\n",
      "        actual verification time 0.13151955604553223\n",
      "        time for verification: 0.21431183815002441\n",
      "    layer progress, group 67 of 76 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.26294827461242676\n",
      "        time for verification: 0.34378981590270996\n",
      "    layer progress, group 68 of 76 \n",
      "        time for training: 0.008038759231567383\n",
      "        actual verification time 0.07682418823242188\n",
      "        time for verification: 0.1536543369293213\n",
      "    layer progress, group 69 of 76 \n",
      "        time for training: 0.008024930953979492\n",
      "        actual verification time 0.16135835647583008\n",
      "        time for verification: 0.22374773025512695\n",
      "    layer progress, group 70 of 76 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.08069777488708496\n",
      "        time for verification: 0.15132951736450195\n",
      "    layer progress, group 71 of 76 \n",
      "        time for training: 0.010072946548461914\n",
      "        actual verification time 0.13102412223815918\n",
      "        time for verification: 0.20153164863586426\n",
      "    layer progress, group 72 of 76 \n",
      "        time for training: 0.010133504867553711\n",
      "        actual verification time 0.09079384803771973\n",
      "        time for verification: 0.1613140106201172\n",
      "    layer progress, group 73 of 76 \n",
      "        time for training: 0.010057210922241211\n",
      "        actual verification time 0.08070921897888184\n",
      "        time for verification: 0.14118576049804688\n",
      "    layer progress, group 74 of 76 \n",
      "        time for training: 0.00806736946105957\n",
      "        actual verification time 0.0706019401550293\n",
      "        time for verification: 0.14333534240722656\n",
      "    layer progress, group 75 of 76 \n",
      "        time for training: 0.010057926177978516\n",
      "        actual verification time 0.12834453582763672\n",
      "        time for verification: 0.19990062713623047\n",
      "    layer progress, group 76 of 76 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.07561206817626953\n",
      "        time for verification: 0.1325242519378662\n",
      "    time for icnn_bound calculation: 2.4376766681671143\n",
      "    time for regrouping method: 0.0\n",
      "\n",
      "approximation of layer: 1\n",
      "    number of fixed neurons for current layer: 0\n",
      "    layer progress, group 1 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.13935112953186035\n",
      "        time for verification: 0.21010231971740723\n",
      "    layer progress, group 2 of 86 \n",
      "        time for training: 0.0020470619201660156\n",
      "        actual verification time 0.1792893409729004\n",
      "        time for verification: 0.24192285537719727\n",
      "    layer progress, group 3 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.17149639129638672\n",
      "        time for verification: 0.23192071914672852\n",
      "    layer progress, group 4 of 86 \n",
      "        time for training: 0.008065938949584961\n",
      "        actual verification time 0.21196866035461426\n",
      "        time for verification: 0.2542867660522461\n",
      "    layer progress, group 5 of 86 \n",
      "        time for training: 0.008080720901489258\n",
      "        actual verification time 0.15114259719848633\n",
      "        time for verification: 0.20348095893859863\n",
      "    layer progress, group 6 of 86 \n",
      "        time for training: 0.008037567138671875\n",
      "        actual verification time 0.17168521881103516\n",
      "        time for verification: 0.2447957992553711\n",
      "    layer progress, group 7 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.2118823528289795\n",
      "        time for verification: 0.2723972797393799\n",
      "    layer progress, group 8 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.15930914878845215\n",
      "        time for verification: 0.22181391716003418\n",
      "    layer progress, group 9 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.1511380672454834\n",
      "        time for verification: 0.21166181564331055\n",
      "    layer progress, group 10 of 86 \n",
      "        time for training: 0.010118722915649414\n",
      "        actual verification time 0.17145776748657227\n",
      "        time for verification: 0.23201394081115723\n",
      "    layer progress, group 11 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.17130136489868164\n",
      "        time for verification: 0.2318878173828125\n",
      "    layer progress, group 12 of 86 \n",
      "        time for training: 0.00803685188293457\n",
      "        actual verification time 0.1411755084991455\n",
      "        time for verification: 0.20369648933410645\n",
      "    layer progress, group 13 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.1128532886505127\n",
      "        time for verification: 0.18135976791381836\n",
      "    layer progress, group 14 of 86 \n",
      "        time for training: 0.008097410202026367\n",
      "        actual verification time 0.17172551155090332\n",
      "        time for verification: 0.23215246200561523\n",
      "    layer progress, group 15 of 86 \n",
      "        time for training: 0.0020411014556884766\n",
      "        actual verification time 0.1822216510772705\n",
      "        time for verification: 0.25301265716552734\n",
      "    layer progress, group 16 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.16916823387145996\n",
      "        time for verification: 0.2315809726715088\n",
      "    layer progress, group 17 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.181471586227417\n",
      "        time for verification: 0.2520732879638672\n",
      "    layer progress, group 18 of 86 \n",
      "        time for training: 0.008053064346313477\n",
      "        actual verification time 0.1814889907836914\n",
      "        time for verification: 0.24442791938781738\n",
      "    layer progress, group 19 of 86 \n",
      "        time for training: 0.008064508438110352\n",
      "        actual verification time 0.1712477207183838\n",
      "        time for verification: 0.24186325073242188\n",
      "    layer progress, group 20 of 86 \n",
      "        time for training: 0.0020360946655273438\n",
      "        actual verification time 0.13312745094299316\n",
      "        time for verification: 0.19147634506225586\n",
      "    layer progress, group 21 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.20193004608154297\n",
      "        time for verification: 0.2623417377471924\n",
      "    layer progress, group 22 of 86 \n",
      "        time for training: 0.010089635848999023\n",
      "        actual verification time 0.1715240478515625\n",
      "        time for verification: 0.23201584815979004\n",
      "    layer progress, group 23 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.3330047130584717\n",
      "        time for verification: 0.4036543369293213\n",
      "    layer progress, group 24 of 86 \n",
      "        time for training: 0.008040428161621094\n",
      "        actual verification time 0.22185659408569336\n",
      "        time for verification: 0.2942969799041748\n",
      "    layer progress, group 25 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.17338109016418457\n",
      "        time for verification: 0.33284783363342285\n",
      "    layer progress, group 26 of 86 \n",
      "        time for training: 0.008023977279663086\n",
      "        actual verification time 0.14104771614074707\n",
      "        time for verification: 0.18348431587219238\n",
      "    layer progress, group 27 of 86 \n",
      "        time for training: 0.010139942169189453\n",
      "        actual verification time 0.18133878707885742\n",
      "        time for verification: 0.25194525718688965\n",
      "    layer progress, group 28 of 86 \n",
      "        time for training: 0.010092020034790039\n",
      "        actual verification time 0.18951654434204102\n",
      "        time for verification: 0.2418534755706787\n",
      "    layer progress, group 29 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.18181848526000977\n",
      "        time for verification: 0.25031495094299316\n",
      "    layer progress, group 30 of 86 \n",
      "        time for training: 0.0020329952239990234\n",
      "        actual verification time 0.17139601707458496\n",
      "        time for verification: 0.23189592361450195\n",
      "    layer progress, group 31 of 86 \n",
      "        time for training: 0.008036375045776367\n",
      "        actual verification time 0.23200368881225586\n",
      "        time for verification: 0.29459095001220703\n",
      "    layer progress, group 32 of 86 \n",
      "        time for training: 0.008023977279663086\n",
      "        actual verification time 0.19175314903259277\n",
      "        time for verification: 0.2623021602630615\n",
      "    layer progress, group 33 of 86 \n",
      "        time for training: 0.002026081085205078\n",
      "        actual verification time 0.19150042533874512\n",
      "        time for verification: 0.2599966526031494\n",
      "    layer progress, group 34 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.21379423141479492\n",
      "        time for verification: 0.2741873264312744\n",
      "    layer progress, group 35 of 86 \n",
      "        time for training: 0.008034467697143555\n",
      "        actual verification time 0.16146564483642578\n",
      "        time for verification: 0.21381402015686035\n",
      "    layer progress, group 36 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.2603492736816406\n",
      "        time for verification: 0.3329651355743408\n",
      "    layer progress, group 37 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.16122698783874512\n",
      "        time for verification: 0.23178648948669434\n",
      "    layer progress, group 38 of 86 \n",
      "        time for training: 0.008039474487304688\n",
      "        actual verification time 0.20186519622802734\n",
      "        time for verification: 0.2644228935241699\n",
      "    layer progress, group 39 of 86 \n",
      "        time for training: 0.010091304779052734\n",
      "        actual verification time 0.2522544860839844\n",
      "        time for verification: 0.30262160301208496\n",
      "    layer progress, group 40 of 86 \n",
      "        time for training: 0.008028507232666016\n",
      "        actual verification time 0.17960214614868164\n",
      "        time for verification: 0.24204111099243164\n",
      "    layer progress, group 41 of 86 \n",
      "        time for training: 0.0020351409912109375\n",
      "        actual verification time 0.1918957233428955\n",
      "        time for verification: 0.2624483108520508\n",
      "    layer progress, group 42 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.17140769958496094\n",
      "        time for verification: 0.24199295043945312\n",
      "    layer progress, group 43 of 86 \n",
      "        time for training: 0.008054494857788086\n",
      "        actual verification time 0.18958616256713867\n",
      "        time for verification: 0.2540309429168701\n",
      "    layer progress, group 44 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.20194005966186523\n",
      "        time for verification: 0.2624351978302002\n",
      "    layer progress, group 45 of 86 \n",
      "        time for training: 0.010111093521118164\n",
      "        actual verification time 0.151289701461792\n",
      "        time for verification: 0.20167136192321777\n",
      "    layer progress, group 46 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.15112709999084473\n",
      "        time for verification: 0.21954083442687988\n",
      "    layer progress, group 47 of 86 \n",
      "        time for training: 0.0020360946655273438\n",
      "        actual verification time 0.3918113708496094\n",
      "        time for verification: 0.46441125869750977\n",
      "    layer progress, group 48 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.2095165252685547\n",
      "        time for verification: 0.272106409072876\n",
      "    layer progress, group 49 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.17146801948547363\n",
      "        time for verification: 0.23198318481445312\n",
      "    layer progress, group 50 of 86 \n",
      "        time for training: 0.008034706115722656\n",
      "        actual verification time 0.2628350257873535\n",
      "        time for verification: 0.32535719871520996\n",
      "    layer progress, group 51 of 86 \n",
      "        time for training: 0.008087396621704102\n",
      "        actual verification time 0.2424793243408203\n",
      "        time for verification: 0.3130302429199219\n",
      "    layer progress, group 52 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.15920352935791016\n",
      "        time for verification: 0.22170233726501465\n",
      "    layer progress, group 53 of 86 \n",
      "        time for training: 0.0020384788513183594\n",
      "        actual verification time 0.17361140251159668\n",
      "        time for verification: 0.2320706844329834\n",
      "    layer progress, group 54 of 86 \n",
      "        time for training: 0.008038520812988281\n",
      "        actual verification time 0.1715385913848877\n",
      "        time for verification: 0.2440192699432373\n",
      "    layer progress, group 55 of 86 \n",
      "        time for training: 0.008059263229370117\n",
      "        actual verification time 0.18337154388427734\n",
      "        time for verification: 0.24383544921875\n",
      "    layer progress, group 56 of 86 \n",
      "        time for training: 0.010079145431518555\n",
      "        actual verification time 0.2121286392211914\n",
      "        time for verification: 0.28272485733032227\n",
      "    layer progress, group 57 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.1980128288269043\n",
      "        time for verification: 0.2595839500427246\n",
      "    layer progress, group 58 of 86 \n",
      "        time for training: 0.003069162368774414\n",
      "        actual verification time 0.22205448150634766\n",
      "        time for verification: 0.28251099586486816\n",
      "    layer progress, group 59 of 86 \n",
      "        time for training: 0.008064985275268555\n",
      "        actual verification time 0.17133665084838867\n",
      "        time for verification: 0.2337508201599121\n",
      "    layer progress, group 60 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.19160866737365723\n",
      "        time for verification: 0.2601909637451172\n",
      "    layer progress, group 61 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.16145706176757812\n",
      "        time for verification: 0.22205018997192383\n",
      "    layer progress, group 62 of 86 \n",
      "        time for training: 0.002032756805419922\n",
      "        actual verification time 0.16138577461242676\n",
      "        time for verification: 0.2318401336669922\n",
      "    layer progress, group 63 of 86 \n",
      "        time for training: 0.00803995132446289\n",
      "        actual verification time 0.27230310440063477\n",
      "        time for verification: 0.3347477912902832\n",
      "    layer progress, group 64 of 86 \n",
      "        time for training: 0.008032560348510742\n",
      "        actual verification time 0.20174264907836914\n",
      "        time for verification: 0.2641763687133789\n",
      "    layer progress, group 65 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.15104222297668457\n",
      "        time for verification: 0.22152161598205566\n",
      "    layer progress, group 66 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.20185351371765137\n",
      "        time for verification: 0.27248525619506836\n",
      "    layer progress, group 67 of 86 \n",
      "        time for training: 0.008044958114624023\n",
      "        actual verification time 0.25238919258117676\n",
      "        time for verification: 0.3251149654388428\n",
      "    layer progress, group 68 of 86 \n",
      "        time for training: 0.008057832717895508\n",
      "        actual verification time 0.1309828758239746\n",
      "        time for verification: 0.19351601600646973\n",
      "    layer progress, group 69 of 86 \n",
      "        time for training: 0.008063077926635742\n",
      "        actual verification time 0.19109129905700684\n",
      "        time for verification: 0.2716689109802246\n",
      "    layer progress, group 70 of 86 \n",
      "        time for training: 0.002026081085205078\n",
      "        actual verification time 0.1917126178741455\n",
      "        time for verification: 0.25217127799987793\n",
      "    layer progress, group 71 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.2552769184112549\n",
      "        time for verification: 0.333892822265625\n",
      "    layer progress, group 72 of 86 \n",
      "        time for training: 0.00809025764465332\n",
      "        actual verification time 0.1516571044921875\n",
      "        time for verification: 0.2245485782623291\n",
      "    layer progress, group 73 of 86 \n",
      "        time for training: 0.00805211067199707\n",
      "        actual verification time 0.19948053359985352\n",
      "        time for verification: 0.2619173526763916\n",
      "    layer progress, group 74 of 86 \n",
      "        time for training: 0.002039194107055664\n",
      "        actual verification time 0.19426393508911133\n",
      "        time for verification: 0.2709486484527588\n",
      "    layer progress, group 75 of 86 \n",
      "        time for training: 0.0020601749420166016\n",
      "        actual verification time 0.24591422080993652\n",
      "        time for verification: 0.32470130920410156\n",
      "    layer progress, group 76 of 86 \n",
      "        time for training: 0.008069992065429688\n",
      "        actual verification time 0.19031906127929688\n",
      "        time for verification: 0.26314806938171387\n",
      "    layer progress, group 77 of 86 \n",
      "        time for training: 0.0020360946655273438\n",
      "        actual verification time 0.22022509574890137\n",
      "        time for verification: 0.29360055923461914\n",
      "    layer progress, group 78 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.14104771614074707\n",
      "        time for verification: 0.22201895713806152\n",
      "    layer progress, group 79 of 86 \n",
      "        time for training: 0.008071184158325195\n",
      "        actual verification time 0.19212985038757324\n",
      "        time for verification: 0.264742374420166\n",
      "    layer progress, group 80 of 86 \n",
      "        time for training: 0.0\n",
      "        actual verification time 0.18193793296813965\n",
      "        time for verification: 0.2626469135284424\n",
      "    layer progress, group 81 of 86 \n",
      "        time for training: 0.008075952529907227\n",
      "        actual verification time 0.18170952796936035\n",
      "        time for verification: 0.2442638874053955\n",
      "    layer progress, group 82 of 86 \n",
      "        time for training: 0.008067846298217773\n",
      "        actual verification time 0.1713876724243164\n",
      "        time for verification: 0.2339169979095459\n",
      "    layer progress, group 83 of 86 \n",
      "        time for training: 0.008069276809692383\n",
      "        actual verification time 0.19004440307617188\n",
      "        time for verification: 0.2627525329589844\n",
      "    layer progress, group 84 of 86 \n",
      "        time for training: 0.002045154571533203\n",
      "        actual verification time 0.18173599243164062\n",
      "        time for verification: 0.2523159980773926\n",
      "    layer progress, group 85 of 86 \n",
      "        time for training: 0.00804448127746582\n",
      "        actual verification time 0.20177364349365234\n",
      "        time for verification: 0.27437305450439453\n",
      "    layer progress, group 86 of 86 \n",
      "        time for training: 0.008059263229370117\n",
      "        actual verification time 0.10071706771850586\n",
      "        time for verification: 0.14101839065551758\n",
      "    time for icnn_bound calculation: 2.913349151611328\n",
      "    time for regrouping method: 0.01017904281616211\n"
     ]
    }
   ],
   "source": [
    "group_size = 6\n",
    "eps = 0.01\n",
    "icnn_factory = ICNNFactory(\"logical\", [3, 1], force_positive_init=False, with_two_layers=False,\n",
    "                               init_scaling=10, init_all_with_zeros=False)\n",
    "\n",
    "print(math.ceil(nn.layer_widths[1] / group_size)+1)\n",
    "icnns, all_group_indices, fixed_neuron_per_layer_lower, fixed_neuron_per_layer_upper, bounds_affine_out, bounds_layer_out = \\\n",
    "    multidhov.start_verification(nn, test_image, icnn_factory, group_size, eps=eps, icnn_epochs=1, icnn_batch_size=1000,\n",
    "                                 sample_count=10, sample_new=False, use_over_approximation=True, break_after=math.ceil(nn.layer_widths[1] / group_size)+1,\n",
    "                                 sample_over_input_space=False, sample_over_output_space=True, use_icnn_bounds=True, use_fixed_neurons=True,\n",
    "                                 force_inclusion_steps=0, preemptive_stop=False, even_gradient_training=False,\n",
    "                                 keep_ambient_space=True, data_grad_descent_steps=0, train_outer=False,\n",
    "                                 should_plot=\"none\", optimizer=\"SdLBFGS\", init_network=True, adapt_lambda=\"none\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounds affine out \n",
      " [[tensor([ 0.2088,  0.0196, -0.0026,  ..., -1.3163, -0.3677, -0.2236],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>), tensor([ 0.7791,  0.5886,  0.5620,  ..., -0.7273,  0.2118,  0.3357],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)], [tensor([-2.3678, -2.2506, -1.4631, -1.5480, -1.6997, -2.3157, -2.0312, -2.1657,\n",
      "        -2.4451, -2.1094, -2.5736, -1.5114, -1.7204, -1.6585, -1.9900, -2.2936,\n",
      "        -1.8326, -1.9801, -2.0940, -1.9852, -2.4245, -2.3254, -1.7885, -2.0473,\n",
      "        -1.7248, -2.0310, -2.1796, -1.5130, -2.1948, -1.2174, -2.3810, -1.6069,\n",
      "        -2.2250, -1.0840, -2.1734, -1.6119, -2.2030, -2.1282, -2.2584, -2.3525,\n",
      "        -1.9250, -2.2268, -1.9972, -1.8989, -2.1081, -1.8777, -2.4823, -2.0515,\n",
      "        -2.2419, -2.3808, -2.0566, -2.1669, -1.8904, -2.0341, -2.4128, -2.7350,\n",
      "        -2.8463, -1.6933, -2.4030, -2.0562, -1.9528, -2.1218, -2.4711, -2.3628,\n",
      "        -2.2642, -2.0658, -1.8225, -1.7588, -2.5546, -1.6129, -1.8245, -2.1414,\n",
      "        -1.9952, -1.9148, -2.2306, -2.0037, -2.5366, -1.4191, -2.1562, -1.9158,\n",
      "        -2.1424, -1.8178, -2.2539, -2.0352, -2.1833, -1.5824, -2.2810, -1.4759,\n",
      "        -2.3608, -1.8562, -2.4729, -2.1235, -2.2414, -1.7541, -2.2816, -2.0879,\n",
      "        -2.0146, -1.8970, -2.4248, -2.0457, -1.6711, -2.2117, -1.8608, -2.7619,\n",
      "        -1.7872, -1.3919, -1.9690, -1.9393, -2.2661, -1.6468, -2.1895, -2.1893,\n",
      "        -1.9798, -2.1747, -1.8322, -2.2181, -2.1823, -1.9345, -2.2441, -1.9113,\n",
      "        -2.1066, -2.3348, -1.8618, -2.3357, -1.8482, -2.2501, -2.1368, -1.9601,\n",
      "        -1.7689, -2.5784, -2.6144, -2.2968, -2.0713, -2.2545, -2.4273, -2.6481,\n",
      "        -1.8894, -2.2630, -1.9883, -1.8003, -1.3780, -1.8716, -2.0415, -2.0463,\n",
      "        -2.5830, -2.0505, -1.8829, -1.3248, -2.2755, -2.2885, -2.6692, -2.1592,\n",
      "        -1.9485, -1.7881, -1.9013, -2.4198, -1.8273, -1.7009, -1.7606, -1.8754,\n",
      "        -1.7755, -1.9315, -2.2627, -2.0458, -2.3787, -1.7453, -2.1708, -2.3769,\n",
      "        -1.7394, -2.0193, -1.3378, -2.1757, -1.9578, -2.4132, -2.0921, -2.1697,\n",
      "        -2.3222, -2.1388, -1.9731, -2.4500, -1.7567, -2.0712, -1.8797, -2.4587,\n",
      "        -2.4395, -2.0947, -1.4238, -1.6812, -2.0884, -2.0478, -2.1802, -2.1793,\n",
      "        -1.7242, -2.2525, -2.0815, -2.1800, -2.2343, -2.2170, -2.2868, -2.3326,\n",
      "        -2.0985, -1.9025, -2.6216, -2.6766, -2.8230, -2.0409, -1.9765, -2.5223,\n",
      "        -2.5186, -2.3216, -2.1099, -1.8069, -1.8001, -1.8937, -2.3844, -1.5554,\n",
      "        -1.7413, -1.9307, -1.7702, -2.1569, -2.5157, -1.9025, -2.8545, -2.3963,\n",
      "        -1.9962, -2.0257, -2.0677, -1.7343, -1.7432, -1.9814, -2.0761, -1.9112,\n",
      "        -1.9711, -2.5830, -1.7412, -2.0081, -1.8990, -2.3547, -1.9318, -1.6319,\n",
      "        -1.3847, -2.1291, -2.3581, -2.3676, -2.3591, -1.6119, -1.5264, -2.2582,\n",
      "        -1.9452, -2.0983, -1.5434, -2.4950, -2.3965, -2.1024, -2.1247, -2.0916,\n",
      "        -2.4071, -1.6716, -2.2409, -2.0783, -1.9700, -1.7985, -1.9317, -1.9193,\n",
      "        -2.8491, -2.1171, -2.0579, -2.2669, -1.9235, -1.8752, -2.1481, -1.7179,\n",
      "        -2.2722, -2.0011, -2.3476, -1.9064, -2.6052, -1.9573, -2.5701, -2.5449,\n",
      "        -2.2295, -2.4069, -1.8491, -1.6295, -2.2801, -2.3843, -1.8899, -2.2901,\n",
      "        -2.2081, -2.2685, -2.5606, -2.3657, -2.0694, -1.8586, -2.1898, -2.7850,\n",
      "        -2.6666, -2.2256, -1.8080, -2.3675, -2.0387, -1.7699, -1.7594, -1.9638,\n",
      "        -1.7930, -2.5955, -2.2309, -2.1528, -2.1117, -1.1386, -1.7030, -2.1813,\n",
      "        -2.1974, -2.0460, -2.0127, -2.1019, -1.7086, -1.7625, -1.2394, -1.9954,\n",
      "        -2.0407, -2.5292, -2.1938, -2.6148, -1.9193, -1.8724, -2.0353, -1.8261,\n",
      "        -1.7046, -1.8707, -1.7427, -2.3210, -2.0953, -2.0234, -1.8730, -1.9157,\n",
      "        -2.1633, -2.2956, -1.8241, -1.9183, -1.9252, -2.2855, -1.7693, -2.0694,\n",
      "        -2.2078, -2.2395, -1.8100, -1.8931, -2.4937, -1.6027, -1.7344, -2.4556,\n",
      "        -1.7214, -2.1970, -1.7753, -2.5441, -2.1143, -1.7223, -2.5505, -1.8186,\n",
      "        -2.3532, -1.9206, -2.3041, -2.0582, -1.8424, -1.8498, -2.1913, -2.2310,\n",
      "        -2.1394, -2.0301, -2.6459, -1.6225, -2.2951, -2.5787, -2.3691, -2.5596,\n",
      "        -2.5056, -2.1484, -1.7632, -2.2438, -2.3388, -2.1026, -2.3081, -1.9033,\n",
      "        -2.3444, -1.8191, -1.9034, -1.3551, -2.2204, -1.6438, -2.2685, -1.7940,\n",
      "        -1.6682, -2.2030, -2.1190, -2.3994, -2.0798, -2.8044, -1.7596, -2.6453,\n",
      "        -1.6814, -2.1698, -1.7821, -1.6353, -2.7215, -1.7791, -2.2148, -2.1106,\n",
      "        -1.8514, -2.5463, -2.1293, -1.9078, -2.0379, -2.4073, -2.1459, -2.1028,\n",
      "        -2.7059, -2.1078, -2.5884, -1.8202, -2.0828, -2.1631, -2.0025, -1.8010,\n",
      "        -1.8327, -2.1799, -1.9555, -1.9659, -2.4192, -1.6459, -1.8146, -1.9345,\n",
      "        -2.6203, -1.9685, -2.5791, -2.2627, -2.0145, -1.9692, -2.2245, -1.6481,\n",
      "        -1.9672, -2.0471, -2.0556, -2.2650, -2.1470, -2.2929, -2.2956, -1.7933,\n",
      "        -2.4848, -1.8552, -1.7189, -2.2088, -2.2806, -1.4828, -1.8546, -2.1495,\n",
      "        -2.3400, -2.2611, -1.9456, -2.0526, -2.1521, -2.4529, -1.5742, -2.0786,\n",
      "        -1.8988, -1.7005, -2.1786, -2.0611, -2.4032, -1.8272, -2.2149, -2.3349,\n",
      "        -2.3801, -2.3148, -1.7153, -2.4166, -1.9706, -2.0655, -2.4320, -1.9155,\n",
      "        -2.3920, -2.0351, -1.7001, -2.1501, -1.9242, -2.3359, -1.5724, -1.7956,\n",
      "        -2.7937, -1.9470, -2.2252, -2.0580, -2.5420, -2.0681, -1.8897, -1.9469,\n",
      "        -1.9611, -1.8518, -2.0844, -2.6111, -1.7594, -2.0141, -1.9591, -1.8939,\n",
      "        -2.2650, -1.9786, -1.4302, -2.2497, -2.2228, -2.7537, -2.2954, -2.2246],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>), tensor([2.1985, 2.2024, 3.5789, 3.2989, 3.1727, 2.7257, 3.1589, 2.9706, 2.3492,\n",
      "        2.3374, 2.2188, 3.5131, 2.9834, 3.2893, 2.7698, 2.7456, 3.0551, 2.7703,\n",
      "        2.4261, 2.7636, 2.3316, 2.5707, 3.1093, 2.7213, 3.2009, 2.8840, 2.6372,\n",
      "        3.2359, 2.7661, 3.7934, 1.9150, 3.2887, 2.6867, 3.5916, 2.4751, 3.0572,\n",
      "        2.5368, 2.8881, 2.4758, 2.3354, 2.7308, 2.2997, 2.9015, 2.8642, 2.6307,\n",
      "        2.6679, 2.1138, 2.7184, 2.5877, 2.4363, 2.8510, 2.2290, 2.7605, 2.9352,\n",
      "        2.5532, 2.4591, 2.0163, 3.2752, 2.3035, 2.7828, 2.5608, 2.5729, 2.1703,\n",
      "        2.6810, 2.4530, 2.5627, 2.7323, 3.0748, 2.3498, 3.0908, 2.9539, 2.4527,\n",
      "        2.4962, 2.8960, 2.6532, 2.6162, 2.2160, 3.6041, 3.0032, 2.8227, 2.6230,\n",
      "        3.2652, 2.3000, 2.5879, 2.6289, 3.2825, 2.4289, 3.2062, 2.0722, 2.9592,\n",
      "        2.0333, 2.4219, 2.9470, 3.5575, 2.5283, 2.6129, 2.5972, 2.9534, 2.9723,\n",
      "        2.5614, 2.8972, 2.7913, 2.9018, 1.8760, 3.3864, 3.4564, 2.7306, 3.0458,\n",
      "        2.2486, 3.0729, 2.5512, 2.5311, 2.8378, 2.2285, 3.0569, 2.5264, 2.6175,\n",
      "        2.5961, 2.5296, 2.6409, 2.6287, 2.5965, 2.9409, 2.4046, 2.7446, 2.3342,\n",
      "        2.9047, 2.5440, 3.0772, 1.9716, 2.1576, 2.7586, 2.4789, 2.6533, 2.2112,\n",
      "        1.8408, 2.9815, 2.7088, 2.7785, 2.7992, 3.3071, 3.1239, 2.4893, 3.1774,\n",
      "        2.4032, 2.8245, 2.6077, 3.3692, 2.4308, 2.2467, 1.9323, 2.7972, 2.7219,\n",
      "        3.2780, 2.9485, 1.9737, 2.9057, 3.7385, 3.2042, 2.6872, 2.8178, 2.6760,\n",
      "        2.4202, 2.7386, 2.2116, 2.8576, 3.0162, 2.2776, 3.1711, 2.4163, 3.9645,\n",
      "        2.4481, 2.7093, 2.1543, 2.6305, 2.5179, 2.1667, 2.6047, 2.6598, 2.0520,\n",
      "        3.2499, 2.4984, 3.2387, 1.9903, 2.1779, 2.6996, 3.1079, 3.0683, 2.8121,\n",
      "        2.5086, 2.6226, 2.2937, 2.8322, 2.4843, 2.4434, 2.6396, 2.6338, 2.3789,\n",
      "        2.7651, 2.4555, 2.8191, 2.7082, 1.9177, 1.8043, 2.0851, 2.5194, 2.5843,\n",
      "        2.2937, 2.0311, 2.0399, 2.4141, 2.8709, 2.6827, 3.2072, 2.4175, 2.9141,\n",
      "        2.7656, 2.7763, 2.8114, 2.5082, 1.9327, 2.6724, 2.2971, 2.3961, 2.4524,\n",
      "        2.7674, 2.5349, 3.2752, 2.6632, 2.7877, 2.4443, 2.6509, 2.4466, 2.1202,\n",
      "        2.9781, 2.3353, 2.5709, 2.1044, 2.5156, 2.9475, 3.4756, 2.5669, 2.3189,\n",
      "        2.3897, 2.2457, 3.0801, 3.5114, 2.4983, 2.5862, 2.5761, 3.1905, 2.3847,\n",
      "        2.0487, 2.6576, 2.6359, 3.1154, 2.4759, 2.8485, 3.0165, 2.8262, 2.6906,\n",
      "        2.8355, 2.7099, 2.5992, 1.6763, 2.7652, 2.6295, 2.3402, 2.8419, 2.9079,\n",
      "        2.4964, 2.8957, 2.8146, 2.8652, 2.3430, 2.9373, 1.6113, 2.4700, 1.9122,\n",
      "        2.3025, 2.5531, 2.2747, 2.6465, 2.9438, 2.1805, 2.2151, 3.0780, 2.2498,\n",
      "        2.4682, 2.1720, 1.9502, 2.3754, 3.0590, 2.9243, 2.5663, 2.1819, 1.9129,\n",
      "        2.4948, 2.9328, 2.1700, 2.6950, 2.9161, 3.1480, 2.9733, 2.6979, 1.7689,\n",
      "        2.1127, 2.9371, 2.5969, 3.9292, 3.1885, 2.4670, 2.7131, 2.7215, 2.7180,\n",
      "        2.5085, 3.2214, 3.0689, 3.8328, 2.5023, 2.7257, 2.1245, 2.3074, 2.0151,\n",
      "        2.5359, 2.9857, 2.3858, 2.7070, 2.9559, 2.5780, 2.9495, 2.1060, 2.3754,\n",
      "        2.7490, 2.9435, 2.4471, 2.6938, 2.2222, 2.7023, 2.6875, 2.6380, 2.3804,\n",
      "        2.8629, 2.6581, 2.5343, 2.1080, 3.1665, 2.9870, 2.4671, 3.0791, 2.9258,\n",
      "        2.0098, 2.9876, 2.4583, 3.1908, 1.8993, 2.4326, 3.2955, 2.3088, 2.7669,\n",
      "        2.3705, 2.6104, 2.2965, 2.8599, 3.0404, 3.1265, 2.3486, 2.6209, 2.5314,\n",
      "        2.9598, 2.1777, 3.2833, 2.4826, 2.1495, 2.1120, 2.6329, 2.0196, 2.5830,\n",
      "        2.9953, 2.7731, 2.1109, 2.6253, 2.6458, 3.1792, 2.3662, 2.9563, 2.5534,\n",
      "        3.6706, 2.4945, 3.0791, 2.2953, 2.7153, 3.2815, 2.2885, 2.4486, 2.0401,\n",
      "        3.1391, 2.0505, 3.0785, 2.2818, 2.9539, 2.3042, 2.9556, 2.9958, 1.6933,\n",
      "        2.8548, 2.2854, 2.5816, 3.0564, 2.0341, 2.3031, 2.5163, 2.3943, 2.1332,\n",
      "        2.5911, 2.6126, 2.3571, 2.4486, 1.9534, 3.2738, 2.5618, 2.5262, 2.7043,\n",
      "        2.9916, 3.1165, 2.2164, 3.1818, 2.5545, 2.2131, 3.1386, 3.0965, 2.8530,\n",
      "        1.9496, 2.5144, 2.0527, 2.6682, 2.6472, 2.7935, 2.5063, 3.1373, 3.1121,\n",
      "        2.6312, 2.4272, 2.1223, 2.9568, 2.5563, 2.5324, 3.5180, 2.0442, 2.8928,\n",
      "        3.0702, 2.2026, 2.4208, 3.1787, 2.5664, 2.6002, 2.0378, 2.5944, 2.5661,\n",
      "        2.8763, 2.4492, 2.1314, 3.1454, 2.7629, 3.0102, 3.0103, 2.5855, 2.3826,\n",
      "        2.2292, 2.7869, 2.7408, 1.9706, 2.4204, 2.4976, 3.1486, 2.0788, 2.7363,\n",
      "        2.4180, 2.0303, 2.9344, 2.0718, 2.6562, 3.4177, 3.0537, 2.7972, 2.2466,\n",
      "        3.6124, 3.4383, 1.8691, 2.8082, 2.1992, 2.7259, 1.9714, 2.4989, 2.9801,\n",
      "        3.0282, 2.6700, 2.6769, 2.3750, 2.2532, 3.5080, 2.4676, 2.6120, 2.4669,\n",
      "        2.5763, 2.6182, 3.6755, 2.8927, 2.4527, 1.9694, 3.1698, 2.4372],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)], [tensor([-35.7740, -45.8241, -38.7560, -31.5890, -35.1164, -38.4393, -42.1957,\n",
      "        -46.6409, -48.8005, -44.3439], dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>), tensor([43.3247, 44.5450, 41.7424, 34.7020, 40.5605, 35.9051, 43.5192, 46.3586,\n",
      "        38.2935, 41.4184], dtype=torch.float64, grad_fn=<AddBackward0>)]]\n",
      "bounds layer out \n",
      " [[tensor([0.2088, 0.0196, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "       dtype=torch.float64, grad_fn=<MaximumBackward0>), tensor([0.7791, 0.5886, 0.5620,  ..., 0.0000, 0.2118, 0.3357],\n",
      "       dtype=torch.float64, grad_fn=<MaximumBackward0>)], [tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64,\n",
      "       grad_fn=<MaximumBackward0>), tensor([2.1985, 2.2024, 3.5789, 3.2989, 3.1727, 2.7257, 3.1589, 2.9706, 2.3492,\n",
      "        2.3374, 2.2188, 3.5131, 2.9834, 3.2893, 2.7698, 2.7456, 3.0551, 2.7703,\n",
      "        2.4261, 2.7636, 2.3316, 2.5707, 3.1093, 2.7213, 3.2009, 2.8840, 2.6372,\n",
      "        3.2359, 2.7661, 3.7934, 1.9150, 3.2887, 2.6867, 3.5916, 2.4751, 3.0572,\n",
      "        2.5368, 2.8881, 2.4758, 2.3354, 2.7308, 2.2997, 2.9015, 2.8642, 2.6307,\n",
      "        2.6679, 2.1138, 2.7184, 2.5877, 2.4363, 2.8510, 2.2290, 2.7605, 2.9352,\n",
      "        2.5532, 2.4591, 2.0163, 3.2752, 2.3035, 2.7828, 2.5608, 2.5729, 2.1703,\n",
      "        2.6810, 2.4530, 2.5627, 2.7323, 3.0748, 2.3498, 3.0908, 2.9539, 2.4527,\n",
      "        2.4962, 2.8960, 2.6532, 2.6162, 2.2160, 3.6041, 3.0032, 2.8227, 2.6230,\n",
      "        3.2652, 2.3000, 2.5879, 2.6289, 3.2825, 2.4289, 3.2062, 2.0722, 2.9592,\n",
      "        2.0333, 2.4219, 2.9470, 3.5575, 2.5283, 2.6129, 2.5972, 2.9534, 2.9723,\n",
      "        2.5614, 2.8972, 2.7913, 2.9018, 1.8760, 3.3864, 3.4564, 2.7306, 3.0458,\n",
      "        2.2486, 3.0729, 2.5512, 2.5311, 2.8378, 2.2285, 3.0569, 2.5264, 2.6175,\n",
      "        2.5961, 2.5296, 2.6409, 2.6287, 2.5965, 2.9409, 2.4046, 2.7446, 2.3342,\n",
      "        2.9047, 2.5440, 3.0772, 1.9716, 2.1576, 2.7586, 2.4789, 2.6533, 2.2112,\n",
      "        1.8408, 2.9815, 2.7088, 2.7785, 2.7992, 3.3071, 3.1239, 2.4893, 3.1774,\n",
      "        2.4032, 2.8245, 2.6077, 3.3692, 2.4308, 2.2467, 1.9323, 2.7972, 2.7219,\n",
      "        3.2780, 2.9485, 1.9737, 2.9057, 3.7385, 3.2042, 2.6872, 2.8178, 2.6760,\n",
      "        2.4202, 2.7386, 2.2116, 2.8576, 3.0162, 2.2776, 3.1711, 2.4163, 3.9645,\n",
      "        2.4481, 2.7093, 2.1543, 2.6305, 2.5179, 2.1667, 2.6047, 2.6598, 2.0520,\n",
      "        3.2499, 2.4984, 3.2387, 1.9903, 2.1779, 2.6996, 3.1079, 3.0683, 2.8121,\n",
      "        2.5086, 2.6226, 2.2937, 2.8322, 2.4843, 2.4434, 2.6396, 2.6338, 2.3789,\n",
      "        2.7651, 2.4555, 2.8191, 2.7082, 1.9177, 1.8043, 2.0851, 2.5194, 2.5843,\n",
      "        2.2937, 2.0311, 2.0399, 2.4141, 2.8709, 2.6827, 3.2072, 2.4175, 2.9141,\n",
      "        2.7656, 2.7763, 2.8114, 2.5082, 1.9327, 2.6724, 2.2971, 2.3961, 2.4524,\n",
      "        2.7674, 2.5349, 3.2752, 2.6632, 2.7877, 2.4443, 2.6509, 2.4466, 2.1202,\n",
      "        2.9781, 2.3353, 2.5709, 2.1044, 2.5156, 2.9475, 3.4756, 2.5669, 2.3189,\n",
      "        2.3897, 2.2457, 3.0801, 3.5114, 2.4983, 2.5862, 2.5761, 3.1905, 2.3847,\n",
      "        2.0487, 2.6576, 2.6359, 3.1154, 2.4759, 2.8485, 3.0165, 2.8262, 2.6906,\n",
      "        2.8355, 2.7099, 2.5992, 1.6763, 2.7652, 2.6295, 2.3402, 2.8419, 2.9079,\n",
      "        2.4964, 2.8957, 2.8146, 2.8652, 2.3430, 2.9373, 1.6113, 2.4700, 1.9122,\n",
      "        2.3025, 2.5531, 2.2747, 2.6465, 2.9438, 2.1805, 2.2151, 3.0780, 2.2498,\n",
      "        2.4682, 2.1720, 1.9502, 2.3754, 3.0590, 2.9243, 2.5663, 2.1819, 1.9129,\n",
      "        2.4948, 2.9328, 2.1700, 2.6950, 2.9161, 3.1480, 2.9733, 2.6979, 1.7689,\n",
      "        2.1127, 2.9371, 2.5969, 3.9292, 3.1885, 2.4670, 2.7131, 2.7215, 2.7180,\n",
      "        2.5085, 3.2214, 3.0689, 3.8328, 2.5023, 2.7257, 2.1245, 2.3074, 2.0151,\n",
      "        2.5359, 2.9857, 2.3858, 2.7070, 2.9559, 2.5780, 2.9495, 2.1060, 2.3754,\n",
      "        2.7490, 2.9435, 2.4471, 2.6938, 2.2222, 2.7023, 2.6875, 2.6380, 2.3804,\n",
      "        2.8629, 2.6581, 2.5343, 2.1080, 3.1665, 2.9870, 2.4671, 3.0791, 2.9258,\n",
      "        2.0098, 2.9876, 2.4583, 3.1908, 1.8993, 2.4326, 3.2955, 2.3088, 2.7669,\n",
      "        2.3705, 2.6104, 2.2965, 2.8599, 3.0404, 3.1265, 2.3486, 2.6209, 2.5314,\n",
      "        2.9598, 2.1777, 3.2833, 2.4826, 2.1495, 2.1120, 2.6329, 2.0196, 2.5830,\n",
      "        2.9953, 2.7731, 2.1109, 2.6253, 2.6458, 3.1792, 2.3662, 2.9563, 2.5534,\n",
      "        3.6706, 2.4945, 3.0791, 2.2953, 2.7153, 3.2815, 2.2885, 2.4486, 2.0401,\n",
      "        3.1391, 2.0505, 3.0785, 2.2818, 2.9539, 2.3042, 2.9556, 2.9958, 1.6933,\n",
      "        2.8548, 2.2854, 2.5816, 3.0564, 2.0341, 2.3031, 2.5163, 2.3943, 2.1332,\n",
      "        2.5911, 2.6126, 2.3571, 2.4486, 1.9534, 3.2738, 2.5618, 2.5262, 2.7043,\n",
      "        2.9916, 3.1165, 2.2164, 3.1818, 2.5545, 2.2131, 3.1386, 3.0965, 2.8530,\n",
      "        1.9496, 2.5144, 2.0527, 2.6682, 2.6472, 2.7935, 2.5063, 3.1373, 3.1121,\n",
      "        2.6312, 2.4272, 2.1223, 2.9568, 2.5563, 2.5324, 3.5180, 2.0442, 2.8928,\n",
      "        3.0702, 2.2026, 2.4208, 3.1787, 2.5664, 2.6002, 2.0378, 2.5944, 2.5661,\n",
      "        2.8763, 2.4492, 2.1314, 3.1454, 2.7629, 3.0102, 3.0103, 2.5855, 2.3826,\n",
      "        2.2292, 2.7869, 2.7408, 1.9706, 2.4204, 2.4976, 3.1486, 2.0788, 2.7363,\n",
      "        2.4180, 2.0303, 2.9344, 2.0718, 2.6562, 3.4177, 3.0537, 2.7972, 2.2466,\n",
      "        3.6124, 3.4383, 1.8691, 2.8082, 2.1992, 2.7259, 1.9714, 2.4989, 2.9801,\n",
      "        3.0282, 2.6700, 2.6769, 2.3750, 2.2532, 3.5080, 2.4676, 2.6120, 2.4669,\n",
      "        2.5763, 2.6182, 3.6755, 2.8927, 2.4527, 1.9694, 3.1698, 2.4372],\n",
      "       dtype=torch.float64, grad_fn=<MaximumBackward0>)], [tensor([-35.7740, -45.8241, -38.7560, -31.5890, -35.1164, -38.4393, -42.1957,\n",
      "        -46.6409, -48.8005, -44.3439], dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>), tensor([43.3247, 44.5450, 41.7424, 34.7020, 40.5605, 35.9051, 43.5192, 46.3586,\n",
      "        38.2935, 41.4184], dtype=torch.float64, grad_fn=<AddBackward0>)]]\n"
     ]
    }
   ],
   "source": [
    "input_flattened = torch.flatten(test_image)\n",
    "bounds_affine_out, bounds_layer_out = nn.calculate_box_bounds([input_flattened.add(-eps), input_flattened.add(eps)])\n",
    "print(\"bounds affine out \\n {}\".format(bounds_affine_out))\n",
    "print(\"bounds layer out \\n {}\".format(bounds_layer_out))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "milp_verifier = MILPVerifier(nn, test_image, eps, print_log=False)\n",
    "snv_verifier = SingleNeuronVerifier(nn, test_image, eps, print_log=False)\n",
    "#dhov_verifier = DHOVVerifier(icnns, group_size, nn, test_image, 1)\n",
    "\n",
    "milp_verifier.generate_constraints_for_net()\n",
    "snv_verifier.generate_constraints_for_net()\n",
    "#dhov_verifier.generate_constraints_for_net()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "layer_index = 1\n",
    "neuron_index = 0\n",
    "milp_model = milp_verifier.model\n",
    "snv_model = snv_verifier.model\n",
    "#dhov_model = dhov_verifier.model\n",
    "dhov_model = icnn_model(icnns[layer_index][0], nn, test_image, eps, layer_index, 0, 6, print_log=False)\n",
    "milp_model.update()\n",
    "snv_model.update()\n",
    "dhov_model.update()\n",
    "\"\"\"all_var = milp_model.getVars()\n",
    "for var in all_var:\n",
    "    print(var)\"\"\"\n",
    "neuron_name = \"relu_var{}[{}]\".format(2*layer_index, neuron_index)\n",
    "icnn_neuron_name = \"icnn_var[{}]\".format(neuron_index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================\n",
      "opt value: 0.0\n",
      "===================================================================================\n",
      "opt value: 0.0\n"
     ]
    }
   ],
   "source": [
    "#milp_copy = milp_model.copy()\n",
    "snv_copy = snv_model.copy()\n",
    "dhov_copy = dhov_model.copy()\n",
    "\n",
    "#milp_copy.Params.LogToConsole = 0\n",
    "snv_copy.Params.LogToConsole = 0\n",
    "dhov_copy.Params.LogToConsole = 0\n",
    "\n",
    "#add_min_constr(milp_copy, neuron_name) #affine_var0[0]\n",
    "add_min_constr(snv_copy, neuron_name)\n",
    "add_min_constr(dhov_copy, icnn_neuron_name)\n",
    "\n",
    "#optimize_model(milp_copy, neuron_name)\n",
    "print(\"===================================================================================\")\n",
    "optimize_model(snv_copy, neuron_name)\n",
    "print(\"===================================================================================\")\n",
    "optimize_model(dhov_copy, icnn_neuron_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "milp_copy = milp_model.copy()\n",
    "snv_copy = snv_model.copy()\n",
    "dhov_copy = dhov_model.copy()\n",
    "\n",
    "milp_copy.Params.LogToConsole = 0\n",
    "snv_copy.Params.LogToConsole = 0\n",
    "dhov_copy.Params.LogToConsole = 0\n",
    "\n",
    "add_max_constr(milp_copy, neuron_name)\n",
    "add_max_constr(snv_copy, neuron_name)\n",
    "add_max_constr(dhov_copy, icnn_neuron_name)\n",
    "\n",
    "optimize_model(milp_copy, neuron_name)\n",
    "print(\"===================================================================================\")\n",
    "optimize_model(snv_copy, neuron_name)\n",
    "print(\"===================================================================================\")\n",
    "optimize_model(dhov_copy, icnn_neuron_name)\n",
    "print(\"===================================================================================\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}