{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from script.NeuralNets.Networks import SequentialNN, ICNN, ICNNApproxMax, ICNNLogical\n",
    "from script.settings import device, data_type\n",
    "import script.DHOV.MultiDHOV as multidhov\n",
    "from script.Verification.Verifier import SingleNeuronVerifier, MILPVerifier, DHOVVerifier\n",
    "import gurobipy as grp\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def add_max_constr(model, neuron_name):\n",
    "    neuron_var = model.getVarByName(neuron_name)\n",
    "    model.setObjective(neuron_var, grp.GRB.MAXIMIZE)\n",
    "\n",
    "def add_min_constr(model, neuron):\n",
    "    neuron_var = model.getVarByName(neuron)\n",
    "    model.setObjective(neuron_var, grp.GRB.MINIMIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def optimize_model(model, neuron_name):\n",
    "    model.update()\n",
    "    model.optimize()\n",
    "    if model.Status == grp.GRB.OPTIMAL:\n",
    "        print(\"opt value: {}\".format(model.getVarByName(neuron_name).getAttr(\"x\")))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def icnn_model(icnn, nn, input_x, eps, layer_index, from_neuron, to_neuron, print_log=False):\n",
    "    m = grp.Model()\n",
    "    if not print_log:\n",
    "        m.Params.LogToConsole = 0\n",
    "\n",
    "    input_flattened = torch.flatten(input_x)\n",
    "    bounds_affine_out, bounds_layer_out = nn.calculate_box_bounds(\n",
    "        [input_flattened.add(-eps), input_flattened.add(eps)])\n",
    "\n",
    "    parameter_list = list(nn.parameters())\n",
    "\n",
    "    input_size = len(parameter_list[layer_index])\n",
    "    lb = bounds_layer_out[layer_index][0].detach().cpu().numpy()\n",
    "    ub = bounds_layer_out[layer_index][1].detach().cpu().numpy()\n",
    "    in_var = m.addMVar(input_size, lb=lb, ub=ub, name=\"icnn_var\")\n",
    "\n",
    "    low = bounds_layer_out[layer_index][0][from_neuron: to_neuron]\n",
    "    up = bounds_layer_out[layer_index][1][from_neuron: to_neuron]\n",
    "    constraint_bounds_affine_out, constraint_bounds_layer_out = icnn.calculate_box_bounds([low, up])\n",
    "    icnn.add_max_output_constraints(m, in_var[from_neuron: to_neuron], constraint_bounds_affine_out, constraint_bounds_layer_out)\n",
    "\n",
    "    return m"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\"\"\"W1 = [1. 1.; 1. -1.]\n",
    "    b1 = [0., 0.]\n",
    "    W2 = [1. 1.; 1. -1.]\n",
    "    b2 = [-0.5, 0.]\n",
    "    W3 = [-1. 1.; 1. 1.]\n",
    "    b3 = [3., 0.] \"\"\"\n",
    "\n",
    "\"\"\"nn = SequentialNN([2, 2, 2, 2])\n",
    "\n",
    "with torch.no_grad():\n",
    "    parameter_list = list(nn.parameters())\n",
    "    parameter_list[0].data = torch.tensor([[1, 1], [1, -1]], dtype=data_type).to(device)\n",
    "    parameter_list[1].data = torch.tensor([0, 0], dtype=data_type).to(device)\n",
    "    parameter_list[2].data = torch.tensor([[1, 1], [1, -1]], dtype=data_type).to(device)\n",
    "    parameter_list[3].data = torch.tensor([-0.5, 0], dtype=data_type).to(device)\n",
    "    parameter_list[4].data = torch.tensor([[-1, 1], [1, 1]], dtype=data_type).to(device)\n",
    "    parameter_list[5].data = torch.tensor([3, 0], dtype=data_type).to(device)\n",
    "\n",
    "test_image = torch.tensor([[0, 0]], dtype=data_type).to(device)\"\"\"\n",
    "\n",
    "transform = Compose([ToTensor(),\n",
    "                         Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "                        )\n",
    "\n",
    "training_data = CIFAR10(root=\"../../cifar\", train=True, download=True, transform=transform)\n",
    "images, labels = training_data.__getitem__(0)\n",
    "test_image, test_label = torch.unsqueeze(images, 0).to(dtype=data_type).to(device), torch.unsqueeze(\n",
    "    torch.tensor(labels), 0).to(dtype=data_type).to(device)\n",
    "\n",
    "nn = SequentialNN([32 * 32 * 3, 1024, 512, 10])\n",
    "nn.load_state_dict(torch.load(\"../../cifar_fc.pth\", map_location=torch.device(device)), strict=False)\n",
    "parameter_list = list(nn.parameters())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "approximation of layer: 0\n",
      "layer progress, group 0 of 512 \n",
      "        time for training: 4.0447893142700195\n",
      "        actual verification time 0.2133779525756836\n",
      "        time for verification: 0.45398855209350586\n",
      "aborting because of force break\n"
     ]
    }
   ],
   "source": [
    "group_size = 2\n",
    "eps = 0.1\n",
    "icnns = []\n",
    "for i in range((len(parameter_list) - 2) // 2):\n",
    "    layer_index = int(i / 2)\n",
    "    in_size = nn.layer_widths[layer_index + 1]\n",
    "    icnns.append([])\n",
    "    for k in range(in_size // group_size):\n",
    "        next_net = ICNNLogical([group_size, 10, 10, 10, 1], force_positive_init=False, with_two_layers=False, init_scaling=10,\n",
    "                                 init_all_with_zeros=False)\n",
    "        icnns[i].append(next_net)\n",
    "    if in_size % group_size > 0:\n",
    "        next_net = ICNNLogical([in_size % group_size, 10, 10, 10, 1], force_positive_init=False, with_two_layers=False,\n",
    "                               init_scaling=10,\n",
    "                               init_all_with_zeros=False)\n",
    "        icnns[i].append(next_net)\n",
    "\n",
    "icnns = \\\n",
    "    multidhov.start_verification(nn, test_image, icnns, group_size, eps=eps, icnn_epochs=100, icnn_batch_size=1000, break_after=1,\n",
    "                                 sample_count=1000, sample_new=False, use_over_approximation=True,\n",
    "                                 sample_over_input_space=False, sample_over_output_space=True,\n",
    "                                 force_inclusion_steps=0, preemptive_stop=False, even_gradient_training=False,\n",
    "                                 keep_ambient_space=True, data_grad_descent_steps=0, train_outer=False,\n",
    "                                 should_plot=\"none\", optimizer=\"SdLBFGS\", init_network=True, adapt_lambda=\"none\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounds affine out \n",
      " [[tensor([-2.3574, -2.5409, -2.5436,  ..., -3.9670, -2.9755, -2.7407],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>), tensor([3.3453, 3.1491, 3.1030,  ..., 1.9234, 2.8196, 2.8528],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)], [tensor([-22.9805, -22.0479, -23.9054, -21.6267, -22.8226, -22.6242, -22.6367,\n",
      "        -24.7446, -22.5921, -22.1851, -22.8050, -22.4265, -21.3492, -21.7042,\n",
      "        -23.4169, -24.9769, -24.2969, -21.8832, -21.1646, -22.4530, -22.2679,\n",
      "        -21.8385, -21.4068, -21.7850, -22.2776, -22.9425, -22.4564, -22.1282,\n",
      "        -24.0513, -24.3423, -23.1279, -22.4770, -21.3975, -21.8276, -22.4681,\n",
      "        -21.8846, -21.9831, -21.7638, -23.9613, -23.7972, -20.9416, -22.6994,\n",
      "        -23.9207, -20.6402, -23.0054, -22.7424, -25.2985, -21.6423, -22.1560,\n",
      "        -23.6571, -23.7020, -21.1016, -22.6308, -20.9909, -21.3039, -24.4835,\n",
      "        -23.4869, -21.9936, -23.3225, -21.4309, -21.3916, -22.7663, -22.6741,\n",
      "        -23.6378, -20.1958, -21.5368, -22.5335, -20.4742, -22.7397, -22.3947,\n",
      "        -22.9322, -22.8139, -22.4191, -21.9615, -22.7727, -22.7349, -21.8633,\n",
      "        -20.5398, -21.1912, -23.6885, -23.1307, -22.0695, -23.4798, -21.6753,\n",
      "        -25.1678, -21.5518, -23.8016, -21.9957, -23.7644, -21.7697, -22.5523,\n",
      "        -22.8065, -21.8945, -21.0936, -22.3660, -20.4094, -21.8993, -21.1016,\n",
      "        -24.5994, -22.4798, -21.4057, -24.1422, -20.3033, -23.3664, -21.4767,\n",
      "        -20.9239, -20.5684, -24.0780, -21.9032, -19.5073, -21.4988, -23.2681,\n",
      "        -21.4240, -22.4882, -23.4500, -22.3304, -22.4047, -23.3899, -22.8226,\n",
      "        -23.1079, -22.2829, -22.6286, -21.7435, -23.4580, -22.4383, -22.4786,\n",
      "        -25.2687, -22.3458, -20.7176, -23.3851, -23.9682, -23.7439, -21.7964,\n",
      "        -22.4726, -24.3229, -22.6299, -23.1588, -21.4810, -23.3001, -21.9795,\n",
      "        -22.8163, -21.8654, -22.3271, -23.8730, -25.9083, -22.5751, -20.8237,\n",
      "        -21.2648, -23.2181, -22.8674, -22.6266, -23.2263, -21.9284, -23.1611,\n",
      "        -21.1485, -22.5857, -22.8241, -23.1509, -23.3812, -21.8250, -22.4792,\n",
      "        -21.2570, -22.5972, -20.9111, -22.4100, -22.0425, -24.0356, -23.6000,\n",
      "        -22.4048, -20.4521, -22.3821, -21.9357, -22.7136, -23.4774, -21.7692,\n",
      "        -24.0591, -23.3402, -22.7781, -22.2443, -24.2500, -22.0290, -22.2439,\n",
      "        -23.3801, -23.4176, -24.3650, -23.8411, -23.4364, -24.0918, -24.0368,\n",
      "        -23.2972, -21.5938, -22.7395, -21.2880, -21.8786, -22.3780, -23.1843,\n",
      "        -22.8551, -22.6388, -25.9782, -23.4559, -22.9922, -21.7824, -22.8856,\n",
      "        -24.5938, -23.4640, -22.1634, -22.1129, -25.0148, -25.0525, -21.8918,\n",
      "        -22.9592, -20.9152, -19.7267, -21.3348, -22.8330, -20.2021, -21.8515,\n",
      "        -22.6523, -21.2989, -23.7928, -24.5412, -22.3451, -23.7712, -22.0511,\n",
      "        -22.3832, -23.4662, -22.7438, -21.7707, -19.4258, -22.5766, -21.5434,\n",
      "        -22.2213, -21.0686, -22.4702, -23.3436, -21.0889, -20.1750, -23.6505,\n",
      "        -22.8441, -21.7884, -23.2214, -23.1389, -22.1083, -23.7699, -22.7485,\n",
      "        -22.7814, -22.6064, -21.9535, -20.3298, -21.3843, -20.4925, -23.7364,\n",
      "        -23.9440, -21.7000, -22.2123, -23.5121, -22.5949, -22.2508, -21.9855,\n",
      "        -21.0848, -23.1471, -21.6641, -21.2662, -23.1385, -22.9549, -21.0211,\n",
      "        -22.6428, -22.6692, -22.4205, -23.2337, -23.6091, -21.5237, -21.7289,\n",
      "        -21.9035, -22.6519, -22.8560, -23.2694, -22.0060, -22.9772, -21.8454,\n",
      "        -22.9917, -21.9736, -22.4730, -21.2166, -23.6010, -22.9614, -24.7775,\n",
      "        -22.9726, -23.5240, -23.7233, -23.0263, -22.5108, -23.6529, -21.5504,\n",
      "        -22.8885, -25.3846, -24.1108, -21.8677, -21.2416, -22.1643, -21.1119,\n",
      "        -22.4783, -23.6873, -20.7179, -21.7250, -23.3005, -22.7718, -21.7914,\n",
      "        -21.9635, -24.6604, -22.9410, -23.6504, -23.0472, -21.4103, -21.0605,\n",
      "        -21.7785, -21.8483, -22.3741, -22.1847, -23.4618, -22.5264, -25.3580,\n",
      "        -22.3269, -25.1593, -22.2487, -23.6518, -20.9669, -21.6683, -21.9215,\n",
      "        -21.6762, -22.1945, -23.7330, -21.9889, -23.2778, -23.8996, -21.7119,\n",
      "        -22.6576, -22.7731, -20.3314, -22.2501, -22.1904, -23.8590, -22.4526,\n",
      "        -23.3333, -22.0226, -23.0642, -23.1592, -23.0905, -23.5581, -22.5295,\n",
      "        -19.7551, -22.8412, -22.6498, -22.3912, -23.7310, -23.4082, -21.8507,\n",
      "        -23.1980, -22.6356, -21.6382, -23.0317, -22.0612, -22.9041, -24.6352,\n",
      "        -23.0790, -22.5214, -23.8366, -23.6659, -22.5794, -23.5997, -22.8851,\n",
      "        -22.1011, -23.7299, -23.9254, -23.3102, -22.9722, -22.6416, -23.4442,\n",
      "        -21.3327, -21.1385, -23.2545, -21.9946, -22.1057, -21.2098, -22.4374,\n",
      "        -21.5226, -22.1002, -19.9012, -23.3274, -22.5517, -22.0330, -21.4426,\n",
      "        -23.8332, -23.1652, -22.4915, -22.2239, -22.2330, -23.0733, -22.0351,\n",
      "        -24.9996, -21.4175, -24.1562, -22.0597, -21.8510, -23.8194, -21.8890,\n",
      "        -22.8970, -22.7781, -22.8551, -24.3793, -22.5389, -22.5103, -21.4376,\n",
      "        -23.2738, -22.6922, -21.7669, -23.0149, -21.9196, -23.0978, -21.4106,\n",
      "        -23.5057, -23.1281, -22.7004, -21.7345, -23.3445, -21.9086, -18.4250,\n",
      "        -21.9161, -21.8757, -23.9362, -23.6634, -22.0875, -23.5107, -20.3361,\n",
      "        -24.0144, -23.3444, -22.5977, -20.9013, -23.2286, -20.3257, -22.7110,\n",
      "        -22.6589, -22.6069, -21.7017, -26.4973, -23.9599, -21.6081, -24.4053,\n",
      "        -22.1220, -21.9187, -19.6294, -22.0467, -24.2576, -21.7285, -21.6695,\n",
      "        -22.4791, -22.7267, -23.9554, -23.4730, -22.8098, -23.3298, -23.3025,\n",
      "        -22.4744, -22.7432, -21.2188, -22.5566, -22.4636, -23.1422, -22.5086,\n",
      "        -21.3382, -21.5564, -22.0066, -25.6797, -24.8380, -22.5371, -24.5542,\n",
      "        -22.8922, -23.1516, -22.2449, -23.7486, -21.9439, -22.5064, -22.7702,\n",
      "        -21.4348, -23.4945, -23.0390, -24.3771, -24.4458, -23.8930, -22.3291,\n",
      "        -22.7562, -21.7683, -23.2232, -21.3992, -22.3348, -24.1145, -22.0531,\n",
      "        -22.3700, -21.5395, -24.2200, -18.9779, -22.9599, -23.3817, -21.3538,\n",
      "        -21.1125, -22.6359, -21.1278, -24.2988, -21.5444, -22.6791, -22.3160,\n",
      "        -24.3759], dtype=torch.float64, grad_fn=<AddBackward0>), tensor([23.2616, 24.1940, 27.1293, 28.3290, 26.6395, 28.3782, 29.5983, 27.8763,\n",
      "        25.7443, 23.4395, 25.5927, 28.2726, 26.9012, 28.4452, 25.4781, 25.2136,\n",
      "        26.8002, 26.1334, 25.0419, 25.1351, 26.7173, 27.7019, 27.4413, 26.1341,\n",
      "        26.2022, 25.9648, 26.4764, 25.2930, 26.1527, 26.2972, 21.3746, 26.8674,\n",
      "        28.0191, 26.1952, 24.2005, 26.4313, 25.7727, 28.8430, 24.1973, 23.2079,\n",
      "        26.6483, 22.9092, 25.6422, 27.6520, 25.3620, 23.2446, 21.0206, 26.8537,\n",
      "        26.8079, 25.6683, 25.1001, 24.1063, 25.2005, 29.6457, 28.5609, 28.4134,\n",
      "        26.1284, 28.2702, 23.9200, 28.1925, 24.3350, 24.6481, 25.0684, 27.0754,\n",
      "        28.3162, 25.9924, 25.0244, 27.5826, 26.2695, 26.7169, 25.1341, 23.7469,\n",
      "        22.9954, 27.3312, 26.8564, 23.6735, 25.9913, 30.6723, 31.1468, 24.4063,\n",
      "        25.1794, 29.0704, 23.0557, 25.5786, 24.5311, 27.1110, 23.9938, 25.7725,\n",
      "        22.8216, 26.0715, 23.5807, 22.7585, 29.4621, 32.8065, 26.0506, 27.5470,\n",
      "        24.7511, 28.2304, 29.3449, 23.5470, 25.7200, 26.8985, 28.9001, 23.0095,\n",
      "        31.6010, 27.8608, 28.2724, 26.5810, 24.4510, 29.3953, 25.9563, 24.6801,\n",
      "        26.7043, 21.7665, 25.0596, 25.5160, 27.7792, 23.3555, 24.7355, 24.2221,\n",
      "        26.3963, 26.1109, 26.3853, 25.8328, 23.8826, 24.5109, 26.0200, 24.2874,\n",
      "        27.5708, 23.3777, 24.5922, 26.1850, 23.7353, 27.0816, 23.3458, 24.4371,\n",
      "        25.5472, 29.0176, 24.6707, 24.9680, 24.9581, 28.8232, 23.8217, 29.5265,\n",
      "        25.2833, 27.2394, 26.2359, 26.3342, 24.2855, 23.2506, 23.8667, 26.8578,\n",
      "        26.2235, 28.5302, 27.9757, 22.8475, 25.6529, 31.3139, 26.7996, 25.0752,\n",
      "        24.6899, 25.9736, 24.6025, 27.9367, 24.3421, 25.2567, 28.9839, 24.4600,\n",
      "        27.5005, 25.8150, 31.3512, 25.3141, 24.5343, 23.5778, 24.6176, 23.9956,\n",
      "        21.8917, 25.4095, 25.5218, 22.2921, 27.5997, 24.3081, 28.5392, 21.5150,\n",
      "        22.3482, 24.2623, 23.3113, 24.9231, 26.5013, 24.2869, 27.2267, 23.8626,\n",
      "        25.9140, 25.4723, 23.8817, 26.3310, 28.3913, 24.0719, 25.0799, 24.5253,\n",
      "        26.6765, 24.8021, 24.1451, 21.0303, 25.7602, 24.2956, 23.7284, 23.7337,\n",
      "        22.5748, 23.5431, 23.0562, 27.2849, 27.0487, 30.9366, 26.4537, 26.4577,\n",
      "        24.3562, 25.5953, 25.6479, 23.7082, 20.9635, 24.0318, 28.6954, 26.2656,\n",
      "        23.8828, 26.4143, 23.8933, 29.3652, 26.5800, 25.9360, 24.5318, 24.0279,\n",
      "        24.3549, 25.1357, 24.6931, 23.9432, 25.6752, 22.9566, 23.4298, 24.6688,\n",
      "        27.0546, 25.5689, 25.3620, 24.7241, 25.0980, 24.5243, 28.1371, 25.6663,\n",
      "        26.1541, 26.4718, 27.4798, 25.5512, 21.2477, 27.6635, 26.8365, 28.5718,\n",
      "        25.6328, 24.7772, 30.8367, 28.1816, 24.4247, 25.9531, 26.3875, 22.6158,\n",
      "        22.6717, 28.9512, 24.4473, 24.2014, 26.0654, 23.9433, 23.1767, 25.6305,\n",
      "        29.9041, 26.9127, 24.4064, 26.2778, 20.1963, 23.3105, 22.5652, 26.4504,\n",
      "        25.9714, 25.5000, 23.3410, 25.3184, 21.6080, 24.0331, 25.4239, 23.9698,\n",
      "        23.8699, 21.5815, 23.2860, 25.8028, 28.5657, 27.6749, 24.3902, 24.6644,\n",
      "        23.1787, 26.8422, 27.1281, 23.7292, 27.2585, 24.9178, 26.0535, 29.7532,\n",
      "        24.4242, 21.6850, 23.3730, 28.9671, 25.9184, 27.9611, 27.2690, 23.5572,\n",
      "        26.0065, 27.2167, 26.4007, 24.2292, 27.7511, 27.2250, 28.9616, 23.5500,\n",
      "        25.7242, 21.3883, 23.1823, 21.0707, 23.3736, 25.4352, 24.5623, 24.6742,\n",
      "        25.4239, 24.1380, 24.7698, 21.4048, 24.2484, 25.1096, 25.8970, 23.4441,\n",
      "        26.9082, 23.2138, 26.1651, 25.4274, 24.5546, 23.9953, 23.9495, 25.4517,\n",
      "        26.2110, 21.8648, 26.6646, 26.5438, 26.3204, 25.1286, 28.9792, 23.1943,\n",
      "        25.6078, 25.9301, 27.7352, 22.1185, 25.1609, 27.5104, 27.5193, 24.8231,\n",
      "        24.6443, 23.9673, 23.8743, 25.1571, 26.2403, 27.8616, 22.6578, 25.3175,\n",
      "        25.9081, 27.4395, 25.4401, 28.1029, 24.9297, 23.8911, 21.7667, 29.1458,\n",
      "        23.1339, 24.9712, 27.6311, 29.0490, 22.9317, 26.0621, 28.0458, 30.4257,\n",
      "        26.1814, 27.2855, 24.2467, 30.9724, 23.4015, 26.4494, 25.2211, 24.8406,\n",
      "        27.4104, 23.3429, 23.8399, 23.2407, 30.2686, 25.6033, 25.6845, 24.7503,\n",
      "        26.2144, 21.3347, 25.8634, 25.4482, 21.2494, 25.6258, 23.5085, 24.6802,\n",
      "        26.4768, 21.8553, 22.7805, 22.6340, 24.2284, 23.5135, 26.0356, 25.9076,\n",
      "        28.0305, 23.6880, 22.4660, 30.4725, 23.7572, 24.2869, 24.3080, 26.8476,\n",
      "        26.8081, 22.6233, 33.8267, 23.5318, 25.6681, 25.5496, 26.5379, 26.7432,\n",
      "        22.7232, 25.9250, 23.1271, 27.1422, 24.7410, 28.4030, 25.4321, 27.6922,\n",
      "        28.6148, 24.8522, 23.2486, 23.7867, 25.9834, 25.1077, 27.0176, 28.9430,\n",
      "        23.2025, 26.5470, 28.6655, 24.0306, 23.2962, 25.0676, 23.4538, 25.2685,\n",
      "        22.6847, 24.3000, 21.5749, 27.3233, 23.7222, 22.8533, 26.3618, 26.4467,\n",
      "        28.9290, 25.2549, 25.8632, 21.9055, 25.1300, 24.8260, 28.0489, 22.1606,\n",
      "        23.8900, 23.9010, 27.2149, 22.2764, 25.0254, 22.3991, 24.6488, 24.6766,\n",
      "        23.3228, 24.5597, 27.5725, 30.9721, 23.1363, 23.9138, 26.4031, 27.8453,\n",
      "        24.6923, 26.5470, 23.0404, 26.9845, 22.8795, 25.8688, 27.9293, 26.5702,\n",
      "        25.0271, 23.8018, 24.0128, 24.0527, 33.9675, 23.2119, 23.1218, 22.9348,\n",
      "        29.0019, 24.1404, 30.8782, 28.1470, 25.6905, 24.6149, 32.2108, 23.8261],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)], [tensor([-342.9977, -426.5471, -373.2269, -305.7694, -346.4352, -372.9309,\n",
      "        -423.3731, -450.0294, -447.1579, -400.9472], dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>), tensor([414.9492, 437.6598, 392.5106, 324.9417, 370.6409, 339.1115, 389.9091,\n",
      "        438.9411, 389.5969, 418.2401], dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)]]\n",
      "bounds layer out \n",
      " [[tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64,\n",
      "       grad_fn=<MaximumBackward0>), tensor([3.3453, 3.1491, 3.1030,  ..., 1.9234, 2.8196, 2.8528],\n",
      "       dtype=torch.float64, grad_fn=<MaximumBackward0>)], [tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64,\n",
      "       grad_fn=<MaximumBackward0>), tensor([23.2616, 24.1940, 27.1293, 28.3290, 26.6395, 28.3782, 29.5983, 27.8763,\n",
      "        25.7443, 23.4395, 25.5927, 28.2726, 26.9012, 28.4452, 25.4781, 25.2136,\n",
      "        26.8002, 26.1334, 25.0419, 25.1351, 26.7173, 27.7019, 27.4413, 26.1341,\n",
      "        26.2022, 25.9648, 26.4764, 25.2930, 26.1527, 26.2972, 21.3746, 26.8674,\n",
      "        28.0191, 26.1952, 24.2005, 26.4313, 25.7727, 28.8430, 24.1973, 23.2079,\n",
      "        26.6483, 22.9092, 25.6422, 27.6520, 25.3620, 23.2446, 21.0206, 26.8537,\n",
      "        26.8079, 25.6683, 25.1001, 24.1063, 25.2005, 29.6457, 28.5609, 28.4134,\n",
      "        26.1284, 28.2702, 23.9200, 28.1925, 24.3350, 24.6481, 25.0684, 27.0754,\n",
      "        28.3162, 25.9924, 25.0244, 27.5826, 26.2695, 26.7169, 25.1341, 23.7469,\n",
      "        22.9954, 27.3312, 26.8564, 23.6735, 25.9913, 30.6723, 31.1468, 24.4063,\n",
      "        25.1794, 29.0704, 23.0557, 25.5786, 24.5311, 27.1110, 23.9938, 25.7725,\n",
      "        22.8216, 26.0715, 23.5807, 22.7585, 29.4621, 32.8065, 26.0506, 27.5470,\n",
      "        24.7511, 28.2304, 29.3449, 23.5470, 25.7200, 26.8985, 28.9001, 23.0095,\n",
      "        31.6010, 27.8608, 28.2724, 26.5810, 24.4510, 29.3953, 25.9563, 24.6801,\n",
      "        26.7043, 21.7665, 25.0596, 25.5160, 27.7792, 23.3555, 24.7355, 24.2221,\n",
      "        26.3963, 26.1109, 26.3853, 25.8328, 23.8826, 24.5109, 26.0200, 24.2874,\n",
      "        27.5708, 23.3777, 24.5922, 26.1850, 23.7353, 27.0816, 23.3458, 24.4371,\n",
      "        25.5472, 29.0176, 24.6707, 24.9680, 24.9581, 28.8232, 23.8217, 29.5265,\n",
      "        25.2833, 27.2394, 26.2359, 26.3342, 24.2855, 23.2506, 23.8667, 26.8578,\n",
      "        26.2235, 28.5302, 27.9757, 22.8475, 25.6529, 31.3139, 26.7996, 25.0752,\n",
      "        24.6899, 25.9736, 24.6025, 27.9367, 24.3421, 25.2567, 28.9839, 24.4600,\n",
      "        27.5005, 25.8150, 31.3512, 25.3141, 24.5343, 23.5778, 24.6176, 23.9956,\n",
      "        21.8917, 25.4095, 25.5218, 22.2921, 27.5997, 24.3081, 28.5392, 21.5150,\n",
      "        22.3482, 24.2623, 23.3113, 24.9231, 26.5013, 24.2869, 27.2267, 23.8626,\n",
      "        25.9140, 25.4723, 23.8817, 26.3310, 28.3913, 24.0719, 25.0799, 24.5253,\n",
      "        26.6765, 24.8021, 24.1451, 21.0303, 25.7602, 24.2956, 23.7284, 23.7337,\n",
      "        22.5748, 23.5431, 23.0562, 27.2849, 27.0487, 30.9366, 26.4537, 26.4577,\n",
      "        24.3562, 25.5953, 25.6479, 23.7082, 20.9635, 24.0318, 28.6954, 26.2656,\n",
      "        23.8828, 26.4143, 23.8933, 29.3652, 26.5800, 25.9360, 24.5318, 24.0279,\n",
      "        24.3549, 25.1357, 24.6931, 23.9432, 25.6752, 22.9566, 23.4298, 24.6688,\n",
      "        27.0546, 25.5689, 25.3620, 24.7241, 25.0980, 24.5243, 28.1371, 25.6663,\n",
      "        26.1541, 26.4718, 27.4798, 25.5512, 21.2477, 27.6635, 26.8365, 28.5718,\n",
      "        25.6328, 24.7772, 30.8367, 28.1816, 24.4247, 25.9531, 26.3875, 22.6158,\n",
      "        22.6717, 28.9512, 24.4473, 24.2014, 26.0654, 23.9433, 23.1767, 25.6305,\n",
      "        29.9041, 26.9127, 24.4064, 26.2778, 20.1963, 23.3105, 22.5652, 26.4504,\n",
      "        25.9714, 25.5000, 23.3410, 25.3184, 21.6080, 24.0331, 25.4239, 23.9698,\n",
      "        23.8699, 21.5815, 23.2860, 25.8028, 28.5657, 27.6749, 24.3902, 24.6644,\n",
      "        23.1787, 26.8422, 27.1281, 23.7292, 27.2585, 24.9178, 26.0535, 29.7532,\n",
      "        24.4242, 21.6850, 23.3730, 28.9671, 25.9184, 27.9611, 27.2690, 23.5572,\n",
      "        26.0065, 27.2167, 26.4007, 24.2292, 27.7511, 27.2250, 28.9616, 23.5500,\n",
      "        25.7242, 21.3883, 23.1823, 21.0707, 23.3736, 25.4352, 24.5623, 24.6742,\n",
      "        25.4239, 24.1380, 24.7698, 21.4048, 24.2484, 25.1096, 25.8970, 23.4441,\n",
      "        26.9082, 23.2138, 26.1651, 25.4274, 24.5546, 23.9953, 23.9495, 25.4517,\n",
      "        26.2110, 21.8648, 26.6646, 26.5438, 26.3204, 25.1286, 28.9792, 23.1943,\n",
      "        25.6078, 25.9301, 27.7352, 22.1185, 25.1609, 27.5104, 27.5193, 24.8231,\n",
      "        24.6443, 23.9673, 23.8743, 25.1571, 26.2403, 27.8616, 22.6578, 25.3175,\n",
      "        25.9081, 27.4395, 25.4401, 28.1029, 24.9297, 23.8911, 21.7667, 29.1458,\n",
      "        23.1339, 24.9712, 27.6311, 29.0490, 22.9317, 26.0621, 28.0458, 30.4257,\n",
      "        26.1814, 27.2855, 24.2467, 30.9724, 23.4015, 26.4494, 25.2211, 24.8406,\n",
      "        27.4104, 23.3429, 23.8399, 23.2407, 30.2686, 25.6033, 25.6845, 24.7503,\n",
      "        26.2144, 21.3347, 25.8634, 25.4482, 21.2494, 25.6258, 23.5085, 24.6802,\n",
      "        26.4768, 21.8553, 22.7805, 22.6340, 24.2284, 23.5135, 26.0356, 25.9076,\n",
      "        28.0305, 23.6880, 22.4660, 30.4725, 23.7572, 24.2869, 24.3080, 26.8476,\n",
      "        26.8081, 22.6233, 33.8267, 23.5318, 25.6681, 25.5496, 26.5379, 26.7432,\n",
      "        22.7232, 25.9250, 23.1271, 27.1422, 24.7410, 28.4030, 25.4321, 27.6922,\n",
      "        28.6148, 24.8522, 23.2486, 23.7867, 25.9834, 25.1077, 27.0176, 28.9430,\n",
      "        23.2025, 26.5470, 28.6655, 24.0306, 23.2962, 25.0676, 23.4538, 25.2685,\n",
      "        22.6847, 24.3000, 21.5749, 27.3233, 23.7222, 22.8533, 26.3618, 26.4467,\n",
      "        28.9290, 25.2549, 25.8632, 21.9055, 25.1300, 24.8260, 28.0489, 22.1606,\n",
      "        23.8900, 23.9010, 27.2149, 22.2764, 25.0254, 22.3991, 24.6488, 24.6766,\n",
      "        23.3228, 24.5597, 27.5725, 30.9721, 23.1363, 23.9138, 26.4031, 27.8453,\n",
      "        24.6923, 26.5470, 23.0404, 26.9845, 22.8795, 25.8688, 27.9293, 26.5702,\n",
      "        25.0271, 23.8018, 24.0128, 24.0527, 33.9675, 23.2119, 23.1218, 22.9348,\n",
      "        29.0019, 24.1404, 30.8782, 28.1470, 25.6905, 24.6149, 32.2108, 23.8261],\n",
      "       dtype=torch.float64, grad_fn=<MaximumBackward0>)], [tensor([-342.9977, -426.5471, -373.2269, -305.7694, -346.4352, -372.9309,\n",
      "        -423.3731, -450.0294, -447.1579, -400.9472], dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>), tensor([414.9492, 437.6598, 392.5106, 324.9417, 370.6409, 339.1115, 389.9091,\n",
      "        438.9411, 389.5969, 418.2401], dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)]]\n"
     ]
    }
   ],
   "source": [
    "input_flattened = torch.flatten(test_image)\n",
    "bounds_affine_out, bounds_layer_out = nn.calculate_box_bounds([input_flattened.add(-eps), input_flattened.add(eps)])\n",
    "print(\"bounds affine out \\n {}\".format(bounds_affine_out))\n",
    "print(\"bounds layer out \\n {}\".format(bounds_layer_out))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "milp_verifier = MILPVerifier(nn, test_image, eps, print_log=True)\n",
    "snv_verifier = SingleNeuronVerifier(nn, test_image, eps, print_log=True)\n",
    "#dhov_verifier = DHOVVerifier(icnns, group_size, nn, test_image, 1)\n",
    "\n",
    "milp_verifier.generate_constraints_for_net()\n",
    "snv_verifier.generate_constraints_for_net()\n",
    "#dhov_verifier.generate_constraints_for_net()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "layer_index = 0\n",
    "neuron_index = 0\n",
    "milp_model = milp_verifier.model\n",
    "snv_model = snv_verifier.model\n",
    "#dhov_model = dhov_verifier.model\n",
    "dhov_model = icnn_model(icnns[layer_index][0], nn, test_image, eps, layer_index, 0, 2, print_log=True)\n",
    "milp_model.update()\n",
    "snv_model.update()\n",
    "dhov_model.update()\n",
    "\"\"\"all_var = milp_model.getVars()\n",
    "for var in all_var:\n",
    "    print(var)\"\"\"\n",
    "neuron_name = \"relu_var{}[{}]\".format(2*layer_index, neuron_index)\n",
    "icnn_neuron_name = \"icnn_var[{}]\".format(neuron_index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max value: 3.345335506481878\n",
      "===================================================================================\n",
      "max value: 3.345335506481878\n",
      "===================================================================================\n",
      "max value: 3.345335506481879\n",
      "===================================================================================\n"
     ]
    }
   ],
   "source": [
    "milp_copy = milp_model.copy()\n",
    "snv_copy = snv_model.copy()\n",
    "dhov_copy = dhov_model.copy()\n",
    "\n",
    "milp_copy.Params.LogToConsole = 0\n",
    "snv_copy.Params.LogToConsole = 0\n",
    "dhov_copy.Params.LogToConsole = 0\n",
    "\n",
    "add_max_constr(milp_copy, neuron_name)\n",
    "add_max_constr(snv_copy, neuron_name)\n",
    "add_max_constr(dhov_copy, icnn_neuron_name)\n",
    "\n",
    "optimize_model(milp_copy, neuron_name)\n",
    "print(\"===================================================================================\")\n",
    "optimize_model(snv_copy, neuron_name)\n",
    "print(\"===================================================================================\")\n",
    "optimize_model(dhov_copy, icnn_neuron_name)\n",
    "print(\"===================================================================================\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max value: 0.0\n",
      "===================================================================================\n",
      "max value: 0.0\n",
      "===================================================================================\n",
      "max value: 0.0\n"
     ]
    }
   ],
   "source": [
    "milp_copy = milp_model.copy()\n",
    "snv_copy = snv_model.copy()\n",
    "dhov_copy = dhov_model.copy()\n",
    "\n",
    "milp_copy.Params.LogToConsole = 0\n",
    "snv_copy.Params.LogToConsole = 0\n",
    "dhov_copy.Params.LogToConsole = 0\n",
    "\n",
    "add_min_constr(milp_copy, neuron_name) #affine_var0[0]\n",
    "add_min_constr(snv_copy, neuron_name)\n",
    "add_min_constr(dhov_copy, icnn_neuron_name)\n",
    "\n",
    "optimize_model(milp_copy, neuron_name)\n",
    "print(\"===================================================================================\")\n",
    "optimize_model(snv_copy, neuron_name)\n",
    "print(\"===================================================================================\")\n",
    "optimize_model(dhov_copy, icnn_neuron_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}