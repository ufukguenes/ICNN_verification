{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from script.NeuralNets.Networks import SequentialNN, ICNN, ICNNApproxMax, ICNNLogical\n",
    "from script.settings import device, data_type\n",
    "import script.DHOV.MultiDHOV as multidhov\n",
    "from script.Verification.Verifier import SingleNeuronVerifier, MILPVerifier, DHOVVerifier\n",
    "import gurobipy as grp\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def add_max_constr(model, neuron_name):\n",
    "    neuron_var = model.getVarByName(neuron_name)\n",
    "    model.setObjective(neuron_var, grp.GRB.MAXIMIZE)\n",
    "\n",
    "def add_min_constr(model, neuron):\n",
    "    neuron_var = model.getVarByName(neuron)\n",
    "    model.setObjective(neuron_var, grp.GRB.MINIMIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def optimize_model(model, neuron_name):\n",
    "    model.update()\n",
    "    model.optimize()\n",
    "    if model.Status == grp.GRB.OPTIMAL:\n",
    "        print(\"opt value: {}\".format(model.getVarByName(neuron_name).getAttr(\"x\")))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def icnn_model(icnn, nn, input_x, eps, layer_index, from_neuron, to_neuron, print_log=False):\n",
    "    m = grp.Model()\n",
    "    if not print_log:\n",
    "        m.Params.LogToConsole = 0\n",
    "\n",
    "    input_flattened = torch.flatten(input_x)\n",
    "    bounds_affine_out, bounds_layer_out = nn.calculate_box_bounds(\n",
    "        [input_flattened.add(-eps), input_flattened.add(eps)])\n",
    "\n",
    "    parameter_list = list(nn.parameters())\n",
    "\n",
    "    input_size = len(parameter_list[2*layer_index])\n",
    "    lb = bounds_layer_out[layer_index][0].detach().cpu().numpy()\n",
    "    ub = bounds_layer_out[layer_index][1].detach().cpu().numpy()\n",
    "    in_var = m.addMVar(input_size, lb=lb, ub=ub, name=\"icnn_var\")\n",
    "\n",
    "    low = bounds_layer_out[layer_index][0][from_neuron: to_neuron]\n",
    "    up = bounds_layer_out[layer_index][1][from_neuron: to_neuron]\n",
    "    constraint_bounds_affine_out, constraint_bounds_layer_out = icnn.calculate_box_bounds([low, up])\n",
    "    icnn.add_max_output_constraints(m, in_var[from_neuron: to_neuron], constraint_bounds_affine_out, constraint_bounds_layer_out)\n",
    "\n",
    "    return m"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\"\"\"W1 = [1. 1.; 1. -1.]\n",
    "    b1 = [0., 0.]\n",
    "    W2 = [1. 1.; 1. -1.]\n",
    "    b2 = [-0.5, 0.]\n",
    "    W3 = [-1. 1.; 1. 1.]\n",
    "    b3 = [3., 0.] \"\"\"\n",
    "\n",
    "\"\"\"nn = SequentialNN([2, 2, 2, 2])\n",
    "\n",
    "with torch.no_grad():\n",
    "    parameter_list = list(nn.parameters())\n",
    "    parameter_list[0].data = torch.tensor([[1, 1], [1, -1]], dtype=data_type).to(device)\n",
    "    parameter_list[1].data = torch.tensor([0, 0], dtype=data_type).to(device)\n",
    "    parameter_list[2].data = torch.tensor([[1, 1], [1, -1]], dtype=data_type).to(device)\n",
    "    parameter_list[3].data = torch.tensor([-0.5, 0], dtype=data_type).to(device)\n",
    "    parameter_list[4].data = torch.tensor([[-1, 1], [1, 1]], dtype=data_type).to(device)\n",
    "    parameter_list[5].data = torch.tensor([3, 0], dtype=data_type).to(device)\n",
    "\n",
    "test_image = torch.tensor([[0, 0]], dtype=data_type).to(device)\"\"\"\n",
    "\n",
    "transform = Compose([ToTensor(),\n",
    "                         Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "                        )\n",
    "\n",
    "training_data = CIFAR10(root=\"../../cifar\", train=True, download=True, transform=transform)\n",
    "images, labels = training_data.__getitem__(0)\n",
    "test_image, test_label = torch.unsqueeze(images, 0).to(dtype=data_type).to(device), torch.unsqueeze(\n",
    "    torch.tensor(labels), 0).to(dtype=data_type).to(device)\n",
    "\n",
    "nn = SequentialNN([32 * 32 * 3, 1024, 512, 10])\n",
    "nn.load_state_dict(torch.load(\"../../cifar_fc.pth\", map_location=torch.device(device)), strict=False)\n",
    "parameter_list = list(nn.parameters())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "\n",
      "approximation of layer: 0\n",
      "number of fixed neurons [571]\n",
      "fixed_neuron_per_layer_lower \n",
      " [[0, 1, 6, 14, 17, 21, 27, 35, 36, 48, 50, 52, 59, 89, 96, 98, 100, 105, 106, 107, 112, 116, 117, 119, 122, 126, 129, 132, 133, 135, 138, 147, 148, 149, 151, 153, 157, 160, 161, 165, 168, 175, 176, 177, 183, 184, 185, 190, 193, 194, 195, 197, 200, 205, 213, 214, 215, 216, 218, 221, 223, 225, 228, 230, 232, 234, 239, 246, 249, 251, 253, 266, 273, 275, 277, 283, 285, 290, 291, 293, 298, 299, 303, 307, 314, 320, 321, 328, 330, 331, 337, 342, 346, 355, 358, 360, 371, 373, 375, 377, 380, 385, 392, 398, 404, 405, 409, 410, 411, 418, 419, 422, 426, 431, 434, 435, 438, 446, 447, 448, 457, 467, 477, 479, 480, 481, 486, 488, 489, 490, 494, 499, 507, 512, 514, 516, 519, 522, 528, 531, 546, 550, 554, 555, 556, 557, 558, 560, 566, 568, 569, 576, 597, 598, 601, 606, 607, 615, 620, 626, 629, 635, 637, 641, 642, 648, 661, 662, 666, 673, 675, 676, 678, 682, 686, 687, 691, 694, 697, 699, 700, 701, 702, 703, 706, 709, 718, 724, 726, 731, 742, 744, 748, 749, 767, 768, 772, 773, 774, 776, 778, 789, 791, 796, 797, 802, 803, 806, 807, 811, 813, 821, 822, 825, 826, 827, 836, 837, 840, 844, 845, 850, 852, 859, 862, 863, 864, 872, 883, 886, 888, 891, 893, 896, 902, 903, 904, 914, 917, 918, 926, 927, 928, 930, 932, 943, 944, 947, 948, 959, 962, 963, 968, 975, 978, 979, 983, 989, 994, 999, 1006, 1008, 1010, 1011, 1014, 1018, 1019, 1020]]\n",
      "fixed_neuron_per_layer_upper \n",
      " [[4, 7, 8, 12, 13, 15, 19, 20, 25, 29, 31, 33, 34, 37, 38, 41, 42, 43, 44, 45, 49, 54, 57, 60, 62, 64, 66, 67, 70, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 92, 99, 103, 108, 109, 110, 115, 118, 121, 127, 136, 143, 144, 152, 159, 162, 163, 167, 172, 173, 179, 181, 186, 187, 192, 201, 203, 206, 207, 209, 210, 212, 222, 224, 226, 229, 231, 235, 236, 242, 245, 247, 252, 254, 256, 259, 262, 265, 269, 270, 274, 276, 289, 296, 300, 301, 302, 306, 308, 312, 315, 324, 326, 329, 332, 338, 344, 345, 350, 352, 356, 357, 361, 362, 364, 367, 369, 370, 374, 379, 381, 386, 387, 393, 394, 395, 399, 402, 403, 408, 412, 413, 416, 423, 428, 432, 443, 449, 454, 458, 463, 464, 465, 469, 475, 484, 491, 496, 509, 518, 520, 521, 523, 524, 525, 527, 534, 535, 536, 543, 544, 553, 575, 578, 580, 581, 587, 590, 591, 592, 593, 594, 595, 596, 600, 602, 603, 608, 612, 613, 614, 616, 617, 622, 625, 627, 628, 630, 631, 634, 638, 640, 644, 650, 652, 653, 654, 656, 657, 658, 659, 663, 671, 672, 674, 679, 684, 689, 690, 693, 696, 705, 707, 714, 725, 728, 729, 737, 745, 746, 747, 750, 751, 752, 753, 756, 758, 760, 761, 762, 763, 770, 780, 784, 785, 795, 798, 799, 801, 808, 810, 812, 815, 819, 823, 828, 830, 833, 834, 835, 838, 839, 841, 846, 849, 854, 856, 858, 861, 865, 867, 871, 877, 879, 880, 897, 899, 905, 907, 911, 913, 915, 919, 920, 929, 935, 938, 942, 946, 949, 951, 952, 957, 965, 970, 971, 972, 977, 981, 984, 985, 986, 988, 990, 992, 993, 995, 997, 998, 1000, 1001, 1013, 1017, 1021]]\n",
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2023-11-12\n",
      "layer progress, group 0 of 10 \n",
      "        time for training: 0.07312226295471191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ufuk\\Documents\\Programming\\ICNN_verification\\script\\Optimizer\\sdlbfgs.py:83: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1420.)\n",
      "  p.data.add_(step_size, update[offset:offset + numel].view_as(p.data))\n"
     ]
    }
   ],
   "source": [
    "group_size = 6\n",
    "eps = 0.01\n",
    "icnns = []\n",
    "for i in range((len(parameter_list) - 2) // 2):\n",
    "    layer_index = int(i / 2)\n",
    "    in_size = nn.layer_widths[layer_index + 1]\n",
    "    icnns.append([])\n",
    "    for k in range(in_size // group_size):\n",
    "        next_net = ICNNLogical([group_size, 10, 10, 10, 1], force_positive_init=False, with_two_layers=False, init_scaling=10,\n",
    "                                 init_all_with_zeros=False)\n",
    "        icnns[i].append(next_net)\n",
    "    if in_size % group_size > 0:\n",
    "        next_net = ICNNLogical([in_size % group_size, 10, 10, 10, 1], force_positive_init=False, with_two_layers=False,\n",
    "                               init_scaling=10,\n",
    "                               init_all_with_zeros=False)\n",
    "        icnns[i].append(next_net)\n",
    "\n",
    "print(math.ceil(nn.layer_widths[1] / group_size)+1)\n",
    "icnns, all_group_indices, fixed_neuron_per_layer_lower, fixed_neuron_per_layer_upper, bounds_affine_out, bounds_layer_out = \\\n",
    "    multidhov.start_verification(nn, test_image, icnns, group_size, eps=eps, icnn_epochs=10, icnn_batch_size=1000,\n",
    "                                 sample_count=100, sample_new=False, use_over_approximation=True, break_after=math.ceil(nn.layer_widths[1] / group_size)+1,\n",
    "                                 sample_over_input_space=False, sample_over_output_space=True, use_icnn_bounds=True, use_fixed_neurons=True,\n",
    "                                 force_inclusion_steps=0, preemptive_stop=False, even_gradient_training=False,\n",
    "                                 keep_ambient_space=True, data_grad_descent_steps=0, train_outer=False,\n",
    "                                 should_plot=\"none\", optimizer=\"SdLBFGS\", init_network=True, adapt_lambda=\"none\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounds affine out \n",
      " [[tensor([ 0.2088,  0.0196, -0.0026,  ..., -1.3163, -0.3677, -0.2236],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>), tensor([ 0.7791,  0.5886,  0.5620,  ..., -0.7273,  0.2118,  0.3357],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)], [tensor([-2.3678, -2.2506, -1.4631, -1.5480, -1.6997, -2.3157, -2.0312, -2.1657,\n",
      "        -2.4451, -2.1094, -2.5736, -1.5114, -1.7204, -1.6585, -1.9900, -2.2936,\n",
      "        -1.8326, -1.9801, -2.0940, -1.9852, -2.4245, -2.3254, -1.7885, -2.0473,\n",
      "        -1.7248, -2.0310, -2.1796, -1.5130, -2.1948, -1.2174, -2.3810, -1.6069,\n",
      "        -2.2250, -1.0840, -2.1734, -1.6119, -2.2030, -2.1282, -2.2584, -2.3525,\n",
      "        -1.9250, -2.2268, -1.9972, -1.8989, -2.1081, -1.8777, -2.4823, -2.0515,\n",
      "        -2.2419, -2.3808, -2.0566, -2.1669, -1.8904, -2.0341, -2.4128, -2.7350,\n",
      "        -2.8463, -1.6933, -2.4030, -2.0562, -1.9528, -2.1218, -2.4711, -2.3628,\n",
      "        -2.2642, -2.0658, -1.8225, -1.7588, -2.5546, -1.6129, -1.8245, -2.1414,\n",
      "        -1.9952, -1.9148, -2.2306, -2.0037, -2.5366, -1.4191, -2.1562, -1.9158,\n",
      "        -2.1424, -1.8178, -2.2539, -2.0352, -2.1833, -1.5824, -2.2810, -1.4759,\n",
      "        -2.3608, -1.8562, -2.4729, -2.1235, -2.2414, -1.7541, -2.2816, -2.0879,\n",
      "        -2.0146, -1.8970, -2.4248, -2.0457, -1.6711, -2.2117, -1.8608, -2.7619,\n",
      "        -1.7872, -1.3919, -1.9690, -1.9393, -2.2661, -1.6468, -2.1895, -2.1893,\n",
      "        -1.9798, -2.1747, -1.8322, -2.2181, -2.1823, -1.9345, -2.2441, -1.9113,\n",
      "        -2.1066, -2.3348, -1.8618, -2.3357, -1.8482, -2.2501, -2.1368, -1.9601,\n",
      "        -1.7689, -2.5784, -2.6144, -2.2968, -2.0713, -2.2545, -2.4273, -2.6481,\n",
      "        -1.8894, -2.2630, -1.9883, -1.8003, -1.3780, -1.8716, -2.0415, -2.0463,\n",
      "        -2.5830, -2.0505, -1.8829, -1.3248, -2.2755, -2.2885, -2.6692, -2.1592,\n",
      "        -1.9485, -1.7881, -1.9013, -2.4198, -1.8273, -1.7009, -1.7606, -1.8754,\n",
      "        -1.7755, -1.9315, -2.2627, -2.0458, -2.3787, -1.7453, -2.1708, -2.3769,\n",
      "        -1.7394, -2.0193, -1.3378, -2.1757, -1.9578, -2.4132, -2.0921, -2.1697,\n",
      "        -2.3222, -2.1388, -1.9731, -2.4500, -1.7567, -2.0712, -1.8797, -2.4587,\n",
      "        -2.4395, -2.0947, -1.4238, -1.6812, -2.0884, -2.0478, -2.1802, -2.1793,\n",
      "        -1.7242, -2.2525, -2.0815, -2.1800, -2.2343, -2.2170, -2.2868, -2.3326,\n",
      "        -2.0985, -1.9025, -2.6216, -2.6766, -2.8230, -2.0409, -1.9765, -2.5223,\n",
      "        -2.5186, -2.3216, -2.1099, -1.8069, -1.8001, -1.8937, -2.3844, -1.5554,\n",
      "        -1.7413, -1.9307, -1.7702, -2.1569, -2.5157, -1.9025, -2.8545, -2.3963,\n",
      "        -1.9962, -2.0257, -2.0677, -1.7343, -1.7432, -1.9814, -2.0761, -1.9112,\n",
      "        -1.9711, -2.5830, -1.7412, -2.0081, -1.8990, -2.3547, -1.9318, -1.6319,\n",
      "        -1.3847, -2.1291, -2.3581, -2.3676, -2.3591, -1.6119, -1.5264, -2.2582,\n",
      "        -1.9452, -2.0983, -1.5434, -2.4950, -2.3965, -2.1024, -2.1247, -2.0916,\n",
      "        -2.4071, -1.6716, -2.2409, -2.0783, -1.9700, -1.7985, -1.9317, -1.9193,\n",
      "        -2.8491, -2.1171, -2.0579, -2.2669, -1.9235, -1.8752, -2.1481, -1.7179,\n",
      "        -2.2722, -2.0011, -2.3476, -1.9064, -2.6052, -1.9573, -2.5701, -2.5449,\n",
      "        -2.2295, -2.4069, -1.8491, -1.6295, -2.2801, -2.3843, -1.8899, -2.2901,\n",
      "        -2.2081, -2.2685, -2.5606, -2.3657, -2.0694, -1.8586, -2.1898, -2.7850,\n",
      "        -2.6666, -2.2256, -1.8080, -2.3675, -2.0387, -1.7699, -1.7594, -1.9638,\n",
      "        -1.7930, -2.5955, -2.2309, -2.1528, -2.1117, -1.1386, -1.7030, -2.1813,\n",
      "        -2.1974, -2.0460, -2.0127, -2.1019, -1.7086, -1.7625, -1.2394, -1.9954,\n",
      "        -2.0407, -2.5292, -2.1938, -2.6148, -1.9193, -1.8724, -2.0353, -1.8261,\n",
      "        -1.7046, -1.8707, -1.7427, -2.3210, -2.0953, -2.0234, -1.8730, -1.9157,\n",
      "        -2.1633, -2.2956, -1.8241, -1.9183, -1.9252, -2.2855, -1.7693, -2.0694,\n",
      "        -2.2078, -2.2395, -1.8100, -1.8931, -2.4937, -1.6027, -1.7344, -2.4556,\n",
      "        -1.7214, -2.1970, -1.7753, -2.5441, -2.1143, -1.7223, -2.5505, -1.8186,\n",
      "        -2.3532, -1.9206, -2.3041, -2.0582, -1.8424, -1.8498, -2.1913, -2.2310,\n",
      "        -2.1394, -2.0301, -2.6459, -1.6225, -2.2951, -2.5787, -2.3691, -2.5596,\n",
      "        -2.5056, -2.1484, -1.7632, -2.2438, -2.3388, -2.1026, -2.3081, -1.9033,\n",
      "        -2.3444, -1.8191, -1.9034, -1.3551, -2.2204, -1.6438, -2.2685, -1.7940,\n",
      "        -1.6682, -2.2030, -2.1190, -2.3994, -2.0798, -2.8044, -1.7596, -2.6453,\n",
      "        -1.6814, -2.1698, -1.7821, -1.6353, -2.7215, -1.7791, -2.2148, -2.1106,\n",
      "        -1.8514, -2.5463, -2.1293, -1.9078, -2.0379, -2.4073, -2.1459, -2.1028,\n",
      "        -2.7059, -2.1078, -2.5884, -1.8202, -2.0828, -2.1631, -2.0025, -1.8010,\n",
      "        -1.8327, -2.1799, -1.9555, -1.9659, -2.4192, -1.6459, -1.8146, -1.9345,\n",
      "        -2.6203, -1.9685, -2.5791, -2.2627, -2.0145, -1.9692, -2.2245, -1.6481,\n",
      "        -1.9672, -2.0471, -2.0556, -2.2650, -2.1470, -2.2929, -2.2956, -1.7933,\n",
      "        -2.4848, -1.8552, -1.7189, -2.2088, -2.2806, -1.4828, -1.8546, -2.1495,\n",
      "        -2.3400, -2.2611, -1.9456, -2.0526, -2.1521, -2.4529, -1.5742, -2.0786,\n",
      "        -1.8988, -1.7005, -2.1786, -2.0611, -2.4032, -1.8272, -2.2149, -2.3349,\n",
      "        -2.3801, -2.3148, -1.7153, -2.4166, -1.9706, -2.0655, -2.4320, -1.9155,\n",
      "        -2.3920, -2.0351, -1.7001, -2.1501, -1.9242, -2.3359, -1.5724, -1.7956,\n",
      "        -2.7937, -1.9470, -2.2252, -2.0580, -2.5420, -2.0681, -1.8897, -1.9469,\n",
      "        -1.9611, -1.8518, -2.0844, -2.6111, -1.7594, -2.0141, -1.9591, -1.8939,\n",
      "        -2.2650, -1.9786, -1.4302, -2.2497, -2.2228, -2.7537, -2.2954, -2.2246],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>), tensor([2.1985, 2.2024, 3.5789, 3.2989, 3.1727, 2.7257, 3.1589, 2.9706, 2.3492,\n",
      "        2.3374, 2.2188, 3.5131, 2.9834, 3.2893, 2.7698, 2.7456, 3.0551, 2.7703,\n",
      "        2.4261, 2.7636, 2.3316, 2.5707, 3.1093, 2.7213, 3.2009, 2.8840, 2.6372,\n",
      "        3.2359, 2.7661, 3.7934, 1.9150, 3.2887, 2.6867, 3.5916, 2.4751, 3.0572,\n",
      "        2.5368, 2.8881, 2.4758, 2.3354, 2.7308, 2.2997, 2.9015, 2.8642, 2.6307,\n",
      "        2.6679, 2.1138, 2.7184, 2.5877, 2.4363, 2.8510, 2.2290, 2.7605, 2.9352,\n",
      "        2.5532, 2.4591, 2.0163, 3.2752, 2.3035, 2.7828, 2.5608, 2.5729, 2.1703,\n",
      "        2.6810, 2.4530, 2.5627, 2.7323, 3.0748, 2.3498, 3.0908, 2.9539, 2.4527,\n",
      "        2.4962, 2.8960, 2.6532, 2.6162, 2.2160, 3.6041, 3.0032, 2.8227, 2.6230,\n",
      "        3.2652, 2.3000, 2.5879, 2.6289, 3.2825, 2.4289, 3.2062, 2.0722, 2.9592,\n",
      "        2.0333, 2.4219, 2.9470, 3.5575, 2.5283, 2.6129, 2.5972, 2.9534, 2.9723,\n",
      "        2.5614, 2.8972, 2.7913, 2.9018, 1.8760, 3.3864, 3.4564, 2.7306, 3.0458,\n",
      "        2.2486, 3.0729, 2.5512, 2.5311, 2.8378, 2.2285, 3.0569, 2.5264, 2.6175,\n",
      "        2.5961, 2.5296, 2.6409, 2.6287, 2.5965, 2.9409, 2.4046, 2.7446, 2.3342,\n",
      "        2.9047, 2.5440, 3.0772, 1.9716, 2.1576, 2.7586, 2.4789, 2.6533, 2.2112,\n",
      "        1.8408, 2.9815, 2.7088, 2.7785, 2.7992, 3.3071, 3.1239, 2.4893, 3.1774,\n",
      "        2.4032, 2.8245, 2.6077, 3.3692, 2.4308, 2.2467, 1.9323, 2.7972, 2.7219,\n",
      "        3.2780, 2.9485, 1.9737, 2.9057, 3.7385, 3.2042, 2.6872, 2.8178, 2.6760,\n",
      "        2.4202, 2.7386, 2.2116, 2.8576, 3.0162, 2.2776, 3.1711, 2.4163, 3.9645,\n",
      "        2.4481, 2.7093, 2.1543, 2.6305, 2.5179, 2.1667, 2.6047, 2.6598, 2.0520,\n",
      "        3.2499, 2.4984, 3.2387, 1.9903, 2.1779, 2.6996, 3.1079, 3.0683, 2.8121,\n",
      "        2.5086, 2.6226, 2.2937, 2.8322, 2.4843, 2.4434, 2.6396, 2.6338, 2.3789,\n",
      "        2.7651, 2.4555, 2.8191, 2.7082, 1.9177, 1.8043, 2.0851, 2.5194, 2.5843,\n",
      "        2.2937, 2.0311, 2.0399, 2.4141, 2.8709, 2.6827, 3.2072, 2.4175, 2.9141,\n",
      "        2.7656, 2.7763, 2.8114, 2.5082, 1.9327, 2.6724, 2.2971, 2.3961, 2.4524,\n",
      "        2.7674, 2.5349, 3.2752, 2.6632, 2.7877, 2.4443, 2.6509, 2.4466, 2.1202,\n",
      "        2.9781, 2.3353, 2.5709, 2.1044, 2.5156, 2.9475, 3.4756, 2.5669, 2.3189,\n",
      "        2.3897, 2.2457, 3.0801, 3.5114, 2.4983, 2.5862, 2.5761, 3.1905, 2.3847,\n",
      "        2.0487, 2.6576, 2.6359, 3.1154, 2.4759, 2.8485, 3.0165, 2.8262, 2.6906,\n",
      "        2.8355, 2.7099, 2.5992, 1.6763, 2.7652, 2.6295, 2.3402, 2.8419, 2.9079,\n",
      "        2.4964, 2.8957, 2.8146, 2.8652, 2.3430, 2.9373, 1.6113, 2.4700, 1.9122,\n",
      "        2.3025, 2.5531, 2.2747, 2.6465, 2.9438, 2.1805, 2.2151, 3.0780, 2.2498,\n",
      "        2.4682, 2.1720, 1.9502, 2.3754, 3.0590, 2.9243, 2.5663, 2.1819, 1.9129,\n",
      "        2.4948, 2.9328, 2.1700, 2.6950, 2.9161, 3.1480, 2.9733, 2.6979, 1.7689,\n",
      "        2.1127, 2.9371, 2.5969, 3.9292, 3.1885, 2.4670, 2.7131, 2.7215, 2.7180,\n",
      "        2.5085, 3.2214, 3.0689, 3.8328, 2.5023, 2.7257, 2.1245, 2.3074, 2.0151,\n",
      "        2.5359, 2.9857, 2.3858, 2.7070, 2.9559, 2.5780, 2.9495, 2.1060, 2.3754,\n",
      "        2.7490, 2.9435, 2.4471, 2.6938, 2.2222, 2.7023, 2.6875, 2.6380, 2.3804,\n",
      "        2.8629, 2.6581, 2.5343, 2.1080, 3.1665, 2.9870, 2.4671, 3.0791, 2.9258,\n",
      "        2.0098, 2.9876, 2.4583, 3.1908, 1.8993, 2.4326, 3.2955, 2.3088, 2.7669,\n",
      "        2.3705, 2.6104, 2.2965, 2.8599, 3.0404, 3.1265, 2.3486, 2.6209, 2.5314,\n",
      "        2.9598, 2.1777, 3.2833, 2.4826, 2.1495, 2.1120, 2.6329, 2.0196, 2.5830,\n",
      "        2.9953, 2.7731, 2.1109, 2.6253, 2.6458, 3.1792, 2.3662, 2.9563, 2.5534,\n",
      "        3.6706, 2.4945, 3.0791, 2.2953, 2.7153, 3.2815, 2.2885, 2.4486, 2.0401,\n",
      "        3.1391, 2.0505, 3.0785, 2.2818, 2.9539, 2.3042, 2.9556, 2.9958, 1.6933,\n",
      "        2.8548, 2.2854, 2.5816, 3.0564, 2.0341, 2.3031, 2.5163, 2.3943, 2.1332,\n",
      "        2.5911, 2.6126, 2.3571, 2.4486, 1.9534, 3.2738, 2.5618, 2.5262, 2.7043,\n",
      "        2.9916, 3.1165, 2.2164, 3.1818, 2.5545, 2.2131, 3.1386, 3.0965, 2.8530,\n",
      "        1.9496, 2.5144, 2.0527, 2.6682, 2.6472, 2.7935, 2.5063, 3.1373, 3.1121,\n",
      "        2.6312, 2.4272, 2.1223, 2.9568, 2.5563, 2.5324, 3.5180, 2.0442, 2.8928,\n",
      "        3.0702, 2.2026, 2.4208, 3.1787, 2.5664, 2.6002, 2.0378, 2.5944, 2.5661,\n",
      "        2.8763, 2.4492, 2.1314, 3.1454, 2.7629, 3.0102, 3.0103, 2.5855, 2.3826,\n",
      "        2.2292, 2.7869, 2.7408, 1.9706, 2.4204, 2.4976, 3.1486, 2.0788, 2.7363,\n",
      "        2.4180, 2.0303, 2.9344, 2.0718, 2.6562, 3.4177, 3.0537, 2.7972, 2.2466,\n",
      "        3.6124, 3.4383, 1.8691, 2.8082, 2.1992, 2.7259, 1.9714, 2.4989, 2.9801,\n",
      "        3.0282, 2.6700, 2.6769, 2.3750, 2.2532, 3.5080, 2.4676, 2.6120, 2.4669,\n",
      "        2.5763, 2.6182, 3.6755, 2.8927, 2.4527, 1.9694, 3.1698, 2.4372],\n",
      "       dtype=torch.float64, grad_fn=<AddBackward0>)], [tensor([-35.7740, -45.8241, -38.7560, -31.5890, -35.1164, -38.4393, -42.1957,\n",
      "        -46.6409, -48.8005, -44.3439], dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>), tensor([43.3247, 44.5450, 41.7424, 34.7020, 40.5605, 35.9051, 43.5192, 46.3586,\n",
      "        38.2935, 41.4184], dtype=torch.float64, grad_fn=<AddBackward0>)]]\n",
      "bounds layer out \n",
      " [[tensor([0.2088, 0.0196, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "       dtype=torch.float64, grad_fn=<MaximumBackward0>), tensor([0.7791, 0.5886, 0.5620,  ..., 0.0000, 0.2118, 0.3357],\n",
      "       dtype=torch.float64, grad_fn=<MaximumBackward0>)], [tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.], dtype=torch.float64,\n",
      "       grad_fn=<MaximumBackward0>), tensor([2.1985, 2.2024, 3.5789, 3.2989, 3.1727, 2.7257, 3.1589, 2.9706, 2.3492,\n",
      "        2.3374, 2.2188, 3.5131, 2.9834, 3.2893, 2.7698, 2.7456, 3.0551, 2.7703,\n",
      "        2.4261, 2.7636, 2.3316, 2.5707, 3.1093, 2.7213, 3.2009, 2.8840, 2.6372,\n",
      "        3.2359, 2.7661, 3.7934, 1.9150, 3.2887, 2.6867, 3.5916, 2.4751, 3.0572,\n",
      "        2.5368, 2.8881, 2.4758, 2.3354, 2.7308, 2.2997, 2.9015, 2.8642, 2.6307,\n",
      "        2.6679, 2.1138, 2.7184, 2.5877, 2.4363, 2.8510, 2.2290, 2.7605, 2.9352,\n",
      "        2.5532, 2.4591, 2.0163, 3.2752, 2.3035, 2.7828, 2.5608, 2.5729, 2.1703,\n",
      "        2.6810, 2.4530, 2.5627, 2.7323, 3.0748, 2.3498, 3.0908, 2.9539, 2.4527,\n",
      "        2.4962, 2.8960, 2.6532, 2.6162, 2.2160, 3.6041, 3.0032, 2.8227, 2.6230,\n",
      "        3.2652, 2.3000, 2.5879, 2.6289, 3.2825, 2.4289, 3.2062, 2.0722, 2.9592,\n",
      "        2.0333, 2.4219, 2.9470, 3.5575, 2.5283, 2.6129, 2.5972, 2.9534, 2.9723,\n",
      "        2.5614, 2.8972, 2.7913, 2.9018, 1.8760, 3.3864, 3.4564, 2.7306, 3.0458,\n",
      "        2.2486, 3.0729, 2.5512, 2.5311, 2.8378, 2.2285, 3.0569, 2.5264, 2.6175,\n",
      "        2.5961, 2.5296, 2.6409, 2.6287, 2.5965, 2.9409, 2.4046, 2.7446, 2.3342,\n",
      "        2.9047, 2.5440, 3.0772, 1.9716, 2.1576, 2.7586, 2.4789, 2.6533, 2.2112,\n",
      "        1.8408, 2.9815, 2.7088, 2.7785, 2.7992, 3.3071, 3.1239, 2.4893, 3.1774,\n",
      "        2.4032, 2.8245, 2.6077, 3.3692, 2.4308, 2.2467, 1.9323, 2.7972, 2.7219,\n",
      "        3.2780, 2.9485, 1.9737, 2.9057, 3.7385, 3.2042, 2.6872, 2.8178, 2.6760,\n",
      "        2.4202, 2.7386, 2.2116, 2.8576, 3.0162, 2.2776, 3.1711, 2.4163, 3.9645,\n",
      "        2.4481, 2.7093, 2.1543, 2.6305, 2.5179, 2.1667, 2.6047, 2.6598, 2.0520,\n",
      "        3.2499, 2.4984, 3.2387, 1.9903, 2.1779, 2.6996, 3.1079, 3.0683, 2.8121,\n",
      "        2.5086, 2.6226, 2.2937, 2.8322, 2.4843, 2.4434, 2.6396, 2.6338, 2.3789,\n",
      "        2.7651, 2.4555, 2.8191, 2.7082, 1.9177, 1.8043, 2.0851, 2.5194, 2.5843,\n",
      "        2.2937, 2.0311, 2.0399, 2.4141, 2.8709, 2.6827, 3.2072, 2.4175, 2.9141,\n",
      "        2.7656, 2.7763, 2.8114, 2.5082, 1.9327, 2.6724, 2.2971, 2.3961, 2.4524,\n",
      "        2.7674, 2.5349, 3.2752, 2.6632, 2.7877, 2.4443, 2.6509, 2.4466, 2.1202,\n",
      "        2.9781, 2.3353, 2.5709, 2.1044, 2.5156, 2.9475, 3.4756, 2.5669, 2.3189,\n",
      "        2.3897, 2.2457, 3.0801, 3.5114, 2.4983, 2.5862, 2.5761, 3.1905, 2.3847,\n",
      "        2.0487, 2.6576, 2.6359, 3.1154, 2.4759, 2.8485, 3.0165, 2.8262, 2.6906,\n",
      "        2.8355, 2.7099, 2.5992, 1.6763, 2.7652, 2.6295, 2.3402, 2.8419, 2.9079,\n",
      "        2.4964, 2.8957, 2.8146, 2.8652, 2.3430, 2.9373, 1.6113, 2.4700, 1.9122,\n",
      "        2.3025, 2.5531, 2.2747, 2.6465, 2.9438, 2.1805, 2.2151, 3.0780, 2.2498,\n",
      "        2.4682, 2.1720, 1.9502, 2.3754, 3.0590, 2.9243, 2.5663, 2.1819, 1.9129,\n",
      "        2.4948, 2.9328, 2.1700, 2.6950, 2.9161, 3.1480, 2.9733, 2.6979, 1.7689,\n",
      "        2.1127, 2.9371, 2.5969, 3.9292, 3.1885, 2.4670, 2.7131, 2.7215, 2.7180,\n",
      "        2.5085, 3.2214, 3.0689, 3.8328, 2.5023, 2.7257, 2.1245, 2.3074, 2.0151,\n",
      "        2.5359, 2.9857, 2.3858, 2.7070, 2.9559, 2.5780, 2.9495, 2.1060, 2.3754,\n",
      "        2.7490, 2.9435, 2.4471, 2.6938, 2.2222, 2.7023, 2.6875, 2.6380, 2.3804,\n",
      "        2.8629, 2.6581, 2.5343, 2.1080, 3.1665, 2.9870, 2.4671, 3.0791, 2.9258,\n",
      "        2.0098, 2.9876, 2.4583, 3.1908, 1.8993, 2.4326, 3.2955, 2.3088, 2.7669,\n",
      "        2.3705, 2.6104, 2.2965, 2.8599, 3.0404, 3.1265, 2.3486, 2.6209, 2.5314,\n",
      "        2.9598, 2.1777, 3.2833, 2.4826, 2.1495, 2.1120, 2.6329, 2.0196, 2.5830,\n",
      "        2.9953, 2.7731, 2.1109, 2.6253, 2.6458, 3.1792, 2.3662, 2.9563, 2.5534,\n",
      "        3.6706, 2.4945, 3.0791, 2.2953, 2.7153, 3.2815, 2.2885, 2.4486, 2.0401,\n",
      "        3.1391, 2.0505, 3.0785, 2.2818, 2.9539, 2.3042, 2.9556, 2.9958, 1.6933,\n",
      "        2.8548, 2.2854, 2.5816, 3.0564, 2.0341, 2.3031, 2.5163, 2.3943, 2.1332,\n",
      "        2.5911, 2.6126, 2.3571, 2.4486, 1.9534, 3.2738, 2.5618, 2.5262, 2.7043,\n",
      "        2.9916, 3.1165, 2.2164, 3.1818, 2.5545, 2.2131, 3.1386, 3.0965, 2.8530,\n",
      "        1.9496, 2.5144, 2.0527, 2.6682, 2.6472, 2.7935, 2.5063, 3.1373, 3.1121,\n",
      "        2.6312, 2.4272, 2.1223, 2.9568, 2.5563, 2.5324, 3.5180, 2.0442, 2.8928,\n",
      "        3.0702, 2.2026, 2.4208, 3.1787, 2.5664, 2.6002, 2.0378, 2.5944, 2.5661,\n",
      "        2.8763, 2.4492, 2.1314, 3.1454, 2.7629, 3.0102, 3.0103, 2.5855, 2.3826,\n",
      "        2.2292, 2.7869, 2.7408, 1.9706, 2.4204, 2.4976, 3.1486, 2.0788, 2.7363,\n",
      "        2.4180, 2.0303, 2.9344, 2.0718, 2.6562, 3.4177, 3.0537, 2.7972, 2.2466,\n",
      "        3.6124, 3.4383, 1.8691, 2.8082, 2.1992, 2.7259, 1.9714, 2.4989, 2.9801,\n",
      "        3.0282, 2.6700, 2.6769, 2.3750, 2.2532, 3.5080, 2.4676, 2.6120, 2.4669,\n",
      "        2.5763, 2.6182, 3.6755, 2.8927, 2.4527, 1.9694, 3.1698, 2.4372],\n",
      "       dtype=torch.float64, grad_fn=<MaximumBackward0>)], [tensor([-35.7740, -45.8241, -38.7560, -31.5890, -35.1164, -38.4393, -42.1957,\n",
      "        -46.6409, -48.8005, -44.3439], dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>), tensor([43.3247, 44.5450, 41.7424, 34.7020, 40.5605, 35.9051, 43.5192, 46.3586,\n",
      "        38.2935, 41.4184], dtype=torch.float64, grad_fn=<AddBackward0>)]]\n"
     ]
    }
   ],
   "source": [
    "input_flattened = torch.flatten(test_image)\n",
    "bounds_affine_out, bounds_layer_out = nn.calculate_box_bounds([input_flattened.add(-eps), input_flattened.add(eps)])\n",
    "print(\"bounds affine out \\n {}\".format(bounds_affine_out))\n",
    "print(\"bounds layer out \\n {}\".format(bounds_layer_out))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#milp_verifier = MILPVerifier(nn, test_image, eps, print_log=False)\n",
    "snv_verifier = SingleNeuronVerifier(nn, test_image, eps, print_log=False)\n",
    "#dhov_verifier = DHOVVerifier(icnns, group_size, nn, test_image, 1)\n",
    "\n",
    "#milp_verifier.generate_constraints_for_net()\n",
    "snv_verifier.generate_constraints_for_net()\n",
    "#dhov_verifier.generate_constraints_for_net()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "layer_index = 1\n",
    "neuron_index = 0\n",
    "#milp_model = milp_verifier.model\n",
    "snv_model = snv_verifier.model\n",
    "#dhov_model = dhov_verifier.model\n",
    "dhov_model = icnn_model(icnns[layer_index][0], nn, test_image, eps, layer_index, 0, 5, print_log=False)\n",
    "#milp_model.update()\n",
    "snv_model.update()\n",
    "dhov_model.update()\n",
    "\"\"\"all_var = milp_model.getVars()\n",
    "for var in all_var:\n",
    "    print(var)\"\"\"\n",
    "neuron_name = \"relu_var{}[{}]\".format(2*layer_index, neuron_index)\n",
    "icnn_neuron_name = \"icnn_var[{}]\".format(neuron_index)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================\n",
      "opt value: 0.0\n",
      "===================================================================================\n",
      "opt value: 0.0\n"
     ]
    }
   ],
   "source": [
    "#milp_copy = milp_model.copy()\n",
    "snv_copy = snv_model.copy()\n",
    "dhov_copy = dhov_model.copy()\n",
    "\n",
    "#milp_copy.Params.LogToConsole = 0\n",
    "snv_copy.Params.LogToConsole = 0\n",
    "dhov_copy.Params.LogToConsole = 0\n",
    "\n",
    "#add_min_constr(milp_copy, neuron_name) #affine_var0[0]\n",
    "add_min_constr(snv_copy, neuron_name)\n",
    "add_min_constr(dhov_copy, icnn_neuron_name)\n",
    "\n",
    "#optimize_model(milp_copy, neuron_name)\n",
    "print(\"===================================================================================\")\n",
    "optimize_model(snv_copy, neuron_name)\n",
    "print(\"===================================================================================\")\n",
    "optimize_model(dhov_copy, icnn_neuron_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================\n",
      "opt value: 1.279494932358921\n",
      "===================================================================================\n",
      "opt value: 2.198499612885324\n",
      "===================================================================================\n"
     ]
    }
   ],
   "source": [
    "#milp_copy = milp_model.copy()\n",
    "snv_copy = snv_model.copy()\n",
    "dhov_copy = dhov_model.copy()\n",
    "\n",
    "#milp_copy.Params.LogToConsole = 0\n",
    "snv_copy.Params.LogToConsole = 0\n",
    "dhov_copy.Params.LogToConsole = 0\n",
    "\n",
    "#add_max_constr(milp_copy, neuron_name)\n",
    "add_max_constr(snv_copy, neuron_name)\n",
    "add_max_constr(dhov_copy, icnn_neuron_name)\n",
    "\n",
    "#optimize_model(milp_copy, neuron_name)\n",
    "print(\"===================================================================================\")\n",
    "optimize_model(snv_copy, neuron_name)\n",
    "print(\"===================================================================================\")\n",
    "optimize_model(dhov_copy, icnn_neuron_name)\n",
    "print(\"===================================================================================\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}