{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:53:53.399243330Z",
     "start_time": "2024-05-02T20:53:52.545426952Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from script.NeuralNets.Networks import SequentialNN\n",
    "from script.settings import device, data_type\n",
    "import script.DHOV.MultiDHOV as multidhov\n",
    "from script.Verification.Verifier import SingleNeuronVerifier, MILPVerifier\n",
    "import gurobipy as grp\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from script.NeuralNets.ICNNFactory import ICNNFactory\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx2pytorch import ConvertModel\n",
    "from auto_LiRPA import BoundedModule, BoundedTensor, PerturbationLpNorm\n",
    "\n",
    "from vnnlib.compat import read_vnnlib_simple\n",
    "from collections import OrderedDict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:53:53.612646766Z",
     "start_time": "2024-05-02T20:53:53.399153350Z"
    }
   },
   "id": "5cfde707d03d80c2",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ufuk/miniconda3/envs/autolirpa_icnn/lib/python3.10/site-packages/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onnx_model = onnx.load(\"mnist-net_256x4.onnx\")\n",
    "pytorch_model = ConvertModel(onnx_model)\n",
    "\n",
    "nn = SequentialNN([28 * 28 * 1, 256, 256, 256, 256, 10])\n",
    "state_dict = pytorch_model.state_dict()\n",
    "my_state_dict = {}\n",
    "for i, key in enumerate(state_dict):\n",
    "    actual_index = i * 2\n",
    "    my_state_dict[f\"{actual_index}.bias\"] = state_dict[f\"_initializer_layers_{actual_index}_bias\"]\n",
    "    my_state_dict[f\"{actual_index}.weight\"] = state_dict[f\"_initializer_layers_{actual_index}_weight\"]\n",
    "    if actual_index == 8:\n",
    "        break\n",
    "    \n",
    "nn.load_state_dict(my_state_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:53:53.985356875Z",
     "start_time": "2024-05-02T20:53:53.614408522Z"
    }
   },
   "id": "4d34658b0fd128c9",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "parameter_list = list(nn.parameters())\n",
    "output_size = 10\n",
    "number_layer = (len(parameter_list) - 2) // 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:53:53.986988942Z",
     "start_time": "2024-05-02T20:53:53.985320675Z"
    }
   },
   "id": "836e67ad709af8dc",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + .05  # revert normalization for viewing\n",
    "    npimg = img.to(\"cpu\").numpy()\n",
    "    plt.imshow(npimg, cmap=\"gray\")\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:53:54.031229072Z",
     "start_time": "2024-05-02T20:53:53.986964302Z"
    }
   },
   "id": "3c77c4167a8ba0f4",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "transform = Compose([ToTensor()])\n",
    "training_data = MNIST(root=\"../../mnist\", train=True, download=True, transform=transform)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:53:54.031785601Z",
     "start_time": "2024-05-02T20:53:54.031168402Z"
    }
   },
   "id": "ddba502d998d0fed",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The label is 3\n",
      "NN classifies image correctly as 3\n"
     ]
    }
   ],
   "source": [
    "image_index = 10\n",
    "\n",
    "# whether a verification attempt should be done using just MILP encoding for comparison (can take long)\n",
    "use_milp = False\n",
    "\n",
    "image, label = training_data[image_index]\n",
    "torch_image = torch.unsqueeze(image, 0).to(dtype=data_type).to(device)\n",
    "imshow(torch_image[0][0])\n",
    "print(f\"The label is {label}\")\n",
    "\n",
    "if torch.argmax(nn(torch_image)).item() == label:\n",
    "    print(\"NN classifies image correctly as {}\".format(label))\n",
    "else:\n",
    "    print(\"NN classifies image wrong\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:53:54.388180294Z",
     "start_time": "2024-05-02T20:53:54.031243102Z"
    }
   },
   "id": "fc556c877bfb08b6",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def add_output_constraints(model, nn_layer_out_bounds, label, output_vars, sovler_bound=1e-3):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param model: the optimization problem in gurobi encoding the NN\n",
    "    :param nn_layer_out_bounds: torch.Tensor, approximating the upper and lower bounding the output layer of the NN\n",
    "    :param label: index of the label or target neuron which is compared against\n",
    "    :param output_vars: the gurobi variables from the model of the NN describing the output neurons of the NN\n",
    "    :param sovler_bound: provides a bound for the gurobi solver. If this bound is achieved, the optimizer stops\n",
    "    \"\"\"\n",
    "    \n",
    "    out_lb = nn_layer_out_bounds[-1][0].detach().cpu().numpy()\n",
    "    out_ub = nn_layer_out_bounds[-1][1].detach().cpu().numpy()\n",
    "    \n",
    "    difference_lb = out_lb - out_ub[label]\n",
    "    difference_ub = out_ub - out_lb[label]\n",
    "    difference_lb = difference_lb.tolist()\n",
    "    difference_ub = difference_ub.tolist()\n",
    "    \n",
    "    difference_lb.pop(label)\n",
    "    difference_ub.pop(label)\n",
    "    \n",
    "    min_diff = min(difference_lb)\n",
    "    max_diff = max(difference_ub)\n",
    "    \n",
    "    difference = model.addVars(9, lb=difference_lb, ub=difference_ub, name=\"diff_var\")\n",
    "    model.addConstrs((difference[i] == output_vars.tolist()[i] - output_vars.tolist()[label] for i in range(0, label)), name=\"diff_const0\")\n",
    "    model.addConstrs((difference[i - 1] == output_vars.tolist()[i] - output_vars.tolist()[label] for i in range(label + 1, 10)), name=\"diff_const1\")\n",
    "\n",
    "    max_var = model.addVar(lb=min_diff, ub=max_diff, name=\"max_var\")\n",
    "    model.addConstr(max_var == grp.max_(difference))\n",
    "\n",
    "    if sovler_bound != None:\n",
    "        model.setParam(\"BestObjStop\", sovler_bound)\n",
    "\n",
    "    model.update()\n",
    "    model.setObjective(max_var, grp.GRB.MAXIMIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:53:54.435278578Z",
     "start_time": "2024-05-02T20:53:54.390033250Z"
    }
   },
   "id": "10fb00c83254fed8",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ufuk/miniconda3/envs/autolirpa_icnn/lib/python3.10/site-packages/vnnlib/compat.py:283: UserWarning: literal negation does not strictly follow SMT-LIB\n",
      "  ast_node = parse_file(vnnlib_filename, strict=False)\n"
     ]
    }
   ],
   "source": [
    "def load_vnnlib_bounds(vnnlib_path, input_shape, n_out):\n",
    "    n_in = np.prod(input_shape)\n",
    "    res = read_vnnlib_simple(vnnlib_path, n_in, n_out)\n",
    "    bnds, spec = res[0]\n",
    "    \n",
    "    bnds = np.array(bnds)\n",
    "    lbs = bnds[:,0]\n",
    "    ubs = bnds[:,1]\n",
    "    \n",
    "    data_min = torch.tensor(lbs, dtype=data_type).reshape(input_shape).to(device)\n",
    "    data_max = torch.tensor(ubs, dtype=data_type).reshape(input_shape).to(device)\n",
    "\n",
    "    return [data_min, data_max]\n",
    "\n",
    "vnnlib_path = 'prop_0_0.03.vnnlib'\n",
    "input_bounds = load_vnnlib_bounds(vnnlib_path, [784,], 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:53:54.469790467Z",
     "start_time": "2024-05-02T20:53:54.435208328Z"
    }
   },
   "id": "4c34f2e96fe0bcf8",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def load_onnx_model(onnx_path, input_shape):\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    torch_model = ConvertModel(onnx_model)\n",
    "    \n",
    "    x_concrete = torch.zeros(input_shape)\n",
    "    model = BoundedModule(torch_model, x_concrete)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:53:54.515207215Z",
     "start_time": "2024-05-02T20:53:54.470045257Z"
    }
   },
   "id": "e429f2b54d1118d1",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ufuk/miniconda3/envs/autolirpa_icnn/lib/python3.10/site-packages/onnx2pytorch/convert/model.py:163: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not self.experimental and inputs[0].shape[self.batch_dim] > 1:\n",
      "/home/ufuk/miniconda3/envs/autolirpa_icnn/lib/python3.10/site-packages/auto_LiRPA/parse_graph.py:154: FutureWarning: 'torch.onnx.symbolic_helper._set_opset_version' is deprecated in version 1.13 and will be removed in 2.0. Please remove its usage and avoid setting internal variables directly.\n",
      "  _set_opset_version(12)\n"
     ]
    }
   ],
   "source": [
    "def load_vnnlib_spec_for_auto_lirpa(vnnlib_path, input_shape, n_out):\n",
    "    n_in = np.prod(input_shape)\n",
    "    res = read_vnnlib_simple(vnnlib_path, n_in, n_out)\n",
    "    bnds, spec = res[0]\n",
    "\n",
    "    bnds = np.array(bnds)\n",
    "    lbs = bnds[:, 0]\n",
    "    ubs = bnds[:, 1]\n",
    "\n",
    "    data_min = torch.tensor(lbs, dtype=torch.float32).reshape(input_shape)\n",
    "    data_max = torch.tensor(ubs, dtype=torch.float32).reshape(input_shape)\n",
    "    center = 0.5 * (data_min + data_max)\n",
    "\n",
    "    ptb = PerturbationLpNorm(x_L=data_min, x_U=data_max)\n",
    "    x = BoundedTensor(center, ptb)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "onnx_path = 'mnist-net_256x4.onnx'\n",
    "vnnlib_path = 'prop_0_0.03.vnnlib'\n",
    "model = load_onnx_model(onnx_path, [1, 1, 1, 784])\n",
    "x = load_vnnlib_spec_for_auto_lirpa(vnnlib_path, [1, 1, 1, 784], 10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:53:54.515713944Z",
     "start_time": "2024-05-02T20:53:54.515157405Z"
    }
   },
   "id": "55f2e928fd1ce2a2",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_layers(model):\n",
    "    return [l for l in model.nodes() if l.perturbed]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:53:54.516382863Z",
     "start_time": "2024-05-02T20:53:54.515232365Z"
    }
   },
   "id": "26f0e37c6f1bd6ce",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_intermediate_bounds(model):\n",
    "    \"\"\"\n",
    "    Returns a dictionary containing the concrete lower and upper bounds of each layer.\n",
    "    \n",
    "    Implemented own method to filter out bounds for weight matrices.\n",
    "    \n",
    "    Only call this method after compute_bounds()!\n",
    "    \"\"\"\n",
    "    od = OrderedDict()\n",
    "    for l in get_layers(model):\n",
    "        if hasattr(l, 'lower'):\n",
    "            od[l.name] = (l.lower, l.upper)\n",
    "            \n",
    "    return od"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:53:54.516716462Z",
     "start_time": "2024-05-02T20:53:54.515256355Z"
    }
   },
   "id": "30fc8c66423e25e7",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[-16.1574, -13.2932, -17.4020, -26.5492, -17.5962, -16.9601, -15.3861,\n          -17.6454, -20.9628, -15.7071]], grad_fn=<ViewBackward0>),\n tensor([[13.8042, 12.8868, 11.2402,  7.7739, 14.1580, 12.1219, 15.4775, 13.8468,\n          12.8768, 15.7155]], grad_fn=<ViewBackward0>))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compute_bounds(x=(x,), method='ibp')\n",
    "model.compute_bounds(x=(x,), method='crown')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:53:54.548638757Z",
     "start_time": "2024-05-02T20:53:54.515274545Z"
    }
   },
   "id": "5fad247c25e5d708",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bounds_dict_crown = get_intermediate_bounds(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:53:54.591189170Z",
     "start_time": "2024-05-02T20:53:54.543484787Z"
    }
   },
   "id": "85879641b67baf63",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/0\n",
      "/input\n",
      "/input.3\n",
      "/input.7\n",
      "/input.11\n",
      "/30\n"
     ]
    }
   ],
   "source": [
    "bounds_dict_crown.keys()\n",
    "crown_bounds_affine_out = []\n",
    "for i, key in enumerate(bounds_dict_crown.keys()):\n",
    "    print(key)\n",
    "    if i == 0: # use this if ibp is used (or i % 2 == 1:)\n",
    "        continue\n",
    "    lb, ub = bounds_dict_crown[key]\n",
    "    crown_bounds_affine_out.append([lb.type(data_type).view(-1).to(device), ub.type(data_type).view(-1).to(device)])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:53:54.591655569Z",
     "start_time": "2024-05-02T20:53:54.591154280Z"
    }
   },
   "id": "fc2b89dc6a3533d6",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "crown_bounds_layer_out = []\n",
    "relu = torch.nn.ReLU()\n",
    "for i, (lb, ub) in enumerate(crown_bounds_affine_out):\n",
    "    if i == len(crown_bounds_affine_out) - 1:\n",
    "        crown_bounds_layer_out.append([lb, ub])\n",
    "    else:\n",
    "        lb_layer = relu(lb)\n",
    "        ub_layer = relu(ub)\n",
    "        crown_bounds_layer_out.append([lb_layer, ub_layer])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:53:54.592302308Z",
     "start_time": "2024-05-02T20:53:54.591203580Z"
    }
   },
   "id": "779526016c313817",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "net_size = [100, 100, 10, 1]\n",
    "current_layer_index = 3 # use either 3 and True or use 4 and False\n",
    "encode_last_affine_layer = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:53:54.592625627Z",
     "start_time": "2024-05-02T20:53:54.591222360Z"
    }
   },
   "id": "fdfe6c0b4f0a15cc",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_count = 10000\n",
    "icnn_epochs = 100\n",
    "force_inclusion_steps = 5\n",
    "icnn_batch_size = 100000\n",
    "\n",
    "preemptive_stop = True\n",
    "init_network = True\n",
    "hyper_lambda=1\n",
    "adapt_lambda = \"included\"\n",
    "print_training_loss = False\n",
    "print_last_loss = False\n",
    "optimizer = \"SdLBFGS\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:53:57.103472018Z",
     "start_time": "2024-05-02T20:53:57.101620942Z"
    }
   },
   "id": "61476a10a81e9a38",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter Username\n",
      "Academic license - for non-commercial use only - expires 2025-02-14\n",
      "Loss is -4236.79296875\n",
      "Loss is -5294.00146484375\n",
      "Loss is -5351.1416015625\n",
      "Loss is -5408.208984375\n",
      "Loss is -5465.193359375\n",
      "Loss is -5522.0791015625\n",
      "Loss is -5578.84912109375\n",
      "Loss is -5635.4794921875\n",
      "Loss is -5691.94921875\n",
      "Loss is -5748.24853515625\n",
      "Loss is -5804.3671875\n",
      "Loss is -5860.30224609375\n",
      "Loss is -5916.0634765625\n",
      "Loss is -5971.6533203125\n",
      "Loss is -6027.083984375\n",
      "Loss is -6082.376953125\n",
      "Loss is -6137.548828125\n",
      "Loss is -6192.630859375\n",
      "Loss is -6247.66259765625\n",
      "Loss is -6302.67822265625\n",
      "Loss is -6357.705078125\n",
      "Loss is -6412.75390625\n",
      "Loss is -6467.8271484375\n",
      "Loss is -6522.92724609375\n",
      "Loss is -6578.048828125\n",
      "Loss is -6633.193359375\n",
      "Loss is -6688.3603515625\n",
      "Loss is -6743.5537109375\n",
      "Loss is -6798.7802734375\n",
      "Loss is -6854.0419921875\n",
      "Loss is -6909.3427734375\n",
      "Loss is -6964.6845703125\n",
      "Loss is -7020.0625\n",
      "Loss is -7075.4716796875\n",
      "Loss is -7130.90234375\n",
      "Loss is -7186.3447265625\n",
      "Loss is -7241.791015625\n",
      "Loss is -7297.2333984375\n",
      "Loss is -7352.6640625\n",
      "Loss is -7408.0703125\n",
      "Loss is -7463.4453125\n",
      "Loss is -7518.7763671875\n",
      "Loss is -7574.05615234375\n",
      "Loss is -7629.275390625\n",
      "Loss is -7684.43115234375\n",
      "Loss is -7739.5146484375\n",
      "Loss is -7794.5224609375\n",
      "Loss is -7849.4453125\n",
      "Loss is -7904.2763671875\n",
      "Loss is -7959.01953125\n",
      "Loss is -8013.677734375\n",
      "Loss is -8068.2548828125\n",
      "Loss is -8122.76171875\n",
      "Loss is -8177.20703125\n",
      "Loss is -8231.6162109375\n",
      "Loss is -8285.9951171875\n",
      "Loss is -8340.35546875\n",
      "Loss is -8394.708984375\n",
      "Loss is -8449.0693359375\n",
      "Loss is -8503.443359375\n",
      "Loss is -8557.7880859375\n",
      "Loss is -8612.044921875\n",
      "Loss is -8666.169921875\n",
      "Loss is -8720.12890625\n",
      "Loss is -8773.9453125\n",
      "Loss is -8827.685546875\n",
      "Loss is -8881.47265625\n",
      "Loss is -8935.4873046875\n",
      "Loss is -8989.9189453125\n",
      "Loss is -9044.9150390625\n",
      "Loss is -9100.6083984375\n",
      "Loss is -9157.08984375\n",
      "Loss is -9214.4375\n",
      "Loss is -9272.6884765625\n",
      "Loss is -9331.80078125\n",
      "Loss is -9391.701171875\n",
      "Loss is -9452.2666015625\n",
      "Loss is -9513.3603515625\n",
      "Loss is -9574.7734375\n",
      "Loss is -9636.33984375\n",
      "Loss is -9697.9052734375\n",
      "Loss is -9759.3544921875\n",
      "Loss is -9820.6201171875\n",
      "Loss is -9881.611328125\n",
      "Loss is -9942.33203125\n",
      "Loss is -10002.80859375\n",
      "Loss is -10063.0908203125\n",
      "Loss is -10123.197265625\n",
      "Loss is -10183.12109375\n",
      "Loss is -10242.912109375\n",
      "Loss is -10302.59765625\n",
      "Loss is -10362.1865234375\n",
      "Loss is -10421.693359375\n",
      "Loss is -10481.13671875\n",
      "Loss is -10540.5205078125\n",
      "Loss is -10599.85546875\n",
      "Loss is -10659.13671875\n",
      "Loss is -10718.365234375\n",
      "Loss is -10777.5322265625\n",
      "Loss is -10836.626953125\n",
      "Loss is -10895.6552734375\n",
      "Loss is -10954.619140625\n",
      "Loss is -11013.515625\n",
      "Loss is -11072.345703125\n",
      "Loss is -11131.1015625\n",
      "Loss is -11189.783203125\n",
      "Loss is -11248.388671875\n",
      "Loss is -11306.919921875\n",
      "Loss is -11365.373046875\n",
      "Loss is -11423.7392578125\n",
      "Loss is -11482.025390625\n",
      "Loss is -11540.228515625\n",
      "Loss is -11598.345703125\n",
      "Loss is -11656.37109375\n",
      "Loss is -11714.291015625\n",
      "Loss is -11772.1201171875\n",
      "Loss is -11829.8671875\n",
      "Loss is -11887.521484375\n",
      "Loss is -11945.0869140625\n",
      "Loss is -12002.55859375\n",
      "Loss is -12059.939453125\n",
      "Loss is -12117.234375\n",
      "Loss is -12174.447265625\n",
      "Loss is -12231.59375\n",
      "Loss is -12288.681640625\n",
      "Loss is -12345.7099609375\n",
      "Loss is -12402.712890625\n",
      "Loss is -12459.689453125\n",
      "Loss is -12516.6484375\n",
      "Loss is -12573.591796875\n",
      "Loss is -12630.51953125\n",
      "Loss is -12687.4443359375\n",
      "Loss is -12744.369140625\n",
      "Loss is -12801.2861328125\n",
      "Loss is -12858.19921875\n",
      "Loss is -12915.1171875\n",
      "Loss is -12972.041015625\n",
      "Loss is -13028.970703125\n",
      "Loss is -13085.91015625\n",
      "Loss is -13142.8681640625\n",
      "Loss is -13199.8525390625\n",
      "Loss is -13256.876953125\n",
      "Loss is -13313.9404296875\n",
      "Loss is -13371.0615234375\n",
      "Loss is -13428.267578125\n",
      "Loss is -13485.578125\n",
      "Loss is -13543.01171875\n",
      "Loss is -13600.6083984375\n",
      "Loss is -13658.3798828125\n",
      "Loss is -13716.3525390625\n",
      "Loss is -13774.5576171875\n",
      "Loss is -13833.0107421875\n",
      "Loss is -13891.69921875\n",
      "Loss is -13950.623046875\n",
      "Loss is -14009.7646484375\n",
      "Loss is -14069.10546875\n",
      "Loss is -14128.62109375\n",
      "Loss is -14188.2900390625\n",
      "Loss is -14248.0888671875\n",
      "Loss is -14308.0029296875\n",
      "Loss is -14368.0\n",
      "Loss is -14428.072265625\n",
      "Loss is -14488.208984375\n",
      "Loss is -14548.39453125\n",
      "Loss is -14608.6259765625\n",
      "Loss is -14668.888671875\n",
      "Loss is -14729.16796875\n",
      "Loss is -14789.46875\n",
      "Loss is -14849.7734375\n",
      "Loss is -14910.0751953125\n",
      "Loss is -14970.3828125\n",
      "Loss is -15030.68359375\n",
      "Loss is -15090.9755859375\n",
      "Loss is -15151.259765625\n",
      "Loss is -15211.5244140625\n",
      "Loss is -15271.763671875\n",
      "Loss is -15331.9873046875\n",
      "Loss is -15392.1875\n",
      "Loss is -15452.353515625\n",
      "Loss is -15512.49609375\n",
      "Loss is -15572.6171875\n",
      "Loss is -15632.703125\n",
      "Loss is -15692.7548828125\n",
      "Loss is -15752.779296875\n",
      "Loss is -15812.767578125\n",
      "Loss is -15872.71875\n",
      "Loss is -15932.626953125\n",
      "Loss is -15992.494140625\n",
      "Loss is -16052.3251953125\n",
      "Loss is -16112.107421875\n",
      "Loss is -16171.837890625\n",
      "Loss is -16231.51171875\n",
      "Loss is -16291.134765625\n",
      "Loss is -16350.701171875\n",
      "Loss is -16410.216796875\n",
      "Loss is -16469.671875\n",
      "Loss is -16529.068359375\n",
      "Loss is -16588.4140625\n",
      "Loss is -16647.703125\n",
      "Loss is -16706.9609375\n",
      "Loss is -16766.19140625\n",
      "Loss is -16825.41015625\n",
      "Loss is -16884.654296875\n",
      "Loss is -16943.9609375\n",
      "Loss is -17003.3515625\n",
      "Loss is -17062.888671875\n",
      "Loss is -17122.609375\n",
      "Loss is -17182.552734375\n",
      "Loss is -17242.763671875\n",
      "Loss is -17303.28515625\n",
      "Loss is -17364.130859375\n",
      "Loss is -17425.302734375\n",
      "Loss is -17486.80859375\n",
      "Loss is -17548.638671875\n",
      "Loss is -17610.775390625\n",
      "Loss is -17673.1875\n",
      "Loss is -17735.8515625\n",
      "Loss is -17798.765625\n",
      "Loss is -17861.90625\n",
      "Loss is -17925.283203125\n",
      "Loss is -17988.875\n",
      "Loss is -18052.68359375\n",
      "Loss is -18116.71484375\n",
      "Loss is -18180.96484375\n",
      "Loss is -18245.4453125\n",
      "Loss is -18310.15625\n",
      "Loss is -18375.107421875\n",
      "Loss is -18440.294921875\n",
      "Loss is -18505.71875\n",
      "Loss is -18571.3828125\n",
      "Loss is -18637.2734375\n",
      "Loss is -18703.375\n",
      "Loss is -18769.67578125\n",
      "Loss is -18836.16796875\n",
      "Loss is -18902.826171875\n",
      "Loss is -18969.65234375\n",
      "Loss is -19036.61328125\n",
      "Loss is -19103.68359375\n",
      "Loss is -19170.857421875\n",
      "Loss is -19238.125\n",
      "Loss is -19305.46875\n",
      "Loss is -19372.87109375\n",
      "Loss is -19440.333984375\n",
      "Loss is -19507.85546875\n",
      "Loss is -19575.41796875\n",
      "Loss is -19643.03515625\n",
      "Loss is -19710.703125\n",
      "Loss is -19778.435546875\n",
      "Loss is -19846.23046875\n",
      "Loss is -19914.09765625\n",
      "Loss is -19982.04296875\n",
      "Loss is -20050.08203125\n",
      "Loss is -20118.220703125\n",
      "Loss is -20186.46875\n",
      "Loss is -20254.841796875\n",
      "Loss is -20323.359375\n",
      "Loss is -20392.015625\n",
      "Loss is -20460.80859375\n",
      "Loss is -20529.7421875\n",
      "Loss is -20598.80859375\n",
      "Loss is -20668.01171875\n",
      "Loss is -20737.34375\n",
      "Loss is -20806.787109375\n",
      "Loss is -20876.34375\n",
      "Loss is -20945.998046875\n",
      "Loss is -21015.7421875\n",
      "Loss is -21085.5546875\n",
      "Loss is -21155.4296875\n",
      "Loss is -21225.3671875\n",
      "Loss is -21295.35546875\n",
      "Loss is -21365.3984375\n",
      "Loss is -21435.4921875\n",
      "Loss is -21505.63671875\n",
      "Loss is -21575.8359375\n",
      "Loss is -21646.083984375\n",
      "Loss is -21716.380859375\n",
      "Loss is -21786.728515625\n",
      "Loss is -21857.125\n",
      "Loss is -21927.5546875\n",
      "Loss is -21998.0078125\n",
      "Loss is -22068.490234375\n",
      "Loss is -22138.978515625\n",
      "Loss is -22209.46484375\n",
      "Loss is -22279.94921875\n",
      "Loss is -22350.4296875\n",
      "Loss is -22420.91015625\n",
      "Loss is -22491.390625\n",
      "Loss is -22561.88671875\n",
      "Loss is -22632.404296875\n",
      "Loss is -22702.953125\n",
      "Loss is -22773.546875\n",
      "Loss is -22844.2109375\n",
      "Loss is -22914.95703125\n",
      "Loss is -22985.80078125\n",
      "Loss is -23056.7578125\n",
      "Loss is -23127.84375\n",
      "Loss is -23199.087890625\n",
      "Loss is -23270.48828125\n",
      "Loss is -23342.04296875\n",
      "Loss is -23413.783203125\n",
      "Loss is -23485.7109375\n",
      "Loss is -23557.8203125\n",
      "Loss is -23630.109375\n",
      "Loss is -23702.5859375\n",
      "Loss is -23775.232421875\n",
      "Loss is -23848.025390625\n",
      "Loss is -23920.951171875\n",
      "Loss is -23994.00390625\n",
      "Loss is -24067.1640625\n",
      "Loss is -24140.412109375\n",
      "Loss is -24213.73046875\n",
      "Loss is -24287.109375\n",
      "Loss is -24360.54296875\n",
      "Loss is -24434.013671875\n",
      "Loss is -24507.5078125\n",
      "Loss is -24581.01953125\n",
      "Loss is -24654.529296875\n",
      "Loss is -24728.037109375\n",
      "Loss is -24801.53125\n",
      "Loss is -24875.00390625\n",
      "Loss is -24948.44921875\n",
      "Loss is -25021.8515625\n",
      "Loss is -25095.212890625\n",
      "Loss is -25168.529296875\n",
      "Loss is -25241.79296875\n",
      "Loss is -25315.00390625\n",
      "Loss is -25388.1640625\n",
      "Loss is -25461.26171875\n",
      "Loss is -25534.298828125\n",
      "Loss is -25607.263671875\n",
      "Loss is -25680.158203125\n",
      "Loss is -25752.974609375\n",
      "Loss is -25825.71875\n",
      "Loss is -25898.388671875\n",
      "Loss is -25970.978515625\n",
      "Loss is -26043.4921875\n",
      "Loss is -26115.9296875\n",
      "Loss is -26188.29296875\n",
      "Loss is -26260.580078125\n",
      "Loss is -26332.80859375\n",
      "Loss is -26404.9609375\n",
      "Loss is -26477.048828125\n",
      "Loss is -26549.0859375\n",
      "Loss is -26621.05859375\n",
      "Loss is -26692.96484375\n",
      "Loss is -26764.802734375\n",
      "Loss is -26836.560546875\n",
      "Loss is -26908.228515625\n",
      "Loss is -26979.79296875\n",
      "Loss is -27051.25\n",
      "Loss is -27122.5703125\n",
      "Loss is -27193.740234375\n",
      "Loss is -27264.7421875\n",
      "Loss is -27335.57421875\n",
      "Loss is -27406.228515625\n",
      "Loss is -27476.7109375\n",
      "Loss is -27547.03125\n",
      "Loss is -27617.1953125\n",
      "Loss is -27687.234375\n",
      "Loss is -27757.171875\n",
      "Loss is -27827.0390625\n",
      "Loss is -27896.85546875\n",
      "Loss is -27966.640625\n",
      "Loss is -28036.412109375\n",
      "Loss is -28106.16796875\n",
      "Loss is -28175.921875\n",
      "Loss is -28245.66015625\n",
      "Loss is -28315.388671875\n",
      "Loss is -28385.1015625\n",
      "Loss is -28454.78125\n",
      "Loss is -28524.421875\n",
      "Loss is -28594.01171875\n",
      "Loss is -28663.5546875\n",
      "Loss is -28733.03515625\n",
      "Loss is -28802.453125\n",
      "Loss is -28871.8125\n",
      "Loss is -28941.11328125\n",
      "Loss is -29010.37109375\n",
      "Loss is -29079.576171875\n",
      "Loss is -29148.736328125\n",
      "Loss is -29217.84375\n",
      "Loss is -29286.91796875\n",
      "Loss is -29355.9609375\n",
      "Loss is -29424.96875\n",
      "Loss is -29493.9453125\n",
      "Loss is -29562.884765625\n",
      "Loss is -29631.791015625\n",
      "Loss is -29700.6796875\n",
      "Loss is -29769.54296875\n",
      "Loss is -29838.380859375\n",
      "Loss is -29907.20703125\n",
      "Loss is -29976.01953125\n",
      "Loss is -30044.8046875\n",
      "Loss is -30113.5703125\n",
      "Loss is -30182.328125\n",
      "Loss is -30251.0859375\n",
      "Loss is -30319.841796875\n",
      "Loss is -30388.59375\n",
      "Loss is -30457.3515625\n",
      "Loss is -30526.109375\n",
      "Loss is -30594.87890625\n",
      "Loss is -30663.6640625\n",
      "Loss is -30732.474609375\n",
      "Loss is -30801.296875\n",
      "Loss is -30870.134765625\n",
      "Loss is -30939.0\n",
      "Loss is -31007.8828125\n",
      "Loss is -31076.7734375\n",
      "Loss is -31145.6875\n",
      "Loss is -31214.6171875\n",
      "Loss is -31283.546875\n",
      "Loss is -31352.48828125\n",
      "Loss is -31421.42578125\n",
      "Loss is -31490.373046875\n",
      "Loss is -31559.3125\n",
      "Loss is -31628.24609375\n",
      "Loss is -31697.1796875\n",
      "Loss is -31766.119140625\n",
      "Loss is -31835.0625\n",
      "Loss is -31904.02734375\n",
      "Loss is -31973.0078125\n",
      "Loss is -32041.990234375\n",
      "Loss is -32111.0\n",
      "Loss is -32180.01953125\n",
      "Loss is -32249.0546875\n",
      "Loss is -32318.091796875\n",
      "Loss is -32387.134765625\n",
      "Loss is -32456.17578125\n",
      "Loss is -32525.20703125\n",
      "Loss is -32594.232421875\n",
      "Loss is -32663.236328125\n",
      "Loss is -32732.22265625\n",
      "Loss is -32801.1796875\n",
      "Loss is -32870.1171875\n",
      "Loss is -32939.0078125\n",
      "Loss is -33007.8671875\n",
      "Loss is -33076.6953125\n",
      "Loss is -33145.4921875\n",
      "Loss is -33214.23828125\n",
      "Loss is -33282.953125\n",
      "Loss is -33351.625\n",
      "Loss is -33420.2578125\n",
      "Loss is -33488.8515625\n",
      "Loss is -33557.39453125\n",
      "Loss is -33625.90625\n",
      "Loss is -33694.3828125\n",
      "Loss is -33762.8125\n",
      "Loss is -33831.203125\n",
      "Loss is -33899.5546875\n",
      "Loss is -33967.8515625\n",
      "Loss is -34036.12109375\n",
      "Loss is -34104.3515625\n",
      "Loss is -34172.546875\n",
      "Loss is -34240.703125\n",
      "Loss is -34308.81640625\n",
      "Loss is -34376.8828125\n",
      "Loss is -34444.91015625\n",
      "Loss is -34512.90625\n",
      "Loss is -34580.859375\n",
      "Loss is -34648.78125\n",
      "Loss is -34716.6640625\n",
      "Loss is -34784.515625\n",
      "Loss is -34852.33203125\n",
      "Loss is -34920.11328125\n",
      "Loss is -34987.859375\n",
      "Loss is -35055.5703125\n",
      "Loss is -35123.24609375\n",
      "Loss is -35190.890625\n",
      "Loss is -35258.50390625\n",
      "Loss is -35326.08203125\n",
      "Loss is -35393.625\n",
      "Loss is -35461.140625\n",
      "Loss is -35528.62890625\n",
      "Loss is -35596.0859375\n",
      "Loss is -35663.5234375\n",
      "Loss is -35730.9296875\n",
      "Loss is -35798.3046875\n",
      "Loss is -35865.6484375\n",
      "Loss is -35932.96484375\n",
      "Loss is -36000.2578125\n",
      "Loss is -36067.53125\n",
      "Loss is -36134.78125\n",
      "Loss is -36202.00390625\n",
      "Loss is -36269.203125\n",
      "Loss is -36336.3828125\n",
      "Loss is -36403.546875\n",
      "Loss is -36470.703125\n",
      "Loss is -36537.84375\n",
      "Loss is -36604.98046875\n",
      "Loss is -36672.1171875\n",
      "Loss is -36739.25\n",
      "Loss is -36806.375\n",
      "Loss is -36873.5\n",
      "Loss is -36940.640625\n",
      "Loss is -37007.78515625\n",
      "Loss is -37074.94140625\n",
      "Loss is -37142.10546875\n",
      "Loss is -37209.27734375\n",
      "Loss is -37276.4453125\n",
      "Loss is -37343.625\n",
      "Loss is -37410.80078125\n",
      "Loss is -37477.9609375\n",
      "Loss is -37545.109375\n",
      "Loss is -37612.25\n",
      "Loss is -37679.37109375\n",
      "Loss is -37746.46875\n",
      "Loss is -37813.546875\n",
      "Loss is -37880.6015625\n",
      "Loss is -37947.6328125\n",
      "Loss is -38014.640625\n",
      "Loss is -38081.6328125\n",
      "Loss is -38148.59375\n",
      "Loss is -38215.53125\n",
      "Loss is -38282.45703125\n",
      "Loss is -38349.36328125\n",
      "Loss is -38416.26171875\n",
      "Loss is -38483.15234375\n",
      "Loss is -38550.03125\n",
      "Loss is -38616.9140625\n",
      "Loss is -38683.78125\n",
      "Loss is -38750.6484375\n",
      "Loss is -38817.5078125\n",
      "Loss is -38884.3515625\n",
      "Loss is -38951.17578125\n",
      "Loss is -39017.98046875\n",
      "Loss is -39084.76953125\n",
      "Loss is -39151.5390625\n",
      "Loss is -39218.2890625\n",
      "Loss is -39285.0234375\n",
      "Loss is -39351.734375\n",
      "Loss is -39418.4296875\n",
      "Loss is -39485.12109375\n",
      "Loss is -39551.796875\n",
      "Loss is -39618.4765625\n",
      "Loss is -39685.15625\n",
      "Loss is -39751.8515625\n",
      "Loss is -39818.55078125\n",
      "Loss is -39885.27734375\n",
      "Loss is -39952.03515625\n",
      "Loss is -40018.82421875\n",
      "Loss is -40085.6484375\n",
      "Loss is -40152.52734375\n",
      "Loss is -40219.4453125\n",
      "Loss is -40286.40625\n",
      "Loss is -40353.421875\n",
      "Loss is -40420.4765625\n",
      "Loss is -40487.578125\n",
      "Loss is -40554.71484375\n",
      "Loss is -40621.890625\n",
      "Loss is -40689.1015625\n",
      "Loss is -40756.35546875\n",
      "Loss is -40823.6328125\n",
      "Loss is -40890.9296875\n",
      "Loss is -40958.265625\n",
      "Loss is -41025.60546875\n",
      "Loss is -41092.97265625\n",
      "Loss is -41160.34375\n",
      "Loss is -41227.7265625\n",
      "Loss is -41295.12109375\n",
      "Loss is -41362.515625\n",
      "Loss is -41429.9140625\n",
      "Loss is -41497.30859375\n",
      "Loss is -41564.7109375\n",
      "Loss is -41632.09375\n",
      "Loss is -41699.4765625\n",
      "Loss is -41766.8515625\n",
      "Loss is -41834.21875\n",
      "Loss is -41901.5703125\n",
      "Loss is -41968.91015625\n",
      "Loss is -42036.24609375\n",
      "Loss is -42103.5625\n",
      "Loss is -42170.8671875\n",
      "Loss is -42238.1640625\n",
      "Loss is -42305.44140625\n",
      "Loss is -42372.703125\n",
      "Loss is -42439.9453125\n",
      "Loss is -42507.1796875\n",
      "Loss is -42574.39453125\n",
      "Loss is -42641.59375\n",
      "Loss is -42708.7734375\n",
      "Loss is -42775.94140625\n",
      "Loss is -42843.09375\n",
      "Loss is -42910.21875\n",
      "Loss is -42977.3359375\n",
      "Loss is -43044.43359375\n",
      "Loss is -43111.51953125\n",
      "Loss is -43178.578125\n",
      "Loss is -43245.625\n",
      "Loss is -43312.65234375\n",
      "Loss is -43379.66015625\n",
      "Loss is -43446.65234375\n",
      "Loss is -43513.625\n",
      "Loss is -43580.58203125\n",
      "Loss is -43647.5234375\n",
      "Loss is -43714.4453125\n",
      "Loss is -43781.3515625\n",
      "Loss is -43848.2421875\n",
      "Loss is -43915.125\n",
      "Loss is -43981.99609375\n",
      "Loss is -44048.8515625\n",
      "Loss is -44115.703125\n",
      "Loss is -44182.5390625\n",
      "Loss is -44249.375\n",
      "Loss is -44316.19921875\n",
      "Loss is -44383.03125\n",
      "Loss is -44449.8515625\n",
      "Loss is -44516.6796875\n",
      "Loss is -44583.5\n",
      "Loss is -44650.328125\n",
      "Loss is -44717.15625\n",
      "Loss is -44783.9921875\n",
      "Loss is -44850.84375\n",
      "Loss is -44917.703125\n",
      "Loss is -44984.58203125\n",
      "Loss is -45051.484375\n",
      "Loss is -45118.3984375\n",
      "Loss is -45185.34375\n",
      "Loss is -45252.30859375\n",
      "Loss is -45319.3046875\n",
      "Loss is -45386.3359375\n",
      "Loss is -45453.40234375\n",
      "Loss is -45520.50390625\n",
      "Loss is -45587.65625\n",
      "Loss is -45654.859375\n",
      "Loss is -45722.125\n",
      "Loss is -45789.4453125\n",
      "Loss is -45856.828125\n",
      "Loss is -45924.2890625\n",
      "Loss is -45991.8203125\n",
      "Loss is -46059.4375\n",
      "Loss is -46127.14453125\n",
      "Loss is -46194.9375\n",
      "Loss is -46262.828125\n",
      "Loss is -46330.8203125\n",
      "Loss is -46398.9296875\n",
      "Loss is -46467.15625\n",
      "Loss is -46535.484375\n",
      "Loss is -46603.9296875\n",
      "Loss is -46672.4921875\n",
      "Loss is -46741.18359375\n",
      "Loss is -46810.0\n",
      "Loss is -46878.9609375\n",
      "Loss is -46948.046875\n",
      "Loss is -47017.265625\n",
      "Loss is -47086.62109375\n",
      "Loss is -47156.09765625\n",
      "Loss is -47225.7109375\n",
      "Loss is -47295.4453125\n",
      "Loss is -47365.3046875\n",
      "Loss is -47435.30078125\n",
      "Loss is -47505.4140625\n",
      "Loss is -47575.6484375\n",
      "Loss is -47646.0\n",
      "Loss is -47716.46484375\n",
      "Loss is -47787.0390625\n",
      "Loss is -47857.71484375\n",
      "Loss is -47928.5\n",
      "Loss is -47999.375\n",
      "Loss is -48070.34765625\n",
      "Loss is -48141.41796875\n",
      "Loss is -48212.56640625\n",
      "Loss is -48283.80078125\n",
      "Loss is -48355.1171875\n",
      "Loss is -48426.49609375\n",
      "Loss is -48497.9453125\n",
      "Loss is -48569.4609375\n",
      "Loss is -48641.03125\n",
      "Loss is -48712.65625\n",
      "Loss is -48784.33203125\n",
      "Loss is -48856.05859375\n",
      "Loss is -48927.83984375\n",
      "Loss is -48999.6640625\n",
      "Loss is -49071.53125\n",
      "Loss is -49143.4375\n",
      "Loss is -49215.390625\n",
      "Loss is -49287.3828125\n",
      "Loss is -49359.41796875\n",
      "Loss is -49431.48828125\n",
      "Loss is -49503.59375\n",
      "Loss is -49575.734375\n",
      "Loss is -49647.9140625\n",
      "Loss is -49720.140625\n",
      "Loss is -49792.40234375\n",
      "Loss is -49864.71875\n",
      "Loss is -49937.07421875\n",
      "Loss is -50009.4765625\n",
      "Loss is -50081.94140625\n",
      "Loss is -50154.453125\n",
      "Loss is -50227.03125\n",
      "Loss is -50299.6640625\n",
      "Loss is -50372.3671875\n",
      "Loss is -50445.1328125\n",
      "Loss is -50517.984375\n",
      "Loss is -50590.90625\n",
      "Loss is -50663.9140625\n",
      "Loss is -50737.0234375\n",
      "Loss is -50810.21875\n",
      "Loss is -50883.50390625\n",
      "Loss is -50956.890625\n",
      "Loss is -51030.375\n",
      "Loss is -51103.9765625\n",
      "Loss is -51177.68359375\n",
      "Loss is -51251.49609375\n",
      "Loss is -51325.41796875\n",
      "Loss is -51399.44140625\n",
      "Loss is -51473.57421875\n",
      "Loss is -51547.8203125\n",
      "Loss is -51622.1796875\n",
      "Loss is -51696.64453125\n",
      "Loss is -51771.2109375\n",
      "Loss is -51845.88671875\n",
      "Loss is -51920.6640625\n",
      "Loss is -51995.53125\n",
      "Loss is -52070.5\n",
      "Loss is -52145.55859375\n",
      "Loss is -52220.703125\n",
      "Loss is -52295.9375\n",
      "Loss is -52371.25\n",
      "Loss is -52446.6484375\n",
      "Loss is -52522.125\n",
      "Loss is -52597.671875\n",
      "Loss is -52673.296875\n",
      "Loss is -52749.0\n",
      "Loss is -52824.7890625\n",
      "Loss is -52900.64453125\n",
      "Loss is -52976.5703125\n",
      "Loss is -53052.5625\n",
      "Loss is -53128.6328125\n",
      "Loss is -53204.76171875\n",
      "Loss is -53280.95703125\n",
      "Loss is -53357.234375\n",
      "Loss is -53433.56640625\n",
      "Loss is -53509.96484375\n",
      "Loss is -53586.43359375\n",
      "Loss is -53662.94921875\n",
      "Loss is -53739.5234375\n",
      "Loss is -53816.1484375\n",
      "Loss is -53892.8203125\n",
      "Loss is -53969.5546875\n",
      "Loss is -54046.32421875\n",
      "Loss is -54123.1328125\n",
      "Loss is -54199.984375\n",
      "Loss is -54276.875\n",
      "Loss is -54353.80078125\n",
      "Loss is -54430.7578125\n",
      "Loss is -54507.7421875\n",
      "Loss is -54584.75\n",
      "Loss is -54661.78125\n",
      "Loss is -54738.8203125\n",
      "Loss is -54815.8828125\n",
      "Loss is -54892.96484375\n",
      "Loss is -54970.046875\n",
      "Loss is -55047.1328125\n",
      "Loss is -55124.2265625\n",
      "Loss is -55201.328125\n",
      "Loss is -55278.4296875\n",
      "Loss is -55355.5390625\n",
      "Loss is -55432.6484375\n",
      "Loss is -55509.765625\n",
      "Loss is -55586.8828125\n",
      "Loss is -55663.9921875\n",
      "Loss is -55741.125\n",
      "Loss is -55818.25\n",
      "Loss is -55895.3828125\n",
      "Loss is -55972.53125\n",
      "Loss is -56049.6875\n",
      "Loss is -56126.859375\n",
      "Loss is -56204.0625\n",
      "Loss is -56281.2890625\n",
      "Loss is -56358.5546875\n",
      "Loss is -56435.8671875\n",
      "Loss is -56513.21875\n",
      "Loss is -56590.625\n",
      "Loss is -56668.078125\n",
      "Loss is -56745.59765625\n",
      "Loss is -56823.1796875\n",
      "Loss is -56900.8359375\n",
      "Loss is -56978.5703125\n",
      "Loss is -57056.390625\n",
      "Loss is -57134.2890625\n",
      "Loss is -57212.29296875\n",
      "Loss is -57290.38671875\n",
      "Loss is -57368.5859375\n",
      "Loss is -57446.890625\n",
      "Loss is -57525.296875\n",
      "Loss is -57603.8046875\n",
      "Loss is -57682.4140625\n",
      "Loss is -57761.140625\n",
      "Loss is -57839.96875\n",
      "Loss is -57918.890625\n",
      "Loss is -57997.92578125\n",
      "Loss is -58077.046875\n",
      "Loss is -58156.2734375\n",
      "Loss is -58235.5859375\n",
      "Loss is -58314.984375\n",
      "Loss is -58394.4609375\n",
      "Loss is -58474.0234375\n",
      "Loss is -58553.6484375\n",
      "Loss is -58633.34375\n",
      "Loss is -58713.1015625\n",
      "Loss is -58792.9140625\n",
      "Loss is -58872.77734375\n",
      "Loss is -58952.67578125\n",
      "Loss is -59032.625\n",
      "Loss is -59112.6015625\n",
      "Loss is -59192.609375\n",
      "Loss is -59272.640625\n",
      "Loss is -59352.6875\n",
      "Loss is -59432.7578125\n",
      "Loss is -59512.828125\n",
      "Loss is -59592.91015625\n",
      "Loss is -59672.9921875\n",
      "Loss is -59753.0703125\n",
      "Loss is -59833.15234375\n",
      "Loss is -59913.21875\n",
      "Loss is -59993.2734375\n",
      "Loss is -60073.3203125\n",
      "Loss is -60153.359375\n",
      "Loss is -60233.375\n",
      "Loss is -60313.375\n",
      "Loss is -60393.3515625\n",
      "Loss is -60473.30859375\n",
      "Loss is -60553.2421875\n",
      "Loss is -60633.15625\n",
      "Loss is -60713.0390625\n",
      "Loss is -60792.90234375\n",
      "Loss is -60872.73828125\n",
      "Loss is -60952.546875\n",
      "Loss is -61032.32421875\n",
      "Loss is -61112.078125\n",
      "Loss is -61191.8046875\n",
      "Loss is -61271.49609375\n",
      "Loss is -61351.1640625\n",
      "Loss is -61430.796875\n",
      "Loss is -61510.40625\n",
      "Loss is -61589.984375\n",
      "Loss is -61669.53125\n",
      "Loss is -61749.05078125\n",
      "Loss is -61828.5390625\n",
      "Loss is -61907.99609375\n",
      "Loss is -61987.421875\n",
      "Loss is -62066.81640625\n",
      "Loss is -62146.1796875\n",
      "Loss is -62225.515625\n",
      "Loss is -62304.81640625\n",
      "Loss is -62384.0859375\n",
      "Loss is -62463.328125\n",
      "Loss is -62542.52734375\n",
      "Loss is -62621.703125\n",
      "Loss is -62700.8359375\n",
      "Loss is -62779.9375\n",
      "Loss is -62859.00390625\n",
      "Loss is -62938.03125\n",
      "Loss is -63017.0234375\n",
      "Loss is -63095.984375\n",
      "Loss is -63174.890625\n",
      "Loss is -63253.75390625\n",
      "Loss is -63332.578125\n",
      "Loss is -63411.359375\n",
      "Loss is -63490.0859375\n",
      "Loss is -63568.7578125\n",
      "Loss is -63647.3828125\n",
      "Loss is -63725.94140625\n",
      "Loss is -63804.44921875\n",
      "Loss is -63882.890625\n",
      "Loss is -63961.2734375\n",
      "Loss is -64039.58984375\n",
      "Loss is -64117.8359375\n",
      "Loss is -64196.0078125\n",
      "Loss is -64274.1015625\n",
      "Loss is -64352.12890625\n",
      "Loss is -64430.078125\n",
      "Loss is -64507.95703125\n",
      "Loss is -64585.75390625\n",
      "Loss is -64663.47265625\n",
      "Loss is -64741.109375\n",
      "Loss is -64818.671875\n",
      "Loss is -64896.14453125\n",
      "Loss is -64973.5390625\n",
      "Loss is -65050.859375\n",
      "Loss is -65128.09375\n",
      "Loss is -65205.24609375\n",
      "Loss is -65282.328125\n",
      "Loss is -65359.328125\n",
      "Loss is -65436.265625\n",
      "Loss is -65513.140625\n",
      "Loss is -65589.9375\n",
      "Loss is -65666.671875\n",
      "Loss is -65743.34375\n",
      "Loss is -65819.953125\n",
      "Loss is -65896.5078125\n",
      "Loss is -65973.0\n",
      "Loss is -66049.453125\n",
      "Loss is -66125.8515625\n",
      "Loss is -66202.1953125\n",
      "Loss is -66278.5\n",
      "Loss is -66354.75\n",
      "Loss is -66430.9609375\n",
      "Loss is -66507.140625\n",
      "Loss is -66583.265625\n",
      "Loss is -66659.359375\n",
      "Loss is -66735.421875\n",
      "Loss is -66811.453125\n",
      "Loss is -66887.4375\n",
      "Loss is -66963.3828125\n",
      "Loss is -67039.3125\n",
      "Loss is -67115.1953125\n",
      "Loss is -67191.0625\n",
      "Loss is -67266.890625\n",
      "Loss is -67342.671875\n",
      "Loss is -67418.4375\n",
      "Loss is -67494.1640625\n",
      "Loss is -67569.875\n",
      "Loss is -67645.546875\n",
      "Loss is -67721.1875\n",
      "Loss is -67796.796875\n",
      "Loss is -67872.390625\n",
      "Loss is -67947.9453125\n",
      "Loss is -68023.46875\n",
      "Loss is -68098.96875\n",
      "Loss is -68174.453125\n",
      "Loss is -68249.8984375\n",
      "Loss is -68325.3125\n",
      "Loss is -68400.703125\n",
      "Loss is -68476.0703125\n",
      "Loss is -68551.40625\n",
      "Loss is -68626.71875\n",
      "Loss is -68701.9921875\n",
      "Loss is -68777.25\n",
      "Loss is -68852.46875\n",
      "Loss is -68927.65625\n",
      "Loss is -69002.828125\n",
      "Loss is -69077.96875\n",
      "Loss is -69153.078125\n",
      "Loss is -69228.171875\n",
      "Loss is -69303.2265625\n",
      "Loss is -69378.265625\n",
      "Loss is -69453.265625\n",
      "Loss is -69528.25\n",
      "Loss is -69603.203125\n",
      "Loss is -69678.1328125\n",
      "Loss is -69753.0390625\n",
      "Loss is -69827.90625\n",
      "Loss is -69902.75\n",
      "Loss is -69977.578125\n",
      "Loss is -70052.375\n",
      "Loss is -70127.1484375\n",
      "Loss is -70201.8828125\n",
      "Loss is -70276.59375\n",
      "Loss is -70351.28125\n",
      "Loss is -70425.953125\n",
      "Loss is -70500.578125\n",
      "Loss is -70575.1796875\n",
      "Loss is -70649.765625\n",
      "Loss is -70724.3203125\n",
      "Loss is -70798.8359375\n",
      "Loss is -70873.3359375\n",
      "Loss is -70947.8046875\n",
      "Loss is -71022.25\n",
      "Loss is -71096.6640625\n",
      "Loss is -71171.046875\n",
      "Loss is -71245.40625\n",
      "Loss is -71319.734375\n",
      "Loss is -71394.0234375\n",
      "Loss is -71468.296875\n",
      "Loss is -71542.5390625\n",
      "Loss is -71616.75\n",
      "Loss is -71690.953125\n",
      "Loss is -71765.109375\n",
      "Loss is -71839.25\n",
      "Loss is -71913.359375\n",
      "Loss is -71987.4453125\n",
      "Loss is -72061.5078125\n",
      "Loss is -72135.546875\n",
      "Loss is -72209.578125\n",
      "Loss is -72283.5625\n",
      "Loss is -72357.5390625\n",
      "Loss is -72431.484375\n",
      "Loss is -72505.421875\n",
      "Loss is -72579.3359375\n",
      "Loss is -72653.21875\n",
      "Loss is -72727.1015625\n",
      "Loss is -72800.96875\n",
      "Loss is -72874.8046875\n",
      "Loss is -72948.6328125\n",
      "Loss is -73022.453125\n",
      "Loss is -73096.2578125\n",
      "Loss is -73170.03125\n",
      "Loss is -73243.8046875\n",
      "Loss is -73317.5625\n",
      "Loss is -73391.3125\n",
      "Loss is -73465.046875\n",
      "Loss is -73538.7578125\n",
      "Loss is -73612.4609375\n",
      "Loss is -73686.15625\n",
      "Loss is -73759.8359375\n",
      "Loss is -73833.5\n",
      "Loss is -73907.15625\n",
      "Loss is -73980.8046875\n",
      "Loss is -74054.4375\n",
      "Loss is -74128.046875\n",
      "Loss is -74201.640625\n",
      "Loss is -74275.234375\n",
      "Loss is -74348.8125\n",
      "Loss is -74422.3671875\n",
      "Loss is -74495.921875\n",
      "Loss is -74569.4453125\n",
      "Loss is -74642.96875\n",
      "Loss is -74716.46875\n",
      "Loss is -74789.9609375\n",
      "Loss is -74863.421875\n",
      "Loss is -74936.8828125\n",
      "Loss is -75010.3203125\n",
      "Loss is -75083.7421875\n",
      "Loss is -75157.15625\n",
      "Loss is -75230.546875\n",
      "Loss is -75303.9140625\n",
      "Loss is -75377.28125\n",
      "Loss is -75450.6328125\n",
      "Loss is -75523.96875\n",
      "Loss is -75597.2734375\n",
      "Loss is -75670.5703125\n",
      "Loss is -75743.859375\n",
      "Loss is -75817.125\n",
      "Loss is -75890.375\n",
      "Loss is -75963.6171875\n",
      "Loss is -76036.828125\n",
      "Loss is -76110.0390625\n",
      "Loss is -76183.234375\n",
      "Loss is -76256.40625\n",
      "Loss is -76329.5703125\n",
      "Loss is -76402.71875\n",
      "Loss is -76475.8515625\n",
      "Loss is -76548.96875\n",
      "Loss is -76622.0703125\n",
      "Loss is -76695.171875\n",
      "Loss is -76768.25\n",
      "Loss is -76841.3046875\n",
      "Loss is -76914.3515625\n",
      "Loss is -76987.40625\n",
      "Loss is -77060.4140625\n",
      "Loss is -77133.4296875\n",
      "Loss is -77206.4296875\n",
      "Loss is -77279.4140625\n",
      "Loss is -77352.390625\n",
      "Loss is -77425.359375\n",
      "Loss is -77498.296875\n",
      "Loss is -77571.25\n",
      "Loss is -77644.171875\n",
      "Loss is -77717.09375\n",
      "Loss is -77790.015625\n",
      "Loss is -77862.9140625\n",
      "Loss is -77935.8125\n",
      "Loss is -78008.703125\n",
      "Loss is -78081.5859375\n",
      "Loss is -78154.46875\n",
      "Loss is -78227.328125\n",
      "Loss is -78300.1953125\n",
      "Loss is -78373.046875\n",
      "Loss is -78445.90625\n",
      "Loss is -78518.7578125\n",
      "Loss is -78591.609375\n",
      "Loss is -78664.453125\n",
      "Loss is -78737.296875\n",
      "Loss is -78810.140625\n",
      "Loss is -78882.9765625\n",
      "Loss is -78955.8203125\n",
      "Loss is -79028.671875\n",
      "Loss is -79101.5\n",
      "Loss is -79174.3515625\n",
      "Loss is -79247.203125\n",
      "Loss is -79320.046875\n",
      "Loss is -79392.890625\n",
      "Loss is -79465.734375\n",
      "Loss is -79538.59375\n",
      "Loss is -79611.453125\n",
      "Loss is -79684.296875\n",
      "Loss is -79757.15625\n",
      "Loss is -79830.015625\n",
      "Loss is -79902.875\n",
      "Loss is -79975.734375\n",
      "Loss is -80048.59375\n",
      "Loss is -80121.46875\n",
      "Loss is -80194.328125\n",
      "Loss is -80267.1875\n",
      "Loss is -80340.0625\n",
      "Loss is -80412.921875\n",
      "Loss is -80485.7734375\n",
      "Loss is -80558.640625\n",
      "Loss is -80631.5078125\n",
      "Loss is -80704.359375\n",
      "Loss is -80777.2109375\n",
      "Loss is -80850.078125\n",
      "Loss is -80922.921875\n",
      "Loss is -80995.765625\n",
      "Loss is -81068.6015625\n",
      "Loss is -81141.4375\n",
      "Loss is -81214.265625\n",
      "Loss is -81287.09375\n",
      "Loss is -81359.9140625\n",
      "Loss is -81432.734375\n",
      "Loss is -81505.53125\n",
      "Loss is -81578.34375\n",
      "Loss is -81651.125\n",
      "Loss is -81723.921875\n",
      "Loss is -81796.703125\n",
      "Loss is -81869.46875\n",
      "Loss is -81942.25\n",
      "Loss is -82015.015625\n",
      "Loss is -82087.765625\n",
      "Loss is -82160.515625\n",
      "Loss is -82233.265625\n",
      "Loss is -82306.0078125\n",
      "Loss is -82378.75\n",
      "Loss is -82451.484375\n",
      "Loss is -82524.21875\n",
      "Loss is -82596.9453125\n",
      "Loss is -82669.671875\n",
      "Loss is -82742.40625\n",
      "Loss is -82815.140625\n",
      "Loss is -82887.859375\n",
      "Loss is -82960.6015625\n",
      "Loss is -83033.34375\n",
      "Loss is -83106.09375\n",
      "Loss is -83178.859375\n",
      "Loss is -83251.625\n",
      "Loss is -83324.3984375\n",
      "Loss is -83397.171875\n",
      "Loss is -83469.96875\n",
      "Loss is -83542.765625\n",
      "Loss is -83615.578125\n",
      "Loss is -83688.4140625\n",
      "Loss is -83761.265625\n",
      "Loss is -83834.125\n",
      "Loss is -83907.0078125\n",
      "Loss is -83979.890625\n",
      "Loss is -84052.8046875\n",
      "Loss is -84125.7421875\n",
      "Loss is -84198.703125\n",
      "Loss is -84271.671875\n",
      "Loss is -84344.65625\n",
      "Loss is -84417.671875\n",
      "Loss is -84490.703125\n",
      "Loss is -84563.75\n",
      "Loss is -84636.8125\n",
      "Loss is -84709.90625\n",
      "Loss is -84783.015625\n",
      "Loss is -84856.140625\n",
      "Loss is -84929.28125\n",
      "Loss is -85002.4453125\n",
      "Loss is -85075.609375\n",
      "Loss is -85148.8046875\n",
      "Loss is -85222.0\n",
      "Loss is -85295.21875\n",
      "Loss is -85368.4375\n",
      "Loss is -85441.6875\n",
      "Loss is -85514.921875\n",
      "Loss is -85588.1875\n",
      "Loss is -85661.4453125\n",
      "Loss is -85734.7109375\n",
      "Loss is -85807.984375\n",
      "Loss is -85881.265625\n",
      "Loss is -85954.546875\n",
      "Loss is -86027.828125\n",
      "Loss is -86101.125\n",
      "Loss is -86174.4140625\n",
      "Loss is -86247.703125\n",
      "Loss is -86321.0\n",
      "Loss is -86394.28125\n",
      "Loss is -86467.5703125\n",
      "Loss is -86540.859375\n",
      "Loss is -86614.1484375\n",
      "Loss is -86687.4375\n",
      "Loss is -86760.7109375\n",
      "Loss is -86834.0\n",
      "Loss is -86907.28125\n",
      "Loss is -86980.5546875\n",
      "Loss is -87053.828125\n",
      "Loss is -87127.1015625\n",
      "Loss is -87200.375\n",
      "Loss is -87273.640625\n",
      "Loss is -87346.90625\n",
      "Loss is -87420.171875\n",
      "Loss is -87493.4296875\n",
      "Loss is -87566.6875\n",
      "Loss is -87639.953125\n",
      "Loss is -87713.203125\n",
      "Loss is -87786.4453125\n",
      "Loss is -87859.6953125\n",
      "Loss is -87932.9375\n",
      "Loss is -88006.15625\n",
      "Loss is -88079.3828125\n",
      "Loss is -88152.6015625\n",
      "Loss is -88225.8125\n",
      "Loss is -88299.0234375\n",
      "Loss is -88372.21875\n",
      "Loss is -88445.40625\n",
      "Loss is -88518.59375\n",
      "Loss is -88591.765625\n",
      "Loss is -88664.9296875\n",
      "Loss is -88738.078125\n",
      "Loss is -88811.2109375\n",
      "Loss is -88884.34375\n",
      "Loss is -88957.453125\n",
      "Loss is -89030.5546875\n",
      "Loss is -89103.640625\n",
      "Loss is -89176.71875\n",
      "Loss is -89249.7890625\n",
      "Loss is -89322.84375\n",
      "Loss is -89395.890625\n",
      "Loss is -89468.921875\n",
      "Loss is -89541.953125\n",
      "Loss is -89614.96875\n",
      "Loss is -89687.96875\n",
      "Loss is -89760.96875\n",
      "Loss is -89833.9375\n",
      "Loss is -89906.921875\n",
      "Loss is -89979.8828125\n",
      "Loss is -90052.8359375\n",
      "Loss is -90125.796875\n",
      "Loss is -90198.7421875\n",
      "Loss is -90271.6875\n",
      "Loss is -90344.625\n",
      "Loss is -90417.5546875\n",
      "Loss is -90490.5078125\n",
      "Loss is -90563.453125\n",
      "Loss is -90636.421875\n",
      "Loss is -90709.3828125\n",
      "Loss is -90782.359375\n",
      "Loss is -90855.328125\n",
      "Loss is -90928.328125\n",
      "Loss is -91001.328125\n",
      "Loss is -91074.3515625\n",
      "Loss is -91147.40625\n",
      "Loss is -91220.453125\n",
      "Loss is -91293.5390625\n",
      "Loss is -91366.640625\n",
      "Loss is -91439.765625\n",
      "Loss is -91512.921875\n",
      "Loss is -91586.109375\n",
      "Loss is -91659.328125\n",
      "Loss is -91732.5625\n",
      "Loss is -91805.828125\n",
      "Loss is -91879.140625\n",
      "Loss is -91952.46875\n",
      "Loss is -92025.828125\n",
      "Loss is -92099.21875\n",
      "Loss is -92172.640625\n",
      "Loss is -92246.109375\n",
      "Loss is -92319.59375\n",
      "Loss is -92393.1171875\n",
      "Loss is -92466.65625\n",
      "Loss is -92540.2421875\n",
      "Loss is -92613.8515625\n",
      "Loss is -92687.484375\n",
      "Loss is -92761.140625\n",
      "Loss is -92834.828125\n",
      "Loss is -92908.5390625\n",
      "Loss is -92982.2734375\n",
      "Loss is -93056.0390625\n",
      "Loss is -93129.828125\n",
      "Loss is -93203.625\n",
      "Loss is -93277.453125\n",
      "Loss is -93351.296875\n",
      "Loss is -93425.15625\n",
      "Loss is -93499.03125\n",
      "Loss is -93572.921875\n",
      "Loss is -93646.828125\n",
      "Loss is -93720.75\n",
      "Loss is -93794.6875\n",
      "Loss is -93868.6328125\n",
      "Loss is -93942.59375\n",
      "Loss is -94016.578125\n",
      "Loss is -94090.5703125\n",
      "Loss is -94164.578125\n",
      "Loss is -94238.6015625\n",
      "Loss is -94312.640625\n",
      "Loss is -94386.703125\n",
      "Loss is -94460.78125\n",
      "Loss is -94534.875\n",
      "Loss is -94608.984375\n",
      "Loss is -94683.125\n",
      "Loss is -94757.28125\n",
      "Loss is -94831.484375\n",
      "Loss is -94905.6875\n",
      "Loss is -94979.9375\n",
      "Loss is -95054.21875\n",
      "Loss is -95128.515625\n",
      "Loss is -95202.8515625\n",
      "Loss is -95277.21875\n",
      "Loss is -95351.609375\n",
      "Loss is -95426.046875\n",
      "Loss is -95500.515625\n",
      "Loss is -95575.0\n",
      "Loss is -95649.515625\n",
      "Loss is -95724.0625\n",
      "Loss is -95798.640625\n",
      "Loss is -95873.234375\n",
      "Loss is -95947.8671875\n",
      "Loss is -96022.515625\n",
      "Loss is -96097.1796875\n",
      "Loss is -96171.8828125\n",
      "Loss is -96246.59375\n",
      "Loss is -96321.3125\n",
      "Loss is -96396.0625\n",
      "Loss is -96470.8125\n",
      "Loss is -96545.5703125\n",
      "Loss is -96620.359375\n",
      "Loss is -96695.1484375\n",
      "Loss is -96769.9375\n",
      "Loss is -96844.75\n",
      "Loss is -96919.5546875\n",
      "Loss is -96994.375\n",
      "Loss is -97069.1953125\n",
      "Loss is -97144.0234375\n",
      "Loss is -97218.84375\n",
      "Loss is -97293.671875\n",
      "Loss is -97368.4921875\n",
      "Loss is -97443.328125\n",
      "Loss is -97518.1640625\n",
      "Loss is -97592.984375\n",
      "Loss is -97667.828125\n",
      "Loss is -97742.65625\n",
      "Loss is -97817.5\n",
      "Loss is -97892.3515625\n",
      "Loss is -97967.1953125\n",
      "Loss is -98042.046875\n",
      "Loss is -98116.8984375\n",
      "Loss is -98191.765625\n",
      "Loss is -98266.640625\n",
      "Loss is -98341.5234375\n",
      "Loss is -98416.421875\n",
      "Loss is -98491.3203125\n",
      "Loss is -98566.25\n",
      "Loss is -98641.171875\n",
      "Loss is -98716.125\n",
      "Loss is -98791.09375\n",
      "Loss is -98866.0625\n",
      "Loss is -98941.046875\n",
      "Loss is -99016.0546875\n",
      "Loss is -99091.0703125\n",
      "Loss is -99166.109375\n",
      "Loss is -99241.1796875\n",
      "Loss is -99316.25\n",
      "Loss is -99391.359375\n",
      "Loss is -99466.484375\n",
      "Loss is -99541.609375\n",
      "Loss is -99616.78125\n",
      "Loss is -99691.9609375\n",
      "Loss is -99767.15625\n",
      "Loss is -99842.3671875\n",
      "Loss is -99917.578125\n",
      "Loss is -99992.8125\n",
      "Loss is -100068.0625\n",
      "Loss is -100143.3203125\n",
      "Loss is -100218.578125\n",
      "Loss is -100293.8515625\n",
      "Loss is -100369.125\n",
      "Loss is -100444.40625\n",
      "Loss is -100519.6953125\n",
      "Loss is -100595.0\n",
      "Loss is -100670.3125\n",
      "Loss is -100745.6328125\n",
      "Loss is -100820.953125\n",
      "Loss is -100896.28125\n",
      "Loss is -100971.625\n",
      "Loss is -101046.96875\n",
      "Loss is -101122.328125\n",
      "Loss is -101197.671875\n",
      "Loss is -101273.03125\n",
      "Loss is -101348.3984375\n",
      "Loss is -101423.7734375\n",
      "Loss is -101499.140625\n",
      "Loss is -101574.515625\n",
      "Loss is -101649.90625\n",
      "Loss is -101725.2734375\n",
      "Loss is -101800.65625\n",
      "Loss is -101876.03125\n",
      "Loss is -101951.40625\n",
      "Loss is -102026.78125\n",
      "Loss is -102102.15625\n",
      "Loss is -102177.515625\n",
      "Loss is -102252.8828125\n",
      "Loss is -102328.234375\n",
      "Loss is -102403.578125\n",
      "Loss is -102478.90625\n",
      "Loss is -102554.25\n",
      "Loss is -102629.5625\n",
      "Loss is -102704.875\n",
      "Loss is -102780.171875\n",
      "Loss is -102855.453125\n",
      "Loss is -102930.734375\n",
      "Loss is -103005.9921875\n",
      "Loss is -103081.2421875\n",
      "Loss is -103156.484375\n",
      "Loss is -103231.703125\n",
      "Loss is -103306.90625\n",
      "Loss is -103382.1015625\n",
      "Loss is -103457.28125\n",
      "Loss is -103532.4375\n",
      "Loss is -103607.578125\n",
      "Loss is -103682.71875\n",
      "Loss is -103757.828125\n",
      "Loss is -103832.921875\n",
      "Loss is -103908.0\n",
      "Loss is -103983.0625\n",
      "Loss is -104058.125\n",
      "Loss is -104133.1484375\n",
      "Loss is -104208.171875\n",
      "Loss is -104283.171875\n",
      "Loss is -104358.140625\n",
      "Loss is -104433.109375\n",
      "Loss is -104508.0625\n",
      "Loss is -104582.9921875\n",
      "Loss is -104657.90625\n",
      "Loss is -104732.8125\n",
      "Loss is -104807.6875\n",
      "Loss is -104882.5546875\n",
      "Loss is -104957.3984375\n",
      "Loss is -105032.2265625\n",
      "Loss is -105107.046875\n",
      "Loss is -105181.8359375\n",
      "Loss is -105256.609375\n",
      "Loss is -105331.375\n",
      "Loss is -105406.1171875\n",
      "Loss is -105480.84375\n",
      "Loss is -105555.5625\n",
      "Loss is -105630.265625\n",
      "Loss is -105704.953125\n",
      "Loss is -105779.6171875\n",
      "Loss is -105854.265625\n",
      "Loss is -105928.8984375\n",
      "Loss is -106003.515625\n",
      "Loss is -106078.1171875\n",
      "Loss is -106152.7109375\n",
      "Loss is -106227.2734375\n",
      "Loss is -106301.828125\n",
      "Loss is -106376.375\n",
      "Loss is -106450.890625\n",
      "Loss is -106525.390625\n",
      "Loss is -106599.890625\n",
      "Loss is -106674.3671875\n",
      "Loss is -106748.8203125\n",
      "Loss is -106823.265625\n",
      "Loss is -106897.703125\n",
      "Loss is -106972.109375\n",
      "Loss is -107046.515625\n",
      "Loss is -107120.890625\n",
      "Loss is -107195.265625\n",
      "Loss is -107269.609375\n",
      "Loss is -107343.953125\n",
      "Loss is -107418.2734375\n",
      "Loss is -107492.578125\n",
      "Loss is -107566.859375\n",
      "Loss is -107641.140625\n",
      "Loss is -107715.3984375\n",
      "Loss is -107789.640625\n",
      "Loss is -107863.875\n",
      "Loss is -107938.09375\n",
      "Loss is -108012.28125\n",
      "Loss is -108086.46875\n",
      "Loss is -108160.640625\n",
      "Loss is -108234.7890625\n",
      "Loss is -108308.9296875\n",
      "Loss is -108383.0546875\n",
      "Loss is -108457.1640625\n",
      "Loss is -108531.265625\n",
      "Loss is -108605.359375\n",
      "Loss is -108679.421875\n",
      "Loss is -108753.4765625\n",
      "Loss is -108827.53125\n",
      "Loss is -108901.5625\n",
      "Loss is -108975.578125\n",
      "Loss is -109049.59375\n",
      "Loss is -109123.6015625\n",
      "Loss is -109197.578125\n",
      "Loss is -109271.5625\n",
      "Loss is -109345.53125\n",
      "Loss is -109419.4921875\n",
      "Loss is -109493.4453125\n",
      "Loss is -109567.390625\n",
      "Loss is -109641.328125\n",
      "Loss is -109715.25\n",
      "Loss is -109789.171875\n",
      "Loss is -109863.09375\n",
      "Loss is -109936.9921875\n",
      "Loss is -110010.90625\n",
      "Loss is -110084.796875\n",
      "Loss is -110158.6875\n",
      "Loss is -110232.578125\n",
      "Loss is -110306.46875\n",
      "Loss is -110380.3515625\n",
      "Loss is -110454.234375\n",
      "Loss is -110528.125\n",
      "Loss is -110602.0\n",
      "Loss is -110675.8828125\n",
      "Loss is -110749.7734375\n",
      "Loss is -110823.6484375\n",
      "Loss is -110897.5234375\n",
      "Loss is -110971.40625\n",
      "Loss is -111045.296875\n",
      "Loss is -111119.1796875\n",
      "Loss is -111193.0546875\n",
      "Loss is -111266.9375\n",
      "Loss is -111340.828125\n",
      "Loss is -111414.71875\n",
      "Loss is -111488.609375\n",
      "Loss is -111562.5\n",
      "Loss is -111636.375\n",
      "Loss is -111710.28125\n",
      "Loss is -111784.171875\n",
      "Loss is -111858.0625\n",
      "Loss is -111931.953125\n",
      "Loss is -112005.859375\n",
      "Loss is -112079.7734375\n",
      "Loss is -112153.6640625\n",
      "Loss is -112227.5625\n",
      "Loss is -112301.4609375\n",
      "Loss is -112375.375\n",
      "Loss is -112449.265625\n",
      "Loss is -112523.1796875\n",
      "Loss is -112597.078125\n",
      "Loss is -112670.96875\n",
      "Loss is -112744.859375\n",
      "Loss is -112818.7578125\n",
      "Loss is -112892.6484375\n",
      "Loss is -112966.53125\n",
      "Loss is -113040.40625\n",
      "Loss is -113114.265625\n",
      "Loss is -113188.140625\n",
      "Loss is -113261.9921875\n",
      "Loss is -113335.8359375\n",
      "Loss is -113409.671875\n",
      "Loss is -113483.4921875\n",
      "Loss is -113557.3125\n",
      "Loss is -113631.109375\n",
      "Loss is -113704.90625\n",
      "Loss is -113778.6953125\n",
      "Loss is -113852.4609375\n",
      "Loss is -113926.2109375\n",
      "Loss is -113999.953125\n",
      "Loss is -114073.65625\n",
      "Loss is -114147.375\n",
      "Loss is -114221.0625\n",
      "Loss is -114294.734375\n",
      "Loss is -114368.390625\n",
      "Loss is -114442.03125\n",
      "Loss is -114515.65625\n",
      "Loss is -114589.265625\n",
      "Loss is -114662.859375\n",
      "Loss is -114736.4375\n",
      "Loss is -114809.9921875\n",
      "Loss is -114883.546875\n",
      "Loss is -114957.078125\n",
      "Loss is -115030.6015625\n",
      "Loss is -115104.109375\n",
      "Loss is -115177.609375\n",
      "Loss is -115251.0859375\n",
      "Loss is -115324.5546875\n",
      "Loss is -115398.0234375\n",
      "Loss is -115471.46875\n",
      "Loss is -115544.90625\n",
      "Loss is -115618.3359375\n",
      "Loss is -115691.75\n",
      "Loss is -115765.171875\n",
      "Loss is -115838.5703125\n",
      "Loss is -115911.96875\n",
      "Loss is -115985.3515625\n",
      "Loss is -116058.7421875\n",
      "Loss is -116132.109375\n",
      "Loss is -116205.5\n",
      "Loss is -116278.890625\n",
      "Loss is -116352.28125\n",
      "Loss is -116425.65625\n",
      "Loss is -116499.03125\n",
      "Loss is -116572.4453125\n",
      "Loss is -116645.84375\n",
      "Loss is -116719.25\n",
      "Loss is -116792.6640625\n",
      "Loss is -116866.09375\n",
      "Loss is -116939.546875\n",
      "Loss is -117013.0078125\n",
      "Loss is -117086.484375\n",
      "Loss is -117159.9765625\n",
      "Loss is -117233.5\n",
      "Loss is -117307.046875\n",
      "Loss is -117380.6171875\n",
      "Loss is -117454.21875\n",
      "Loss is -117527.859375\n",
      "Loss is -117601.5390625\n",
      "Loss is -117675.265625\n",
      "Loss is -117749.0234375\n",
      "Loss is -117822.828125\n",
      "Loss is -117896.6875\n",
      "Loss is -117970.6015625\n",
      "Loss is -118044.578125\n",
      "Loss is -118118.6015625\n",
      "Loss is -118192.6875\n",
      "Loss is -118266.8515625\n",
      "Loss is -118341.0859375\n",
      "Loss is -118415.390625\n",
      "Loss is -118489.765625\n",
      "Loss is -118564.2109375\n",
      "Loss is -118638.75\n",
      "Loss is -118713.375\n",
      "Loss is -118788.078125\n",
      "Loss is -118862.8828125\n",
      "Loss is -118937.78125\n",
      "Loss is -119012.7578125\n",
      "Loss is -119087.8515625\n",
      "Loss is -119163.03125\n",
      "Loss is -119238.296875\n",
      "Loss is -119313.671875\n",
      "Loss is -119389.1328125\n",
      "Loss is -119464.6796875\n",
      "Loss is -119540.3359375\n",
      "Loss is -119616.078125\n",
      "Loss is -119691.921875\n",
      "Loss is -119767.8515625\n",
      "Loss is -119843.890625\n",
      "Loss is -119920.015625\n",
      "Loss is -119996.2265625\n",
      "Loss is -120072.53125\n",
      "Loss is -120148.921875\n",
      "Loss is -120225.390625\n",
      "Loss is -120301.953125\n",
      "Loss is -120378.578125\n",
      "Loss is -120455.2890625\n",
      "Loss is -120532.0703125\n",
      "Loss is -120608.921875\n",
      "Loss is -120685.84375\n",
      "Loss is -120762.8203125\n",
      "Loss is -120839.859375\n",
      "Loss is -120916.953125\n",
      "Loss is -120994.109375\n",
      "Loss is -121071.3125\n",
      "Loss is -121148.5625\n",
      "Loss is -121225.8515625\n",
      "Loss is -121303.1875\n",
      "Loss is -121380.5625\n",
      "Loss is -121457.96875\n",
      "Loss is -121535.4140625\n",
      "Loss is -121612.890625\n",
      "Loss is -121690.3828125\n",
      "Loss is -121767.90625\n",
      "Loss is -121845.4453125\n",
      "Loss is -121923.0078125\n",
      "Loss is -122000.5859375\n",
      "Loss is -122078.1875\n",
      "Loss is -122155.78125\n",
      "Loss is -122233.4140625\n",
      "Loss is -122311.03125\n",
      "Loss is -122388.65625\n",
      "Loss is -122466.296875\n",
      "Loss is -122543.9375\n",
      "Loss is -122621.5859375\n",
      "Loss is -122699.21875\n",
      "Loss is -122776.859375\n",
      "Loss is -122854.5\n",
      "Loss is -122932.140625\n",
      "Loss is -123009.78125\n",
      "Loss is -123087.40625\n",
      "Loss is -123165.03125\n",
      "Loss is -123242.6484375\n",
      "Loss is -123320.2578125\n",
      "Loss is -123397.859375\n",
      "Loss is -123475.453125\n",
      "Loss is -123553.046875\n",
      "Loss is -123630.6171875\n",
      "Loss is -123708.1875\n",
      "Loss is -123785.75\n",
      "Loss is -123863.2890625\n",
      "Loss is -123940.828125\n",
      "Loss is -124018.359375\n",
      "Loss is -124095.875\n",
      "Loss is -124173.375\n",
      "Loss is -124250.859375\n",
      "Loss is -124328.3359375\n",
      "Loss is -124405.8125\n",
      "Loss is -124483.2578125\n",
      "Loss is -124560.703125\n",
      "Loss is -124638.1328125\n",
      "Loss is -124715.546875\n",
      "Loss is -124792.953125\n",
      "Loss is -124870.34375\n",
      "Loss is -124947.71875\n",
      "Loss is -125025.09375\n",
      "Loss is -125102.4375\n",
      "Loss is -125179.7734375\n",
      "Loss is -125257.109375\n",
      "Loss is -125334.421875\n",
      "Loss is -125411.7265625\n",
      "Loss is -125489.0234375\n",
      "Loss is -125566.296875\n",
      "Loss is -125643.5625\n",
      "Loss is -125720.8203125\n",
      "Loss is -125798.046875\n",
      "Loss is -125875.2734375\n",
      "Loss is -125952.484375\n",
      "Loss is -126029.6875\n",
      "Loss is -126106.8671875\n",
      "Loss is -126184.03125\n",
      "Loss is -126261.203125\n",
      "Loss is -126338.3515625\n",
      "Loss is -126415.484375\n",
      "Loss is -126492.609375\n",
      "Loss is -126569.71875\n",
      "Loss is -126646.8125\n",
      "Loss is -126723.890625\n",
      "Loss is -126800.96875\n",
      "Loss is -126878.03125\n",
      "Loss is -126955.078125\n",
      "Loss is -127032.125\n",
      "Loss is -127109.140625\n",
      "Loss is -127186.15625\n",
      "Loss is -127263.15625\n",
      "Loss is -127340.1328125\n",
      "Loss is -127417.1171875\n",
      "Loss is -127494.0703125\n",
      "Loss is -127571.03125\n",
      "Loss is -127647.9765625\n",
      "Loss is -127724.890625\n",
      "Loss is -127801.8203125\n",
      "Loss is -127878.71875\n",
      "Loss is -127955.609375\n",
      "Loss is -128032.5\n",
      "Loss is -128109.3828125\n",
      "Loss is -128186.234375\n",
      "Loss is -128263.1015625\n",
      "Loss is -128339.953125\n",
      "Loss is -128416.765625\n",
      "Loss is -128493.6015625\n",
      "Loss is -128570.40625\n",
      "Loss is -128647.2265625\n",
      "Loss is -128724.0078125\n",
      "Loss is -128800.796875\n",
      "Loss is -128877.5859375\n",
      "Loss is -128954.3515625\n",
      "Loss is -129031.125\n",
      "Loss is -129107.875\n",
      "Loss is -129184.625\n",
      "Loss is -129261.359375\n",
      "Loss is -129338.09375\n",
      "Loss is -129414.828125\n",
      "Loss is -129491.546875\n",
      "Loss is -129568.2578125\n",
      "Loss is -129644.96875\n",
      "Loss is -129721.671875\n",
      "Loss is -129798.3671875\n",
      "Loss is -129875.046875\n",
      "Loss is -129951.75\n",
      "Loss is -130028.4296875\n",
      "Loss is -130105.1171875\n",
      "Loss is -130181.796875\n",
      "Loss is -130258.4609375\n",
      "Loss is -130335.125\n",
      "Loss is -130411.7890625\n",
      "Loss is -130488.453125\n",
      "Loss is -130565.109375\n",
      "Loss is -130641.7578125\n",
      "Loss is -130718.40625\n",
      "Loss is -130795.0625\n",
      "Loss is -130871.71875\n",
      "Loss is -130948.34375\n",
      "Loss is -131024.984375\n",
      "Loss is -131101.625\n",
      "Loss is -131178.25\n",
      "Loss is -131254.90625\n",
      "Loss is -131331.53125\n",
      "Loss is -131408.15625\n",
      "Loss is -131484.78125\n",
      "Loss is -131561.40625\n",
      "Loss is -131638.03125\n",
      "Loss is -131714.625\n",
      "Loss is -131791.234375\n",
      "Loss is -131867.84375\n",
      "Loss is -131944.453125\n",
      "Loss is -132021.0625\n",
      "Loss is -132097.640625\n",
      "Loss is -132174.25\n",
      "Loss is -132250.828125\n",
      "Loss is -132327.40625\n",
      "Loss is -132403.984375\n",
      "Loss is -132480.546875\n",
      "Loss is -132557.09375\n",
      "Loss is -132633.65625\n",
      "Loss is -132710.21875\n",
      "Loss is -132786.765625\n",
      "Loss is -132863.28125\n",
      "Loss is -132939.8125\n",
      "Loss is -133016.34375\n",
      "Loss is -133092.84375\n",
      "Loss is -133169.375\n",
      "Loss is -133245.875\n",
      "Loss is -133322.34375\n",
      "Loss is -133398.828125\n",
      "Loss is -133475.296875\n",
      "Loss is -133551.75\n",
      "Loss is -133628.203125\n",
      "Loss is -133704.640625\n",
      "Loss is -133781.0625\n",
      "Loss is -133857.5\n",
      "Loss is -133933.90625\n",
      "Loss is -134010.3125\n",
      "Loss is -134086.703125\n",
      "Loss is -134163.078125\n",
      "Loss is -134239.453125\n",
      "Loss is -134315.8125\n",
      "Loss is -134392.15625\n",
      "Loss is -134468.5\n",
      "Loss is -134544.84375\n",
      "Loss is -134621.15625\n",
      "Loss is -134697.46875\n",
      "Loss is -134773.75\n",
      "Loss is -134850.03125\n",
      "Loss is -134926.3125\n",
      "Loss is -135002.578125\n",
      "Loss is -135078.828125\n",
      "Loss is -135155.0625\n",
      "Loss is -135231.296875\n",
      "Loss is -135307.515625\n",
      "Loss is -135383.734375\n",
      "Loss is -135459.90625\n",
      "Loss is -135536.109375\n",
      "Loss is -135612.28125\n",
      "Loss is -135688.4375\n",
      "Loss is -135764.59375\n",
      "Loss is -135840.71875\n",
      "Loss is -135916.875\n",
      "Loss is -135993.0\n",
      "Loss is -136069.109375\n",
      "Loss is -136145.21875\n",
      "Loss is -136221.3125\n",
      "Loss is -136297.40625\n",
      "Loss is -136373.5\n",
      "Loss is -136449.578125\n",
      "Loss is -136525.65625\n",
      "Loss is -136601.71875\n",
      "Loss is -136677.78125\n",
      "Loss is -136753.828125\n",
      "Loss is -136829.875\n",
      "Loss is -136905.921875\n",
      "Loss is -136981.96875\n",
      "Loss is -137058.0\n",
      "Loss is -137134.046875\n",
      "Loss is -137210.078125\n",
      "Loss is -137286.125\n",
      "Loss is -137362.15625\n",
      "Loss is -137438.203125\n",
      "Loss is -137514.25\n",
      "Loss is -137590.28125\n",
      "Loss is -137666.34375\n",
      "Loss is -137742.375\n",
      "Loss is -137818.4375\n",
      "Loss is -137894.5\n",
      "Loss is -137970.546875\n",
      "Loss is -138046.609375\n",
      "Loss is -138122.671875\n",
      "Loss is -138198.75\n",
      "Loss is -138274.84375\n",
      "Loss is -138350.90625\n",
      "Loss is -138427.015625\n",
      "Loss is -138503.125\n",
      "Loss is -138579.21875\n",
      "Loss is -138655.328125\n",
      "Loss is -138731.4375\n",
      "Loss is -138807.5625\n",
      "Loss is -138883.65625\n",
      "Loss is -138959.78125\n",
      "Loss is -139035.921875\n",
      "Loss is -139112.03125\n",
      "Loss is -139188.171875\n",
      "Loss is -139264.3125\n",
      "Loss is -139340.4375\n",
      "Loss is -139416.578125\n",
      "Loss is -139492.71875\n",
      "Loss is -139568.84375\n",
      "Loss is -139644.984375\n",
      "Loss is -139721.109375\n",
      "Loss is -139797.25\n",
      "Loss is -139873.375\n",
      "Loss is -139949.5\n",
      "Loss is -140025.625\n",
      "Loss is -140101.734375\n",
      "Loss is -140177.84375\n",
      "Loss is -140253.9375\n",
      "Loss is -140330.0625\n",
      "Loss is -140406.140625\n",
      "Loss is -140482.234375\n",
      "Loss is -140558.3125\n",
      "Loss is -140634.40625\n",
      "Loss is -140710.46875\n",
      "Loss is -140786.53125\n",
      "Loss is -140862.578125\n",
      "Loss is -140938.640625\n",
      "Loss is -141014.6875\n",
      "Loss is -141090.71875\n",
      "Loss is -141166.734375\n",
      "Loss is -141242.75\n",
      "Loss is -141318.765625\n",
      "Loss is -141394.75\n",
      "Loss is -141470.75\n",
      "Loss is -141546.71875\n",
      "Loss is -141622.703125\n",
      "Loss is -141698.65625\n",
      "Loss is -141774.625\n",
      "Loss is -141850.5625\n",
      "Loss is -141926.5\n",
      "Loss is -142002.4375\n",
      "Loss is -142078.34375\n",
      "Loss is -142154.25\n",
      "Loss is -142230.15625\n",
      "Loss is -142306.046875\n",
      "Loss is -142381.9375\n",
      "Loss is -142457.8125\n",
      "Loss is -142533.671875\n",
      "Loss is -142609.53125\n",
      "Loss is -142685.375\n",
      "Loss is -142761.234375\n",
      "Loss is -142837.0625\n",
      "Loss is -142912.875\n",
      "Loss is -142988.6875\n",
      "Loss is -143064.5\n",
      "Loss is -143140.28125\n",
      "Loss is -143216.0625\n",
      "Loss is -143291.84375\n",
      "Loss is -143367.609375\n",
      "Loss is -143443.375\n",
      "Loss is -143519.109375\n",
      "Loss is -143594.84375\n",
      "Loss is -143670.59375\n",
      "Loss is -143746.3125\n",
      "Loss is -143822.03125\n",
      "Loss is -143897.71875\n",
      "Loss is -143973.4375\n",
      "Loss is -144049.109375\n",
      "Loss is -144124.78125\n",
      "Loss is -144200.453125\n",
      "Loss is -144276.125\n",
      "Loss is -144351.78125\n",
      "Loss is -144427.40625\n",
      "Loss is -144503.046875\n",
      "Loss is -144578.671875\n",
      "Loss is -144654.28125\n",
      "Loss is -144729.875\n",
      "Loss is -144805.484375\n",
      "Loss is -144881.078125\n",
      "Loss is -144956.65625\n",
      "Loss is -145032.234375\n",
      "Loss is -145107.796875\n",
      "Loss is -145183.375\n",
      "Loss is -145258.90625\n",
      "Loss is -145334.4375\n",
      "Loss is -145409.96875\n",
      "Loss is -145485.5\n",
      "Loss is -145561.0\n",
      "Loss is -145636.515625\n",
      "Loss is -145712.015625\n",
      "Loss is -145787.5\n",
      "Loss is -145862.984375\n",
      "Loss is -145938.453125\n",
      "Loss is -146013.90625\n",
      "Loss is -146089.375\n",
      "Loss is -146164.8125\n",
      "Loss is -146240.25\n",
      "Loss is -146315.6875\n",
      "Loss is -146391.125\n",
      "Loss is -146466.53125\n",
      "Loss is -146541.9375\n",
      "Loss is -146617.34375\n",
      "Loss is -146692.71875\n",
      "Loss is -146768.09375\n",
      "Loss is -146843.46875\n",
      "Loss is -146918.828125\n",
      "Loss is -146994.203125\n",
      "Loss is -147069.5625\n",
      "Loss is -147144.90625\n",
      "Loss is -147220.21875\n",
      "Loss is -147295.53125\n",
      "Loss is -147370.875\n",
      "Loss is -147446.171875\n",
      "Loss is -147521.46875\n",
      "Loss is -147596.78125\n",
      "Loss is -147672.0625\n",
      "Loss is -147747.34375\n",
      "Loss is -147822.59375\n",
      "Loss is -147897.875\n",
      "Loss is -147973.125\n",
      "Loss is -148048.375\n",
      "Loss is -148123.609375\n",
      "Loss is -148198.828125\n",
      "Loss is -148274.0625\n",
      "Loss is -148349.28125\n",
      "Loss is -148424.484375\n",
      "Loss is -148499.6875\n",
      "Loss is -148574.875\n",
      "Loss is -148650.0625\n",
      "Loss is -148725.25\n",
      "Loss is -148800.40625\n",
      "Loss is -148875.5625\n",
      "Loss is -148950.71875\n",
      "Loss is -149025.875\n",
      "Loss is -149101.015625\n",
      "Loss is -149176.15625\n",
      "Loss is -149251.265625\n",
      "Loss is -149326.375\n",
      "Loss is -149401.5\n",
      "Loss is -149476.59375\n",
      "Loss is -149551.671875\n",
      "Loss is -149626.765625\n",
      "Loss is -149701.84375\n",
      "Loss is -149776.90625\n",
      "Loss is -149851.96875\n",
      "Loss is -149927.0\n",
      "Loss is -150002.0625\n",
      "Loss is -150077.109375\n",
      "Loss is -150152.125\n",
      "Loss is -150227.125\n",
      "Loss is -150302.15625\n",
      "Loss is -150377.15625\n",
      "Loss is -150452.140625\n",
      "Loss is -150527.125\n",
      "Loss is -150602.125\n",
      "Loss is -150677.078125\n",
      "Loss is -150752.03125\n",
      "Loss is -150826.984375\n",
      "Loss is -150901.9375\n",
      "Loss is -150976.84375\n",
      "Loss is -151051.765625\n",
      "Loss is -151126.671875\n",
      "Loss is -151201.5625\n",
      "Loss is -151276.46875\n",
      "Loss is -151351.34375\n",
      "Loss is -151426.1875\n",
      "Loss is -151501.03125\n",
      "Loss is -151575.890625\n",
      "Loss is -151650.71875\n",
      "Loss is -151725.53125\n",
      "Loss is -151800.34375\n",
      "Loss is -151875.125\n",
      "Loss is -151949.90625\n",
      "Loss is -152024.671875\n",
      "Loss is -152099.4375\n",
      "Loss is -152174.171875\n",
      "Loss is -152248.875\n",
      "Loss is -152323.59375\n",
      "Loss is -152398.28125\n",
      "Loss is -152472.9375\n",
      "Loss is -152547.625\n",
      "Loss is -152622.265625\n",
      "Loss is -152696.875\n",
      "Loss is -152771.5\n",
      "Loss is -152846.09375\n",
      "Loss is -152920.6875\n",
      "Loss is -152995.25\n",
      "Loss is -153069.796875\n",
      "Loss is -153144.34375\n",
      "Loss is -153218.859375\n",
      "Loss is -153293.34375\n",
      "Loss is -153367.84375\n",
      "Loss is -153442.3125\n",
      "Loss is -153516.78125\n",
      "Loss is -153591.203125\n",
      "Loss is -153665.640625\n",
      "Loss is -153740.046875\n",
      "Loss is -153814.453125\n",
      "Loss is -153888.8125\n",
      "Loss is -153963.203125\n",
      "Loss is -154037.5625\n",
      "Loss is -154111.890625\n",
      "Loss is -154186.21875\n",
      "Loss is -154260.53125\n",
      "Loss is -154334.84375\n",
      "Loss is -154409.125\n",
      "Loss is -154483.40625\n",
      "Loss is -154557.703125\n",
      "Loss is -154631.9375\n",
      "Loss is -154706.203125\n",
      "Loss is -154780.453125\n",
      "Loss is -154854.6875\n",
      "Loss is -154928.90625\n",
      "Loss is -155003.140625\n",
      "Loss is -155077.359375\n",
      "Loss is -155151.5625\n",
      "Loss is -155225.75\n",
      "Loss is -155299.96875\n",
      "Loss is -155374.15625\n",
      "Loss is -155448.34375\n",
      "Loss is -155522.53125\n",
      "Loss is -155596.703125\n",
      "Loss is -155670.890625\n",
      "Loss is -155745.0625\n",
      "Loss is -155819.25\n",
      "Loss is -155893.40625\n",
      "Loss is -155967.5625\n",
      "Loss is -156041.75\n",
      "Loss is -156115.9375\n",
      "Loss is -156190.09375\n",
      "Loss is -156264.25\n",
      "Loss is -156338.4375\n",
      "Loss is -156412.59375\n",
      "Loss is -156486.765625\n",
      "Loss is -156560.9375\n",
      "Loss is -156635.109375\n",
      "Loss is -156709.28125\n",
      "Loss is -156783.46875\n",
      "Loss is -156857.640625\n",
      "Loss is -156931.828125\n",
      "Loss is -157006.0\n",
      "Loss is -157080.1875\n",
      "Loss is -157154.375\n",
      "Loss is -157228.578125\n",
      "Loss is -157302.765625\n",
      "Loss is -157376.96875\n",
      "Loss is -157451.171875\n",
      "Loss is -157525.390625\n",
      "Loss is -157599.59375\n",
      "Loss is -157673.8125\n",
      "Loss is -157748.03125\n",
      "Loss is -157822.25\n",
      "Loss is -157896.5\n",
      "Loss is -157970.75\n",
      "Loss is -158044.96875\n",
      "Loss is -158119.21875\n",
      "Loss is -158193.46875\n",
      "Loss is -158267.75\n",
      "Loss is -158342.0\n",
      "Loss is -158416.28125\n",
      "Loss is -158490.5625\n",
      "Loss is -158564.84375\n",
      "Loss is -158639.125\n",
      "Loss is -158713.40625\n",
      "Loss is -158787.71875\n",
      "Loss is -158862.015625\n",
      "Loss is -158936.328125\n",
      "Loss is -159010.65625\n",
      "Loss is -159084.96875\n",
      "Loss is -159159.296875\n",
      "Loss is -159233.640625\n",
      "Loss is -159307.953125\n",
      "Loss is -159382.3125\n",
      "Loss is -159456.65625\n",
      "Loss is -159531.015625\n",
      "Loss is -159605.375\n",
      "Loss is -159679.75\n",
      "Loss is -159754.109375\n",
      "Loss is -159828.5\n",
      "Loss is -159902.875\n",
      "Loss is -159977.25\n",
      "Loss is -160051.65625\n",
      "Loss is -160126.046875\n",
      "Loss is -160200.4375\n",
      "Loss is -160274.84375\n",
      "Loss is -160349.21875\n",
      "Loss is -160423.65625\n",
      "Loss is -160498.0625\n",
      "Loss is -160572.484375\n",
      "Loss is -160646.890625\n",
      "Loss is -160721.3125\n",
      "Loss is -160795.734375\n",
      "Loss is -160870.15625\n",
      "Loss is -160944.578125\n",
      "Loss is -161019.0\n",
      "Loss is -161093.4375\n",
      "Loss is -161167.859375\n",
      "Loss is -161242.28125\n",
      "Loss is -161316.71875\n",
      "Loss is -161391.125\n",
      "Loss is -161465.578125\n",
      "Loss is -161540.0\n",
      "Loss is -161614.421875\n",
      "Loss is -161688.84375\n",
      "Loss is -161763.25\n",
      "Loss is -161837.6875\n",
      "Loss is -161912.078125\n",
      "Loss is -161986.5\n",
      "Loss is -162060.90625\n",
      "Loss is -162135.3125\n",
      "Loss is -162209.6875\n",
      "Loss is -162284.09375\n",
      "Loss is -162358.46875\n",
      "Loss is -162432.84375\n",
      "Loss is -162507.21875\n",
      "Loss is -162581.5625\n",
      "Loss is -162655.9375\n",
      "Loss is -162730.28125\n",
      "Loss is -162804.59375\n",
      "Loss is -162878.9375\n",
      "Loss is -162953.21875\n",
      "Loss is -163027.53125\n",
      "Loss is -163101.8125\n",
      "Loss is -163176.09375\n",
      "Loss is -163250.3125\n",
      "Loss is -163324.5625\n",
      "Loss is -163398.78125\n",
      "Loss is -163473.0\n",
      "Loss is -163547.1875\n",
      "Loss is -163621.34375\n",
      "Loss is -163695.5\n",
      "Loss is -163769.625\n",
      "Loss is -163843.71875\n",
      "Loss is -163917.8125\n",
      "Loss is -163991.875\n",
      "Loss is -164065.90625\n",
      "Loss is -164139.921875\n",
      "Loss is -164213.921875\n",
      "Loss is -164287.875\n",
      "Loss is -164361.8125\n",
      "Loss is -164435.75\n",
      "Loss is -164509.640625\n",
      "Loss is -164583.515625\n",
      "Loss is -164657.375\n",
      "Loss is -164731.21875\n",
      "Loss is -164805.015625\n",
      "Loss is -164878.796875\n",
      "Loss is -164952.5625\n",
      "Loss is -165026.296875\n",
      "Loss is -165100.015625\n",
      "Loss is -165173.6875\n",
      "Loss is -165247.375\n",
      "Loss is -165321.03125\n",
      "Loss is -165394.65625\n",
      "Loss is -165468.265625\n",
      "Loss is -165541.84375\n",
      "Loss is -165615.40625\n",
      "Loss is -165688.953125\n",
      "Loss is -165762.484375\n",
      "Loss is -165836.0\n",
      "Loss is -165909.5\n",
      "Loss is -165982.984375\n",
      "Loss is -166056.4375\n",
      "Loss is -166129.90625\n",
      "Loss is -166203.34375\n",
      "Loss is -166276.765625\n",
      "Loss is -166350.171875\n",
      "Loss is -166423.578125\n",
      "Loss is -166496.921875\n",
      "Loss is -166570.296875\n",
      "Loss is -166643.65625\n",
      "Loss is -166717.0\n",
      "Loss is -166790.328125\n",
      "Loss is -166863.65625\n",
      "Loss is -166936.96875\n",
      "Loss is -167010.265625\n",
      "Loss is -167083.546875\n",
      "Loss is -167156.8125\n",
      "Loss is -167230.09375\n",
      "Loss is -167303.359375\n",
      "Loss is -167376.59375\n",
      "Loss is -167449.8125\n",
      "Loss is -167523.046875\n",
      "Loss is -167596.28125\n",
      "Loss is -167669.46875\n",
      "Loss is -167742.6875\n",
      "Loss is -167815.875\n",
      "Loss is -167889.0625\n",
      "Loss is -167962.21875\n",
      "Loss is -168035.375\n",
      "Loss is -168108.546875\n",
      "Loss is -168181.6875\n",
      "Loss is -168254.84375\n",
      "Loss is -168327.96875\n",
      "Loss is -168401.078125\n",
      "Loss is -168474.1875\n",
      "Loss is -168547.3125\n",
      "Loss is -168620.40625\n",
      "Loss is -168693.5\n",
      "Loss is -168766.578125\n",
      "Loss is -168839.65625\n",
      "Loss is -168912.71875\n",
      "Loss is -168985.78125\n",
      "Loss is -169058.84375\n",
      "Loss is -169131.875\n",
      "Loss is -169204.90625\n",
      "Loss is -169277.9375\n",
      "Loss is -169350.953125\n",
      "Loss is -169423.984375\n",
      "Loss is -169496.984375\n",
      "Loss is -169569.984375\n",
      "Loss is -169642.96875\n",
      "Loss is -169715.9375\n",
      "Loss is -169788.90625\n",
      "Loss is -169861.875\n",
      "Loss is -169934.84375\n",
      "Loss is -170007.78125\n",
      "Loss is -170080.71875\n",
      "Loss is -170153.65625\n",
      "Loss is -170226.59375\n",
      "Loss is -170299.515625\n",
      "Loss is -170372.4375\n",
      "Loss is -170445.34375\n",
      "Loss is -170518.25\n",
      "Loss is -170591.140625\n",
      "Loss is -170664.015625\n",
      "Loss is -170736.90625\n",
      "Loss is -170809.75\n",
      "Loss is -170882.625\n",
      "Loss is -170955.46875\n",
      "Loss is -171028.3125\n",
      "Loss is -171101.15625\n",
      "Loss is -171174.0\n",
      "Loss is -171246.8125\n",
      "Loss is -171319.625\n",
      "Loss is -171392.453125\n",
      "Loss is -171465.25\n",
      "Loss is -171538.03125\n",
      "Loss is -171610.84375\n",
      "Loss is -171683.625\n",
      "Loss is -171756.375\n",
      "Loss is -171829.140625\n",
      "Loss is -171901.90625\n",
      "Loss is -171974.671875\n",
      "Loss is -172047.390625\n",
      "Loss is -172120.140625\n",
      "Loss is -172192.875\n",
      "Loss is -172265.578125\n",
      "Loss is -172338.28125\n",
      "Loss is -172411.0\n",
      "Loss is -172483.703125\n",
      "Loss is -172556.390625\n",
      "Loss is -172629.0625\n",
      "Loss is -172701.734375\n",
      "Loss is -172774.40625\n",
      "Loss is -172847.0625\n",
      "Loss is -172919.734375\n",
      "Loss is -172992.375\n",
      "Loss is -173065.0\n",
      "Loss is -173137.640625\n",
      "Loss is -173210.25\n",
      "Loss is -173282.875\n",
      "Loss is -173355.5\n",
      "Loss is -173428.09375\n",
      "Loss is -173500.6875\n",
      "Loss is -173573.28125\n",
      "Loss is -173645.84375\n",
      "Loss is -173718.421875\n",
      "Loss is -173790.96875\n",
      "Loss is -173863.53125\n",
      "Loss is -173936.09375\n",
      "Loss is -174008.625\n",
      "Loss is -174081.171875\n",
      "Loss is -174153.6875\n",
      "Loss is -174226.21875\n",
      "Loss is -174298.75\n",
      "Loss is -174371.25\n",
      "Loss is -174443.734375\n",
      "Loss is -174516.21875\n",
      "Loss is -174588.71875\n",
      "Loss is -174661.1875\n",
      "Loss is -174733.65625\n",
      "Loss is -174806.109375\n",
      "Loss is -174878.59375\n",
      "Loss is -174951.015625\n",
      "Loss is -175023.46875\n",
      "Loss is -175095.875\n",
      "Loss is -175168.328125\n",
      "Loss is -175240.75\n",
      "Loss is -175313.15625\n",
      "Loss is -175385.546875\n",
      "Loss is -175457.953125\n",
      "Loss is -175530.34375\n",
      "Loss is -175602.734375\n",
      "Loss is -175675.09375\n",
      "Loss is -175747.46875\n",
      "Loss is -175819.828125\n",
      "Loss is -175892.15625\n",
      "Loss is -175964.515625\n",
      "Loss is -176036.875\n",
      "Loss is -176109.1875\n",
      "Loss is -176181.515625\n",
      "Loss is -176253.84375\n",
      "Loss is -176326.15625\n",
      "Loss is -176398.453125\n",
      "Loss is -176470.734375\n",
      "Loss is -176543.03125\n",
      "Loss is -176615.3125\n",
      "Loss is -176687.578125\n",
      "Loss is -176759.875\n",
      "Loss is -176832.125\n",
      "Loss is -176904.375\n",
      "Loss is -176976.625\n",
      "Loss is -177048.859375\n",
      "Loss is -177121.109375\n",
      "Loss is -177193.34375\n",
      "Loss is -177265.546875\n",
      "Loss is -177337.75\n",
      "Loss is -177409.96875\n",
      "Loss is -177482.15625\n",
      "Loss is -177554.375\n",
      "Loss is -177626.5625\n",
      "Loss is -177698.71875\n",
      "Loss is -177770.890625\n",
      "Loss is -177843.0625\n",
      "Loss is -177915.21875\n",
      "Loss is -177987.359375\n",
      "Loss is -178059.5\n",
      "Loss is -178131.65625\n",
      "Loss is -178203.78125\n",
      "Loss is -178275.921875\n",
      "Loss is -178348.03125\n",
      "Loss is -178420.125\n",
      "Loss is -178492.25\n",
      "Loss is -178564.359375\n",
      "Loss is -178636.4375\n",
      "Loss is -178708.546875\n",
      "Loss is -178780.625\n",
      "Loss is -178852.71875\n",
      "Loss is -178924.765625\n",
      "Loss is -178996.828125\n",
      "Loss is -179068.875\n",
      "Loss is -179140.9375\n",
      "Loss is -179212.96875\n",
      "Loss is -179285.015625\n",
      "Loss is -179357.03125\n",
      "Loss is -179429.0625\n",
      "Loss is -179501.078125\n",
      "Loss is -179573.09375\n",
      "Loss is -179645.09375\n",
      "Loss is -179717.09375\n",
      "Loss is -179789.09375\n",
      "Loss is -179861.09375\n",
      "Loss is -179933.09375\n",
      "Loss is -180005.0625\n",
      "Loss is -180077.015625\n",
      "Loss is -180148.96875\n",
      "Loss is -180220.9375\n",
      "Loss is -180292.890625\n",
      "Loss is -180364.828125\n",
      "Loss is -180436.765625\n",
      "Loss is -180508.703125\n",
      "Loss is -180580.640625\n",
      "Loss is -180652.5625\n",
      "Loss is -180724.46875\n",
      "Loss is -180796.390625\n",
      "Loss is -180868.296875\n",
      "Loss is -180940.1875\n",
      "Loss is -181012.09375\n",
      "Loss is -181084.0\n",
      "Loss is -181155.859375\n",
      "Loss is -181227.71875\n",
      "Loss is -181299.625\n",
      "Loss is -181371.46875\n",
      "Loss is -181443.34375\n",
      "Loss is -181515.171875\n",
      "Loss is -181587.03125\n",
      "Loss is -181658.875\n",
      "Loss is -181730.703125\n",
      "Loss is -181802.53125\n",
      "Loss is -181874.375\n",
      "Loss is -181946.1875\n",
      "Loss is -182017.984375\n",
      "Loss is -182089.796875\n",
      "Loss is -182161.625\n",
      "Loss is -182233.40625\n",
      "Loss is -182305.21875\n",
      "Loss is -182376.984375\n",
      "Loss is -182448.765625\n",
      "Loss is -182520.53125\n",
      "Loss is -182592.3125\n",
      "Loss is -182664.0625\n",
      "Loss is -182735.828125\n",
      "Loss is -182807.578125\n",
      "Loss is -182879.3125\n",
      "Loss is -182951.0625\n",
      "Loss is -183022.8125\n",
      "Loss is -183094.53125\n",
      "Loss is -183166.265625\n",
      "Loss is -183237.96875\n",
      "Loss is -183309.6875\n",
      "Loss is -183381.40625\n",
      "Loss is -183453.125\n",
      "Loss is -183524.8125\n",
      "Loss is -183596.5\n",
      "Loss is -183668.1875\n",
      "Loss is -183739.875\n",
      "Loss is -183811.53125\n",
      "Loss is -183883.21875\n",
      "Loss is -183954.90625\n",
      "Loss is -184026.546875\n",
      "Loss is -184098.21875\n",
      "Loss is -184169.859375\n",
      "Loss is -184241.5\n",
      "Loss is -184313.125\n",
      "Loss is -184384.765625\n",
      "Loss is -184456.40625\n",
      "Loss is -184528.03125\n",
      "Loss is -184599.640625\n",
      "Loss is -184671.28125\n",
      "Loss is -184742.890625\n",
      "Loss is -184814.46875\n",
      "Loss is -184886.09375\n",
      "Loss is -184957.6875\n",
      "Loss is -185029.265625\n",
      "Loss is -185100.84375\n",
      "Loss is -185172.421875\n",
      "Loss is -185244.0\n",
      "Loss is -185315.578125\n",
      "Loss is -185387.15625\n",
      "Loss is -185458.6875\n",
      "Loss is -185530.25\n",
      "Loss is -185601.796875\n",
      "Loss is -185673.34375\n",
      "Loss is -185744.859375\n",
      "Loss is -185816.40625\n",
      "Loss is -185887.96875\n",
      "Loss is -185959.46875\n",
      "Loss is -186030.984375\n",
      "Loss is -186102.515625\n",
      "Loss is -186174.0\n",
      "Loss is -186245.5\n",
      "Loss is -186317.0\n",
      "Loss is -186388.515625\n",
      "Loss is -186460.0\n",
      "Loss is -186531.46875\n",
      "Loss is -186602.96875\n",
      "Loss is -186674.4375\n",
      "Loss is -186745.9375\n",
      "Loss is -186817.375\n",
      "Loss is -186888.8125\n",
      "Loss is -186960.28125\n",
      "Loss is -187031.75\n",
      "Loss is -187103.1875\n",
      "Loss is -187174.625\n",
      "Loss is -187246.0625\n",
      "Loss is -187317.5\n",
      "Loss is -187388.90625\n",
      "Loss is -187460.375\n",
      "Loss is -187531.78125\n",
      "Loss is -187603.1875\n",
      "Loss is -187674.59375\n",
      "Loss is -187746.0\n",
      "Loss is -187817.375\n",
      "Loss is -187888.78125\n",
      "Loss is -187960.15625\n",
      "Loss is -188031.5625\n",
      "Loss is -188102.9375\n",
      "Loss is -188174.296875\n",
      "Loss is -188245.6875\n",
      "Loss is -188317.046875\n",
      "Loss is -188388.40625\n",
      "Loss is -188459.78125\n",
      "Loss is -188531.09375\n",
      "Loss is -188602.46875\n",
      "Loss is -188673.78125\n",
      "Loss is -188745.109375\n",
      "Loss is -188816.46875\n",
      "Loss is -188887.796875\n",
      "Loss is -188959.09375\n",
      "Loss is -189030.4375\n",
      "Loss is -189101.75\n",
      "Loss is -189173.0625\n",
      "Loss is -189244.34375\n",
      "Loss is -189315.65625\n",
      "Loss is -189386.9375\n",
      "Loss is -189458.25\n",
      "Loss is -189529.53125\n",
      "Loss is -189600.8125\n",
      "Loss is -189672.078125\n",
      "Loss is -189743.375\n",
      "Loss is -189814.625\n",
      "Loss is -189885.875\n",
      "Loss is -189957.125\n",
      "Loss is -190028.40625\n",
      "Loss is -190099.65625\n",
      "Loss is -190170.890625\n",
      "Loss is -190242.09375\n",
      "Loss is -190313.359375\n",
      "Loss is -190384.609375\n",
      "Loss is -190455.8125\n",
      "Loss is -190527.046875\n",
      "Loss is -190598.25\n",
      "Loss is -190669.46875\n",
      "Loss is -190740.6875\n",
      "Loss is -190811.875\n",
      "Loss is -190883.0625\n",
      "Loss is -190954.265625\n",
      "Loss is -191025.4375\n",
      "Loss is -191096.625\n",
      "Loss is -191167.8125\n",
      "Loss is -191238.96875\n",
      "Loss is -191310.140625\n",
      "Loss is -191381.3125\n",
      "Loss is -191452.46875\n",
      "Loss is -191523.625\n",
      "Loss is -191594.765625\n",
      "Loss is -191665.90625\n",
      "Loss is -191737.046875\n",
      "Loss is -191808.1875\n",
      "Loss is -191879.3125\n",
      "Loss is -191950.453125\n",
      "Loss is -192021.578125\n",
      "Loss is -192092.671875\n",
      "Loss is -192163.78125\n",
      "Loss is -192234.90625\n",
      "Loss is -192306.0\n",
      "Loss is -192377.09375\n",
      "Loss is -192448.1875\n",
      "Loss is -192519.265625\n",
      "Loss is -192590.328125\n",
      "Loss is -192661.4375\n",
      "Loss is -192732.5\n",
      "Loss is -192803.5625\n",
      "Loss is -192874.640625\n",
      "Loss is -192945.671875\n",
      "Loss is -193016.71875\n",
      "Loss is -193087.765625\n",
      "Loss is -193158.8125\n",
      "Loss is -193229.84375\n",
      "Loss is -193300.90625\n",
      "Loss is -193371.90625\n",
      "Loss is -193442.9375\n",
      "Loss is -193513.9375\n",
      "Loss is -193584.96875\n",
      "Loss is -193655.953125\n",
      "Loss is -193726.96875\n",
      "Loss is -193797.953125\n",
      "Loss is -193868.953125\n",
      "Loss is -193939.9375\n",
      "Loss is -194010.921875\n",
      "Loss is -194081.90625\n",
      "Loss is -194152.875\n",
      "Loss is -194223.84375\n",
      "Loss is -194294.8125\n",
      "Loss is -194365.765625\n",
      "Loss is -194436.71875\n",
      "Loss is -194507.6875\n",
      "Loss is -194578.625\n",
      "Loss is -194649.53125\n",
      "Loss is -194720.484375\n",
      "Loss is -194791.421875\n",
      "Loss is -194862.328125\n",
      "Loss is -194933.25\n",
      "Loss is -195004.1875\n",
      "Loss is -195075.0625\n",
      "Loss is -195145.96875\n",
      "Loss is -195216.875\n",
      "Loss is -195287.78125\n",
      "Loss is -195358.65625\n",
      "Loss is -195429.5625\n",
      "Loss is -195500.4375\n",
      "Loss is -195571.3125\n",
      "Loss is -195642.1875\n",
      "Loss is -195713.0625\n",
      "Loss is -195783.90625\n",
      "Loss is -195854.765625\n",
      "Loss is -195925.625\n",
      "Loss is -195996.453125\n",
      "Loss is -196067.296875\n",
      "Loss is -196138.140625\n",
      "Loss is -196208.96875\n",
      "Loss is -196279.8125\n",
      "Loss is -196350.640625\n",
      "Loss is -196421.4375\n",
      "Loss is -196492.28125\n",
      "Loss is -196563.078125\n",
      "Loss is -196633.875\n",
      "Loss is -196704.6875\n",
      "Loss is -196775.46875\n",
      "Loss is -196846.25\n",
      "Loss is -196917.03125\n",
      "Loss is -196987.8125\n",
      "Loss is -197058.578125\n",
      "Loss is -197129.375\n",
      "Loss is -197200.125\n",
      "Loss is -197270.875\n",
      "Loss is -197341.65625\n",
      "Loss is -197412.40625\n",
      "Loss is -197483.15625\n",
      "Loss is -197553.890625\n",
      "Loss is -197624.609375\n",
      "Loss is -197695.34375\n",
      "Loss is -197766.0625\n",
      "Loss is -197836.796875\n",
      "Loss is -197907.515625\n",
      "Loss is -197978.21875\n",
      "Loss is -198048.9375\n",
      "Loss is -198119.625\n",
      "Loss is -198190.3125\n",
      "Loss is -198261.015625\n",
      "Loss is -198331.6875\n",
      "Loss is -198402.375\n",
      "Loss is -198473.046875\n",
      "Loss is -198543.71875\n",
      "Loss is -198614.359375\n",
      "Loss is -198685.0\n",
      "Loss is -198755.65625\n",
      "Loss is -198826.3125\n",
      "Loss is -198896.953125\n",
      "Loss is -198967.59375\n",
      "Loss is -199038.21875\n",
      "Loss is -199108.828125\n",
      "Loss is -199179.4375\n",
      "Loss is -199250.0625\n",
      "Loss is -199320.65625\n",
      "Loss is -199391.265625\n",
      "Loss is -199461.859375\n",
      "Loss is -199532.453125\n",
      "Loss is -199603.0\n",
      "Loss is -199673.59375\n",
      "Loss is -199744.15625\n",
      "Loss is -199814.734375\n",
      "Loss is -199885.28125\n",
      "Loss is -199955.828125\n",
      "Loss is -200026.375\n",
      "Loss is -200096.90625\n",
      "Loss is -200167.453125\n",
      "Loss is -200237.984375\n",
      "Loss is -200308.5\n",
      "Loss is -200379.0\n",
      "Loss is -200449.53125\n",
      "Loss is -200520.03125\n",
      "Loss is -200590.53125\n",
      "Loss is -200661.0\n",
      "Loss is -200731.5\n",
      "Loss is -200801.984375\n",
      "Loss is -200872.46875\n",
      "Loss is -200942.9375\n",
      "Loss is -201013.40625\n",
      "Loss is -201083.859375\n",
      "Loss is -201154.328125\n",
      "Loss is -201224.765625\n",
      "Loss is -201295.21875\n",
      "Loss is -201365.65625\n",
      "Loss is -201436.09375\n",
      "Loss is -201506.53125\n",
      "Loss is -201576.953125\n",
      "Loss is -201647.359375\n",
      "Loss is -201717.78125\n",
      "Loss is -201788.1875\n",
      "Loss is -201858.59375\n",
      "Loss is -201928.96875\n",
      "Loss is -201999.375\n",
      "Loss is -202069.765625\n",
      "Loss is -202140.15625\n",
      "Loss is -202210.53125\n",
      "Loss is -202280.875\n",
      "Loss is -202351.25\n",
      "Loss is -202421.609375\n",
      "Loss is -202491.96875\n",
      "Loss is -202562.3125\n",
      "Loss is -202632.6875\n",
      "Loss is -202703.015625\n",
      "Loss is -202773.375\n",
      "Loss is -202843.6875\n",
      "Loss is -202914.03125\n",
      "Loss is -202984.34375\n",
      "Loss is -203054.671875\n",
      "Loss is -203125.0\n",
      "Loss is -203195.28125\n",
      "Loss is -203265.609375\n",
      "Loss is -203335.9375\n",
      "Loss is -203406.203125\n",
      "Loss is -203476.5\n",
      "Loss is -203546.8125\n",
      "Loss is -203617.09375\n",
      "Loss is -203687.375\n",
      "Loss is -203757.65625\n",
      "Loss is -203827.9375\n",
      "Loss is -203898.203125\n",
      "Loss is -203968.453125\n",
      "Loss is -204038.734375\n",
      "Loss is -204109.0\n",
      "Loss is -204179.25\n",
      "Loss is -204249.5\n",
      "Loss is -204319.75\n",
      "Loss is -204390.0\n",
      "Loss is -204460.234375\n",
      "Loss is -204530.46875\n",
      "Loss is -204600.6875\n",
      "Loss is -204670.9375\n",
      "Loss is -204741.15625\n",
      "Loss is -204811.390625\n",
      "Loss is -204881.625\n",
      "Loss is -204951.84375\n",
      "Loss is -205022.03125\n",
      "Loss is -205092.25\n",
      "Loss is -205162.46875\n",
      "Loss is -205232.65625\n",
      "Loss is -205302.859375\n",
      "Loss is -205373.03125\n",
      "Loss is -205443.21875\n",
      "Loss is -205513.40625\n",
      "Loss is -205583.609375\n",
      "Loss is -205653.78125\n",
      "Loss is -205723.953125\n",
      "Loss is -205794.125\n",
      "Loss is -205864.28125\n",
      "Loss is -205934.46875\n",
      "Loss is -206004.625\n",
      "Loss is -206074.796875\n",
      "Loss is -206144.9375\n",
      "Loss is -206215.09375\n",
      "Loss is -206285.21875\n",
      "Loss is -206355.390625\n",
      "Loss is -206425.53125\n",
      "Loss is -206495.65625\n",
      "Loss is -206565.78125\n",
      "Loss is -206635.921875\n",
      "Loss is -206706.0625\n",
      "Loss is -206776.1875\n",
      "Loss is -206846.3125\n",
      "Loss is -206916.4375\n",
      "Loss is -206986.53125\n",
      "Loss is -207056.671875\n",
      "Loss is -207126.78125\n",
      "Loss is -207196.90625\n",
      "Loss is -207267.0\n",
      "Loss is -207337.109375\n",
      "Loss is -207407.203125\n",
      "Loss is -207477.296875\n",
      "Loss is -207547.390625\n",
      "Loss is -207617.5\n",
      "Loss is -207687.5625\n",
      "Loss is -207757.640625\n",
      "Loss is -207827.71875\n",
      "Loss is -207897.8125\n",
      "Loss is -207967.890625\n",
      "Loss is -208037.953125\n",
      "Loss is -208108.03125\n",
      "Loss is -208178.09375\n",
      "Loss is -208248.15625\n",
      "Loss is -208318.21875\n",
      "Loss is -208388.28125\n",
      "Loss is -208458.328125\n",
      "Loss is -208528.390625\n",
      "Loss is -208598.4375\n",
      "Loss is -208668.46875\n",
      "Loss is -208738.515625\n",
      "Loss is -208808.546875\n",
      "Loss is -208878.59375\n",
      "Loss is -208948.625\n",
      "Loss is -209018.65625\n",
      "Loss is -209088.6875\n",
      "Loss is -209158.703125\n",
      "Loss is -209228.71875\n",
      "Loss is -209298.71875\n",
      "Loss is -209368.75\n",
      "Loss is -209438.78125\n",
      "Loss is -209508.78125\n",
      "Loss is -209578.765625\n",
      "Loss is -209648.78125\n",
      "Loss is -209718.78125\n",
      "Loss is -209788.78125\n",
      "Loss is -209858.765625\n",
      "Loss is -209928.78125\n",
      "Loss is -209998.765625\n",
      "Loss is -210068.734375\n",
      "Loss is -210138.734375\n",
      "Loss is -210208.703125\n",
      "Loss is -210278.6875\n",
      "Loss is -210348.65625\n",
      "Loss is -210418.625\n",
      "Loss is -210488.578125\n",
      "Loss is -210558.578125\n",
      "Loss is -210628.546875\n",
      "Loss is -210698.5\n",
      "Loss is -210768.453125\n",
      "Loss is -210838.40625\n",
      "Loss is -210908.34375\n",
      "Loss is -210978.3125\n",
      "Loss is -211048.25\n",
      "Loss is -211118.1875\n",
      "Loss is -211188.125\n",
      "Loss is -211258.078125\n",
      "Loss is -211328.015625\n",
      "Loss is -211397.9375\n",
      "Loss is -211467.875\n",
      "Loss is -211537.796875\n",
      "Loss is -211607.71875\n",
      "Loss is -211677.640625\n",
      "Loss is -211747.5625\n",
      "Loss is -211817.46875\n",
      "Loss is -211887.40625\n",
      "Loss is -211957.28125\n",
      "Loss is -212027.203125\n",
      "Loss is -212097.125\n",
      "Loss is -212167.0\n",
      "Loss is -212236.90625\n",
      "Loss is -212306.84375\n",
      "Loss is -212376.734375\n",
      "Loss is -212446.59375\n",
      "Loss is -212516.5\n",
      "Loss is -212586.390625\n",
      "Loss is -212656.25\n",
      "Loss is -212726.15625\n",
      "Loss is -212796.03125\n",
      "Loss is -212865.90625\n",
      "Loss is -212935.78125\n",
      "Loss is -213005.65625\n",
      "Loss is -213075.53125\n",
      "Loss is -213145.40625\n",
      "Loss is -213215.25\n",
      "Loss is -213285.125\n",
      "Loss is -213354.96875\n",
      "Loss is -213424.84375\n",
      "Loss is -213494.703125\n",
      "Loss is -213564.5625\n",
      "Loss is -213634.40625\n",
      "Loss is -213704.265625\n",
      "Loss is -213774.09375\n",
      "Loss is -213843.9375\n",
      "Loss is -213913.796875\n",
      "Loss is -213983.625\n",
      "Loss is -214053.46875\n",
      "Loss is -214123.28125\n",
      "Loss is -214193.125\n",
      "Loss is -214262.953125\n",
      "Loss is -214332.78125\n",
      "Loss is -214402.59375\n",
      "Loss is -214472.421875\n",
      "Loss is -214542.25\n",
      "Loss is -214612.0625\n",
      "Loss is -214681.875\n",
      "Loss is -214751.671875\n",
      "Loss is -214821.5\n",
      "Loss is -214891.296875\n",
      "Loss is -214961.09375\n",
      "Loss is -215030.90625\n",
      "Loss is -215100.71875\n",
      "Loss is -215170.5\n",
      "Loss is -215240.296875\n",
      "Loss is -215310.09375\n",
      "Loss is -215379.875\n",
      "Loss is -215449.671875\n",
      "Loss is -215519.46875\n",
      "Loss is -215589.25\n",
      "Loss is -215659.015625\n",
      "Loss is -215728.796875\n",
      "Loss is -215798.59375\n",
      "Loss is -215868.359375\n",
      "Loss is -215938.125\n",
      "Loss is -216007.890625\n",
      "Loss is -216077.671875\n",
      "Loss is -216147.40625\n",
      "Loss is -216217.1875\n",
      "Loss is -216286.9375\n",
      "Loss is -216356.703125\n",
      "Loss is -216426.453125\n",
      "Loss is -216496.21875\n",
      "Loss is -216565.953125\n",
      "Loss is -216635.71875\n",
      "Loss is -216705.46875\n",
      "Loss is -216775.21875\n",
      "Loss is -216844.9375\n",
      "Loss is -216914.671875\n",
      "Loss is -216984.421875\n",
      "Loss is -217054.125\n",
      "Loss is -217123.875\n",
      "Loss is -217193.59375\n",
      "Loss is -217263.34375\n",
      "Loss is -217333.0625\n",
      "Loss is -217402.78125\n",
      "Loss is -217472.5\n",
      "Loss is -217542.21875\n",
      "Loss is -217611.9375\n",
      "Loss is -217681.65625\n",
      "Loss is -217751.34375\n",
      "Loss is -217821.078125\n",
      "Loss is -217890.78125\n",
      "Loss is -217960.46875\n",
      "Loss is -218030.1875\n",
      "Loss is -218099.875\n",
      "Loss is -218169.578125\n",
      "Loss is -218239.265625\n",
      "Loss is -218308.953125\n",
      "Loss is -218378.65625\n",
      "Loss is -218448.34375\n",
      "Loss is -218518.03125\n",
      "Loss is -218587.71875\n",
      "Loss is -218657.40625\n",
      "Loss is -218727.078125\n",
      "Loss is -218796.75\n",
      "Loss is -218866.4375\n",
      "Loss is -218936.09375\n",
      "Loss is -219005.765625\n",
      "Loss is -219075.4375\n",
      "Loss is -219145.109375\n",
      "Loss is -219214.78125\n",
      "Loss is -219284.4375\n",
      "Loss is -219354.078125\n",
      "Loss is -219423.71875\n",
      "Loss is -219493.40625\n",
      "Loss is -219563.03125\n",
      "Loss is -219632.6875\n",
      "Loss is -219702.34375\n",
      "Loss is -219771.96875\n",
      "Loss is -219841.625\n",
      "Loss is -219911.265625\n",
      "Loss is -219980.890625\n",
      "Loss is -220050.546875\n",
      "Loss is -220120.15625\n",
      "Loss is -220189.78125\n",
      "Loss is -220259.4375\n",
      "Loss is -220329.0625\n",
      "Loss is -220398.703125\n",
      "Loss is -220468.3125\n",
      "Loss is -220537.9375\n",
      "Loss is -220607.5625\n",
      "Loss is -220677.171875\n",
      "Loss is -220746.78125\n"
     ]
    }
   ],
   "source": [
    "from script.DHOV.Sampling.PerGroupLineSearchSampling import PerGroupLineSearchSamplingStrategy\n",
    "from script.Verification.Verification import add_layer_to_model\n",
    "center = torch.flatten(torch_image)\n",
    "sampling_strategy = PerGroupLineSearchSamplingStrategy(center, input_bounds, nn, sample_count=sample_count, rand_samples_percent=0.2, rand_sample_alternation_percent=0.05, num_iterations=3000)\n",
    "\n",
    "icnn_factory = ICNNFactory(\"logical\", net_size, always_use_logical_layer=False)\n",
    "bounds_affine_out, bounds_layer_out = crown_bounds_affine_out, crown_bounds_layer_out\n",
    "filler_gurobi_model = grp.Model()\n",
    "\n",
    "list_of_icnns = []\n",
    "affine_w, affine_b = parameter_list[2*current_layer_index], parameter_list[2*current_layer_index+1]\n",
    "layer_out_size = affine_w.shape[0]\n",
    "group_size = layer_out_size\n",
    "\n",
    "parameter_list = list(nn.parameters())\n",
    "all_group_indices = []\n",
    "for i in range(current_layer_index + 1):\n",
    "    all_group_indices.append([])\n",
    "all_group_indices[-1].append(list(range(0, layer_out_size)))\n",
    "group_indices = all_group_indices[-1]\n",
    "group_i = 0\n",
    "\n",
    "included_space, ambient_space = sampling_strategy.sampling_by_round(affine_w, affine_b, all_group_indices, filler_gurobi_model, current_layer_index, bounds_layer_out, bounds_layer_out, list_of_icnns)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:54:02.289768329Z",
     "start_time": "2024-05-02T20:53:58.085300987Z"
    }
   },
   "id": "f52a04644e4f3e8c",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icnn got enlarged by 3.9262843132019043\n",
      "icnn got enlarged by 3.9262843132019043\n",
      "icnn got enlarged by 3.9262843132019043\n",
      "icnn got enlarged by 3.9262843132019043\n",
      "icnn got enlarged by 3.9262843132019043\n",
      "        time for training: 4.6495726108551025\n",
      "        time for verification: 1.9073486328125e-05\n"
     ]
    }
   ],
   "source": [
    "from script.dataInit import ConvexDataset\n",
    "from script.NeuralNets.trainFunction import train_icnn\n",
    "import script.DHOV.Normalisation as norm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "index_to_select = torch.tensor(group_indices[group_i]).to(device)\n",
    "\n",
    "size_of_icnn_input = len(index_to_select)\n",
    "current_icnn = icnn_factory.get_new_icnn(size_of_icnn_input)\n",
    "\n",
    "t = time.time()\n",
    "group_inc_space = included_space[group_i]\n",
    "group_amb_space = ambient_space[group_i]\n",
    "\n",
    "\n",
    "mean = norm.get_mean(group_inc_space, group_amb_space)\n",
    "std = norm.get_std(group_inc_space, group_amb_space)\n",
    "group_norm_included_space, group_norm_ambient_space = norm.normalize_data(group_inc_space,\n",
    "                                                                          group_amb_space,\n",
    "                                                                          mean, std)\n",
    "dataset = ConvexDataset(data=group_norm_included_space)\n",
    "train_loader = DataLoader(dataset, batch_size=icnn_batch_size)\n",
    "dataset = ConvexDataset(data=group_norm_ambient_space)\n",
    "ambient_loader = DataLoader(dataset, batch_size=icnn_batch_size)\n",
    "\n",
    "if init_network:\n",
    "    low = torch.index_select(bounds_layer_out[current_layer_index][0], 0, index_to_select)\n",
    "    up = torch.index_select(bounds_layer_out[current_layer_index][1], 0, index_to_select)\n",
    "    low = torch.div(torch.add(low, -mean), std)\n",
    "    up = torch.div(torch.add(up, -mean), std)\n",
    "    current_icnn.init_with_box_bounds(low, up)\n",
    "\n",
    "# train icnn\n",
    "current_icnn.use_training_setup = True  # is only relevant for ApproxMaxICNNs\n",
    "epochs_per_inclusion = icnn_epochs // force_inclusion_steps\n",
    "epochs_in_last_inclusion = icnn_epochs % force_inclusion_steps\n",
    "for inclusion_round in range(force_inclusion_steps):\n",
    "    if inclusion_round > 0:\n",
    "        if inclusion_round == force_inclusion_steps - 1 and epochs_in_last_inclusion > 0:\n",
    "            epochs_per_inclusion = epochs_in_last_inclusion\n",
    "\n",
    "        out = current_icnn(group_norm_included_space)\n",
    "        max_out = torch.max(out)\n",
    "        current_icnn.apply_enlargement(max_out)\n",
    "        print(\"icnn got enlarged by {}\".format(max_out))\n",
    "\n",
    "    train_icnn(current_icnn, train_loader, ambient_loader, epochs=epochs_per_inclusion,\n",
    "               hyper_lambda=hyper_lambda,\n",
    "               optimizer=optimizer, adapt_lambda=adapt_lambda, preemptive_stop=preemptive_stop,\n",
    "               verbose=print_training_loss, print_last_loss=print_last_loss)\n",
    "    \n",
    "out = current_icnn(group_norm_included_space)\n",
    "max_out = torch.max(out)\n",
    "current_icnn.apply_enlargement(max_out)\n",
    "print(\"icnn got enlarged by {}\".format(max_out))\n",
    "\n",
    "current_icnn.apply_normalisation(mean, std)\n",
    "\n",
    "current_icnn.use_training_setup = False\n",
    "\n",
    "print(\"        time for training: {}\".format(time.time() - t))\n",
    "\n",
    "t = time.time()\n",
    "# verify and enlarge convex approximation\n",
    "\n",
    "\"\"\"\n",
    "if use_over_approximation:\n",
    "    copy_model = nn_encoding_model.copy()\n",
    "    adversarial_input, c = ver.verification(current_icnn, copy_model, affine_w.cpu().detach().cpu().numpy(),\n",
    "                                            affine_b.detach().cpu().numpy(), group_indices[group_i],\n",
    "                                            bounds_affine_out[current_layer_index],\n",
    "                                            bounds_layer_out[current_layer_index], prev_layer_index,\n",
    "                                            has_relu=True, icnn_as_lp=encode_icnn_enlargement_as_lp)\n",
    "    current_icnn.apply_enlargement(c)\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "print(\"        time for verification: {}\".format(time.time() - t))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:54:15.431126320Z",
     "start_time": "2024-05-02T20:54:10.776029601Z"
    }
   },
   "id": "a3f8e01d158ae0e",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "gurobi_model = grp.Model()\n",
    "group_name = 1\n",
    "\n",
    "\n",
    "index_to_select = torch.tensor(group_indices[0]).to(device)\n",
    "\n",
    "\n",
    "low = torch.index_select(bounds_layer_out[current_layer_index][0], 0, index_to_select)\n",
    "up = torch.index_select(bounds_layer_out[current_layer_index][1], 0, index_to_select)\n",
    "constraint_icnn_bounds_affine_out, constraint_icnn_bounds_layer_out = current_icnn.calculate_box_bounds([low, up])\n",
    "current_in_vars = gurobi_model.addMVar(len(group_indices[0]), lb=low.detach().cpu().numpy(), ub=up.detach().cpu().numpy(), name=\"icnn_var_group_{}_{}\".format(current_layer_index, group_name))\n",
    "\n",
    "current_icnn.add_max_output_constraints(gurobi_model, current_in_vars, constraint_icnn_bounds_affine_out,\n",
    "                                           constraint_icnn_bounds_layer_out)\n",
    "\n",
    "\n",
    "for i, var in enumerate(current_in_vars.tolist()):\n",
    "    var.setAttr(\"varname\", \"output_layer_[{}]_[{}]\".format(current_layer_index, i))\n",
    "\n",
    "\n",
    "gurobi_model.update()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:54:20.702975352Z",
     "start_time": "2024-05-02T20:54:20.547617628Z"
    }
   },
   "id": "ae79b84ec6a4f94c",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from script.Verification.VerificationBasics import add_affine_constr\n",
    "if encode_last_affine_layer:\n",
    "    affine_w, affine_b = parameter_list[-2].detach().cpu().numpy(), parameter_list[-1].detach().cpu().numpy()\n",
    "    last_layer_index = current_layer_index + 1\n",
    "    output_second_last_layer = []\n",
    "    for m in range(affine_w.shape[1]):\n",
    "        output_second_last_layer.append(gurobi_model.getVarByName(\"output_layer_[{}]_[{}]\".format(last_layer_index - 1, m)))\n",
    "    output_second_last_layer = grp.MVar.fromlist(output_second_last_layer)\n",
    "    in_lb = bounds_affine_out[last_layer_index][0].detach().cpu().numpy()\n",
    "    in_ub = bounds_affine_out[last_layer_index][1].detach().cpu().numpy()\n",
    "    output_nn = add_affine_constr(gurobi_model, affine_w, affine_b, output_second_last_layer, in_lb, in_ub, i=last_layer_index)\n",
    "    for m, var in enumerate(output_nn.tolist()):\n",
    "        var.setAttr(\"varname\", \"output_layer_[{}]_[{}]\".format(last_layer_index, m))\n",
    "\n",
    "    gurobi_model.update()\n",
    "    print(\"done\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:54:21.284623680Z",
     "start_time": "2024-05-02T20:54:21.278453482Z"
    }
   },
   "id": "c7ca329bb78e4501",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def optimize_model(model, output_vars):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param model: the optimization problem in gurobi encoding the NN and the objective \n",
    "    :param output_vars: the gurobi variables from the model of the NN describing the output neurons of the NN\n",
    "    :return True if verification was successful, else false \n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    model.update()\n",
    "    model.optimize()\n",
    "    \n",
    "    if model.Status == grp.GRB.OPTIMAL or model.Status == grp.GRB.USER_OBJ_LIMIT:\n",
    "        \n",
    "        for i, var in enumerate(output_vars.tolist()):\n",
    "            print(\"var {}: {}\".format(i, var.getAttr(\"x\")))\n",
    "        max_var = model.getVarByName(\"max_var\").getAttr(\"x\")\n",
    "        \n",
    "        if max_var < 0:\n",
    "            print(\"property verified with max difference {}\".format(max_var))\n",
    "            return True\n",
    "        else:\n",
    "             print(\"property NOT verified with max difference {}\".format(max_var))\n",
    "             return False\n",
    "\n",
    "    elif model.Status == grp.GRB.INFEASIBLE:\n",
    "        print(\"model infeasible\")\n",
    "\n",
    "        model.computeIIS()\n",
    "        print(\"constraint\")\n",
    "        all_constr = model.getConstrs()\n",
    "\n",
    "        for const in all_constr:\n",
    "            if const.IISConstr:\n",
    "                print(\"{}\".format(const))\n",
    "\n",
    "        print(\"lower bound\")\n",
    "        all_var = model.getVars()\n",
    "        for var in all_var:\n",
    "            if var.IISLB:\n",
    "                print(\"{}, lb: {}, ub: {}\".format(var, var.getAttr(\"lb\"), var.getAttr(\"ub\")))\n",
    "\n",
    "        print(\"upper bound\")\n",
    "        all_var = model.getVars()\n",
    "        for var in all_var:\n",
    "            if var.IISUB:\n",
    "                print(\"{}, lb: {}, ub: {}\".format(var, var.getAttr(\"lb\"), var.getAttr(\"ub\")))\n",
    "\n",
    "    print(\"time to optimize: {}\".format(time.time() - start))\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:54:22.024713976Z",
     "start_time": "2024-05-02T20:54:22.020654214Z"
    }
   },
   "id": "71f2e34ddd69e1f0",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def get_output_vars_dhov(model, output_size, output_layer_index):\n",
    "    output_vars = []\n",
    "    for i in range(output_size):\n",
    "        output_vars.append(model.getVarByName(\"output_layer_[{}]_[{}]\".format(output_layer_index, i)))\n",
    "    output_vars = grp.MVar.fromlist(output_vars)\n",
    "    return output_vars\n",
    "\n",
    "test_model = gurobi_model.copy()\n",
    "test_out_vars = get_output_vars_dhov(test_model, output_size, number_layer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:54:22.599182288Z",
     "start_time": "2024-05-02T20:54:22.558668711Z"
    }
   },
   "id": "e6c92ed76660156",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter BestObjStop to value 10\n"
     ]
    }
   ],
   "source": [
    "add_output_constraints(test_model, bounds_layer_out, label, test_out_vars, sovler_bound=10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:54:22.819257031Z",
     "start_time": "2024-05-02T20:54:22.773617264Z"
    }
   },
   "id": "33a5ed712b23e197",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gurobi Optimizer version 11.0.1 build v11.0.1rc0 (linux64 - \"Linux Mint 21.3\")\n",
      "\n",
      "CPU model: AMD Ryzen 7 7700X 8-Core Processor, instruction set [SSE2|AVX|AVX2|AVX512]\n",
      "Thread count: 8 physical cores, 16 logical processors, using up to 16 threads\n",
      "\n",
      "Optimize a model with 861 rows, 697 columns and 50756 nonzeros\n",
      "Model fingerprint: 0xdbdc6425\n",
      "Model has 1 general constraint\n",
      "Variable types: 697 continuous, 0 integer (0 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [3e-07, 3e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [8e-02, 1e+02]\n",
      "  RHS range        [1e-03, 3e+01]\n",
      "Presolve added 0 rows and 1 columns\n",
      "Presolve removed 210 rows and 0 columns\n",
      "Presolve time: 0.14s\n",
      "Presolved: 651 rows, 698 columns, 50093 nonzeros\n",
      "Variable types: 689 continuous, 9 integer (9 binary)\n",
      "Found heuristic solution: objective 33.0606104\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.15 seconds (0.14 work units)\n",
      "Thread count was 16 (of 16 available processors)\n",
      "\n",
      "Solution count 1: 33.0606 \n",
      "\n",
      "Optimization achieved user objective limit\n",
      "Best objective 3.306061036960e+01, best bound 4.226461029053e+01, gap 27.8398%\n",
      "var 0: -3.1412036212681116\n",
      "var 1: -1.3663481297856066\n",
      "var 2: -4.385937925661022\n",
      "var 3: -21.655444198346743\n",
      "var 4: -1.213681804387079\n",
      "var 5: -3.7874318558591185\n",
      "var 6: -3.3018614661121237\n",
      "var 7: -3.6374816042280855\n",
      "var 8: -6.823716784138437\n",
      "var 9: 11.405166171257392\n",
      "property NOT verified with max difference 33.060610369604134\n",
      "overall time: 0.14845895767211914\n"
     ]
    }
   ],
   "source": [
    "overall_time = time.time()\n",
    "optimize_model(test_model, test_out_vars)\n",
    "print(\"overall time: {}\".format(time.time() - overall_time))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:54:23.224338308Z",
     "start_time": "2024-05-02T20:54:23.073543255Z"
    }
   },
   "id": "d18f82b4b08b3d6a",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T20:54:23.639570074Z",
     "start_time": "2024-05-02T20:54:23.636920500Z"
    }
   },
   "id": "d286c37aef989710",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "8d3710152236ad3f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
